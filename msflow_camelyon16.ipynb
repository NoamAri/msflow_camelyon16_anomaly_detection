{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9903e0a8-b529-4682-8e1f-598f67d4f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Data Preprocessing\n",
    "# ----------------------------\n",
    "\n",
    "def transform_image(pil_image: Image.Image) -> torch.Tensor:\n",
    "    transform_ops = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform_ops(pil_image.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ee42e7-8903-456d-8813-d8bb1556de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camelyon16Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train'):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.names = []\n",
    "        self.split = split\n",
    "\n",
    "        if split == 'train':\n",
    "            subdir = os.path.join(root_dir, 'train', 'good')\n",
    "            self.image_paths = [os.path.join(subdir, f) for f in os.listdir(subdir)]\n",
    "            self.labels = [0] * len(self.image_paths)\n",
    "        else:\n",
    "            good_dir = os.path.join(root_dir, 'test', 'good')\n",
    "            bad_dir = os.path.join(root_dir, 'test', 'Ungood')\n",
    "            self.image_paths += [os.path.join(good_dir, f) for f in os.listdir(good_dir)]\n",
    "            self.labels += [0] * len(os.listdir(good_dir))\n",
    "            self.image_paths += [os.path.join(bad_dir, f) for f in os.listdir(bad_dir)]\n",
    "            self.labels += [1] * len(os.listdir(bad_dir))\n",
    "\n",
    "        self.names = [os.path.basename(p) for p in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        name = self.names[idx]\n",
    "        image = Image.open(img_path)\n",
    "        return transform_image(image), label, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2f4701-50f0-4a89-9a84-c51b49b5dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. Feature Extractor\n",
    "# ----------------------------\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        return [out1, out2, out3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad29934f-499f-4998-a6cd-c5a12ae6846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. ActNorm\n",
    "# ----------------------------\n",
    "class ActNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
    "        self.logs = nn.Parameter(torch.zeros(1, num_channels, 1, 1))\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, x):\n",
    "        with torch.no_grad():\n",
    "            mean = x.mean([0, 2, 3], keepdim=True)\n",
    "            std = x.std([0, 2, 3], keepdim=True)\n",
    "            self.bias.data.copy_(-mean)\n",
    "            self.logs.data.copy_(torch.log(1 / (std + 1e-6)))\n",
    "            self.initialized = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.initialized:\n",
    "            self.initialize(x)\n",
    "        return (x + self.bias) * torch.exp(self.logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19313fe0-67c5-4c95-a298-b7b81cbc3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. RealNVP Block\n",
    "# ----------------------------\n",
    "class RealNVPBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.norm = ActNorm2d(in_channels)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels // 2, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, in_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_a, x_b = x.chunk(2, dim=1)\n",
    "        h = self.net(x_a)\n",
    "        s, t = h.chunk(2, dim=1)\n",
    "        s = torch.sigmoid(s + 2.0)\n",
    "        y_b = s * x_b + t\n",
    "        log_det_jacobian = torch.sum(torch.log(s).view(x.size(0), -1), dim=1)\n",
    "        out = torch.cat([x_a, y_b], dim=1)\n",
    "        out = self.norm(out)\n",
    "        return out, log_det_jacobian\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5. MSFlow\n",
    "# ----------------------------\n",
    "class MSFlow(nn.Module):\n",
    "    def __init__(self, in_channels_list):\n",
    "        super().__init__()\n",
    "        self.subflows = nn.ModuleList([\n",
    "            nn.Sequential(*[RealNVPBlock(c) for _ in range(3)])\n",
    "            for c in in_channels_list\n",
    "        ])\n",
    "\n",
    "    def forward(self, features):\n",
    "        total_log_likelihood = []\n",
    "        for i, flow in enumerate(self.subflows):\n",
    "            x = features[i]\n",
    "            log_det = 0\n",
    "            for block in flow:\n",
    "                x, det = block(x)\n",
    "                log_det += det\n",
    "            log_prob = -0.5 * torch.sum(x ** 2, dim=[1, 2, 3]) + log_det\n",
    "            total_log_likelihood.append(log_prob)\n",
    "\n",
    "        fused_score = sum(total_log_likelihood) / len(total_log_likelihood)\n",
    "        return -fused_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32bbb18-1fcc-400a-8a25-6f3f686a3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Training â€“ Image-level AUROC Only\n",
    "# ----------------------------\n",
    "def train_msflow():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    root_dir = r\"C:\\afeca academy\\×¡×™××¡×˜×¨ ×‘\\advanced deep learning\\project Normalizing Flow\\MedlAnaomaly-Data\\Camelyon16\\Camelyon16\"\n",
    "\n",
    "    train_set = Camelyon16Dataset(root_dir, split='train')\n",
    "    test_set = Camelyon16Dataset(root_dir, split='test')\n",
    "    train_loader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    resnet = ResNetFeatureExtractor().to(device)\n",
    "    flow_model = MSFlow(in_channels_list=[256, 512, 1024]).to(device)\n",
    "    optimizer = torch.optim.Adam(flow_model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        print(f\"\\nğŸ” Epoch {epoch+1}/10\", flush=True)\n",
    "        flow_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (imgs, _, _) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            features = resnet(imgs)\n",
    "            loss = flow_model(features).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            print(f\"  ğŸ“¦ Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\", flush=True)\n",
    "\n",
    "        print(f\"âœ… Epoch {epoch+1} Avg Loss: {total_loss/len(train_loader):.4f}\", flush=True)\n",
    "\n",
    "        # Evaluation\n",
    "        flow_model.eval()\n",
    "        all_scores, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels, _ in test_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                features = resnet(imgs)\n",
    "                scores = flow_model(features)\n",
    "                all_scores.extend(scores.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        preds = [1 if s > np.median(all_scores) else 0 for s in all_scores]\n",
    "        cm = confusion_matrix(all_labels, preds)\n",
    "        auc = roc_auc_score(all_labels, all_scores)\n",
    "        print(f\"ğŸ“Š Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"ğŸ“ˆ Image-level ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c65f9c-c6cf-40c8-bb7e-91d5d25dbc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "ğŸ” Epoch 1/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 323792.3125\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 359444.3438\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 351001.7812\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 350395.4062\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 347852.7500\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 332094.7500\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 345539.5312\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 348751.6250\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 345478.7500\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 336762.3125\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 340683.4375\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 346451.6250\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 339819.3125\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 346239.1250\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 334955.5000\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 340436.4375\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 329445.4375\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 331054.3750\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 339281.8750\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 329189.2812\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 313356.7500\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 317285.8125\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 320206.9375\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 327705.5625\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 330179.9688\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 340535.5625\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 329134.0625\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 329703.7500\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 322539.3125\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 309647.1250\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 309611.0000\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 312501.0625\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 327985.8125\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 310818.2500\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 310119.3438\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 312700.0625\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 318148.5625\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 311015.9375\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 309437.5312\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 317520.7812\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 317842.9375\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 299631.2812\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 316631.0625\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 309810.9688\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 304978.0000\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 304603.0000\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 311001.2812\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 304610.3750\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 291458.1875\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 305308.1875\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 295832.9375\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 292012.6250\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 301786.5625\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 292327.5938\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 297993.8125\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 291961.1250\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 290394.7188\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 299464.8125\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 294789.9688\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 288210.7500\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 294591.5625\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 281287.3750\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 284330.1875\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 281613.0312\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 290240.0000\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 294201.1562\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 295514.5000\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 280484.9062\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 285634.9375\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 289484.5938\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 271875.8750\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 275768.0625\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 273242.4062\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 274716.2500\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 280080.7188\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 283667.2500\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 274658.2500\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 270810.0625\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 284794.5938\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 269582.9375\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 279294.3125\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 268007.2500\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 276795.6875\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 266421.8750\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 263171.0938\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 263612.5938\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 276701.8750\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 262988.7500\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 269986.4375\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 278153.2500\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 264953.5625\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 271518.1875\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 272004.2500\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 268290.7188\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 259775.3438\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 256099.4375\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 265548.0625\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 264595.4375\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 254341.0625\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 254380.2500\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 270718.5000\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 261838.5312\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 257716.4062\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 257653.7188\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 257724.7812\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 248918.7500\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 267075.9062\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 256536.5312\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 260840.3281\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 254805.1250\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 263318.0312\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 253426.5312\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 259339.6562\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 255234.0000\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 251163.5312\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 255505.7188\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 244540.6406\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 257454.7812\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 243946.7188\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 265890.1875\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 258243.0312\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 249495.0000\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 251238.3750\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 247175.2812\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 254150.9844\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 251557.5469\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 256543.3906\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 249502.9688\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 253231.8750\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 242594.3438\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 253930.3906\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 253304.4688\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 243243.3281\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 247670.4375\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 254065.0312\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 247944.9844\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 250019.1406\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 238630.3750\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 241960.8438\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 246089.8750\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 248617.9688\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 255978.4688\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 240186.5625\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 236500.5625\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 246577.0000\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 243773.1875\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 242041.3594\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 236767.7500\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 246551.2500\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 239541.5625\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 235332.1250\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 232387.3906\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 244346.4062\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 229289.3906\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 237549.7969\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 249764.5312\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 227659.4062\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 249297.7031\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 231180.3281\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 228946.1875\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 237783.4375\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 232650.3906\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 247374.7656\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 243402.5312\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 236497.9688\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 233603.1562\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 231259.6562\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 232118.7500\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 230904.3594\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 231421.7188\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 227140.2344\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 230457.2031\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 229537.6562\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 241255.3125\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 222596.6406\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 226880.0312\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 232983.6875\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 227584.9375\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 237095.5000\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 224706.0156\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 243235.4688\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 231911.2656\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 227817.8594\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 231829.8438\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 243465.4688\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 240376.4844\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 220747.2812\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 227644.4219\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 228118.5000\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 232962.2812\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 234535.5312\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 218928.3750\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 221700.5000\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 227320.4688\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 227023.6875\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 233199.5000\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 225362.0312\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 224534.0312\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 221799.3125\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 235623.9375\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 224373.2188\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 223728.9688\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 226345.8750\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 221269.9062\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 229753.1094\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 220435.1250\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 231448.4688\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 235602.3125\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 210191.0156\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 228494.1875\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 227668.9688\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 228077.1719\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 216339.1875\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 234080.4688\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 232568.2500\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 226579.6562\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 228635.1250\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 214751.1562\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 227471.9375\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 220166.7500\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 217196.5938\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 215660.6562\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 224222.1562\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 216473.5312\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 224844.3438\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 214864.2344\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 213716.1250\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 224851.5938\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 223618.1719\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 217293.2812\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 226107.5625\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 223403.7500\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 217880.7656\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 215600.5000\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 214564.1719\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 235511.2812\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 212701.8750\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 212543.8438\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 219784.8125\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 224902.4844\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 216094.3281\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 220068.2812\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 222183.0000\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 215477.1875\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 222527.5000\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 218248.6719\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 208883.8125\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 227633.4375\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 219322.6094\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 209414.3438\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 225675.8281\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 214749.3125\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 204272.4062\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 215465.0312\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 209098.3125\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 216695.0000\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 211758.3125\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 209261.3438\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 213222.3125\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 220601.9688\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 216264.7188\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 214572.1406\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 213054.9531\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 222223.2500\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 217554.6250\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 220772.2188\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 216532.1250\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 218646.6250\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 211808.3438\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 212402.1875\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 199979.1562\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 209940.7656\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 205919.9531\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 210900.4062\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 212327.3750\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 209536.8281\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 211748.2500\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 217181.3281\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 209982.3438\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 202761.2031\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 207016.6562\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 216790.5625\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 205768.3281\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 204904.3125\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 204049.5938\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 211200.6719\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 203914.5781\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 216435.2344\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 211532.0781\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 210174.1250\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 221680.5781\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 212643.9219\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 217179.8125\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 216732.3125\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 213124.7500\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 205250.0156\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 195925.6875\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 202387.0625\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 207518.3594\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 203746.6719\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 212376.6094\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 197629.6406\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 212260.0938\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 198143.4688\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 203293.4375\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 209555.8906\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 195690.7031\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 204640.6875\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 213251.4062\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 205265.7812\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 212440.8594\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 211662.3438\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 215210.7188\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 200954.3438\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 207675.3281\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 201393.0938\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 202035.1250\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 209884.4062\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 207424.6250\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 210058.5625\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 205358.7188\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 204711.8594\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 199146.6875\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 198182.3438\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 208859.2188\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 203421.1250\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 211259.2500\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 213326.1250\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 197415.6094\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 190950.0781\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 209056.4688\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 201070.5312\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 196862.9062\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 204988.4531\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 201213.0000\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 203052.2500\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 200896.9688\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 202529.3594\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 205914.4375\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 195102.5625\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 208715.6562\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 196447.4375\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 207632.3906\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 209104.5781\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 204202.8438\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 192217.2500\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 203066.6250\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 198374.7031\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 202472.0156\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 198016.9062\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 201536.9062\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 191065.2969\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 204299.0312\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 197697.2188\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 210835.1562\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 194905.9844\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 197852.1250\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 197090.6562\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 205981.5938\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 202231.7812\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 204593.7344\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 196390.1094\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 195336.2344\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 205709.8906\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 191302.3906\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 193133.5000\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 209459.6875\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 190312.3906\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 200757.2188\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 197113.6875\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 194165.9062\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 196818.7500\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 202009.0938\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 184826.9844\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 199080.9062\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 196288.4375\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 201119.6875\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 191373.7500\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 187571.1406\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 189016.3750\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 184381.6562\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 192893.3125\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 197418.5938\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 193370.7500\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 196509.3906\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 186691.4375\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 202591.2344\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 196879.6250\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 201567.0938\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 193739.6250\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 185491.2500\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 192083.2812\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 205708.4219\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 193567.5625\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 195735.9688\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 191718.7031\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 196179.1875\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 187681.2188\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 189869.8438\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 204478.0000\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 197885.4062\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 188550.8750\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 195586.1250\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 189689.4375\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 189485.4688\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 196498.5938\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 190073.3750\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 190345.5312\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 199942.0469\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 188975.9062\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 197545.1875\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 190317.4375\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 183945.4062\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 198042.5781\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 198198.6875\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 196188.4062\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 193719.0469\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 195447.0938\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 189767.0781\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 194566.6406\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 193767.5469\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 192563.5781\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 184384.0000\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 195618.0000\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 183039.8438\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 183483.7969\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 196877.9062\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 197323.0469\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 190692.2500\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 189270.9531\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 192875.2031\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 187236.0156\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 195473.0625\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 178650.2188\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 179739.8438\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 179527.9844\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 189728.5938\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 184385.0000\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 180671.4375\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 191695.2656\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 176245.4844\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 179846.0938\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 187650.6875\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 181313.3438\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 204457.5938\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 184329.8906\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 192527.3281\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 180337.8438\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 183460.7188\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 188589.3281\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 175799.0156\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 183103.1562\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 189098.5625\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 195477.3594\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 184938.4531\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 179407.6875\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 184564.6250\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 188897.7812\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 192985.0469\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 181745.3906\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 187700.3125\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 189454.0312\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 197569.7031\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 186607.0469\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 186207.5000\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 178388.6250\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 186751.6406\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 191972.4219\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 183933.6406\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 188211.7969\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 189014.6875\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 183786.0625\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 190879.2188\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 177578.9844\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 185008.3281\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 186408.4062\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 179016.1406\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 175579.3594\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 188835.0000\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 178408.1562\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 185821.7344\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 186736.0312\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 184090.2500\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 182059.0938\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 178161.7500\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 181425.1875\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 177505.3438\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 187627.7500\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 183224.4219\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 189634.4062\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 171742.0312\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 179713.6406\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 177404.1875\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 185531.5000\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 181477.3125\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 174736.8125\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 181753.2812\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 179099.1562\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 186673.3750\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 176369.6875\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 176160.0469\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 176094.9375\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 183069.8750\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 174914.5938\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 182308.4531\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 187334.0781\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 174555.0312\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 182695.5781\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 174645.7500\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 184047.0312\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 179042.4844\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 182971.9688\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 181771.2969\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 180786.1250\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 184920.9531\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 175038.0156\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 172784.7188\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 177935.5000\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 171948.0781\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 175914.0000\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 180766.0781\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 183433.2031\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 171819.9844\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 177776.8750\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 181934.0625\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 174963.7812\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 173564.2969\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 184984.2188\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 166374.2031\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 182198.3438\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 165124.8594\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 183023.0312\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 179403.2656\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 175018.4688\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 180024.0000\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 175337.8125\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 172991.6094\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 176446.3125\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 167163.7344\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 181256.8906\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 170269.4844\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 168970.0000\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 171515.7812\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 171309.4844\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 170840.2188\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 169416.0312\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 175911.2812\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 168024.0469\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 180031.4375\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 173864.0312\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 174298.7031\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 178127.5938\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 182007.7031\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 174862.2812\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 169442.8438\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 172371.9375\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 177082.1875\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 185897.3125\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 178514.6562\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 177515.2344\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 169965.1406\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 180466.9688\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 176211.9219\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 179767.6719\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 174869.7812\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 175586.6406\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 173335.7188\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 167136.3438\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 179994.9531\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 180392.8750\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 181469.4219\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 177167.7656\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 177856.3281\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 164294.8438\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 172138.6562\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 173150.9062\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 173478.1250\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 165191.8125\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 176158.0312\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 175861.8594\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 166677.9375\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 177896.7031\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 166876.2188\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 171781.7188\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 169415.5469\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 180516.9062\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 167618.2812\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 165848.5000\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 180149.7344\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 171636.8438\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 171180.7656\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 169691.1562\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 166469.6406\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 178521.9531\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 172844.8281\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 174591.0625\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 171864.1250\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 178743.1094\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 171013.2188\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 172165.0625\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 170193.5000\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 171442.3750\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 169972.0000\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 162919.2500\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 167457.7812\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 170333.6562\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 168889.4688\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 170826.2656\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 177068.8438\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 167046.5938\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 169426.0938\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 174032.8438\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 168538.9375\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 168948.6875\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 165518.2188\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 164113.7031\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 171222.8906\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 165418.5000\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 162827.7656\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 169220.4062\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 167803.8594\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 173181.3594\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 164654.3281\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 162831.2188\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 169094.8906\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 166381.2500\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 173733.4062\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 163284.6250\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 166727.8281\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 158800.0938\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 160926.7031\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 168096.5312\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 168938.3125\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 163320.7969\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 172572.4688\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 160779.1719\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 172782.5781\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 174035.3594\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 171437.7500\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 172120.7500\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 173097.6250\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 170572.0312\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 163555.0625\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 171696.7031\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 159952.0156\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 168751.8281\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 160281.2812\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 169614.8125\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 164924.2812\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 165164.1875\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 162010.2656\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 159178.8281\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 173459.9688\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 157625.5938\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 161034.5000\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 161495.4219\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 165612.0156\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 165502.9688\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 157222.3906\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 167143.4688\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 160120.6719\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 159189.7812\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 161309.0625\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 162277.9531\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 162142.7031\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 163078.2500\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 165799.2969\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 163167.8281\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 166770.5625\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 163089.5469\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 154011.4219\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 153971.1250\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 160966.8125\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 158375.5938\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 172305.9062\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 165408.0781\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 169003.5156\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 166571.7344\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 167256.3125\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 173514.6406\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 158875.8438\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 159395.6875\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 167162.7500\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 161064.1875\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 154976.8125\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 165292.8281\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 170752.3438\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 169620.6406\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 169061.4688\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 162472.0000\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 165009.2500\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 161849.1875\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 165592.3438\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 159431.7344\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 156944.1719\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 165331.0312\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 160142.6250\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 155815.8750\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 154950.8906\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 168979.2188\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 161132.0625\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 156939.2344\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 166845.5625\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 163260.4375\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 163723.2812\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 161578.2031\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 157205.9062\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 155278.1562\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 159628.5156\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 160755.6250\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 166950.9844\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 165072.2500\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 159794.4844\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 160753.1562\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 164227.1562\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 151965.6875\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 159506.8594\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 151559.1250\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 155584.8906\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 157203.9531\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 155590.5781\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 162940.8281\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 159664.0312\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 158521.7031\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 160764.3906\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 161477.2188\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 161243.4062\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 148750.8125\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 157670.4375\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 160988.4688\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 159604.6562\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 162568.0000\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 151630.3750\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 151159.1875\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 154545.2500\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 155565.0938\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 160507.0312\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 158878.0625\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 152021.2656\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 164708.6875\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 158535.2969\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 163314.5156\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 157497.9062\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 161773.8438\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 160484.2500\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 162204.0938\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 165127.4062\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 163461.2500\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 150632.0469\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 153675.7031\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 160429.9062\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 150881.0156\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 157973.0625\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 153315.0938\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 157995.4062\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 157356.7812\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 157649.3906\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 152816.3594\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 163898.7188\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 156360.8438\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 151852.9844\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 154933.9062\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 148560.8750\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 162223.8750\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 154426.6719\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 153674.3594\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 151808.8438\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 158988.7500\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 151797.2031\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 161466.8438\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 157269.7969\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 156174.0625\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 155001.0938\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 156252.2188\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 161516.7344\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 162942.3281\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 147111.6250\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 153287.0000\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 161568.8438\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 157078.2812\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 159523.2812\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 158642.0312\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 158467.1094\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 155901.2500\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 158495.4375\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 145234.9531\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 160669.7500\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 150283.6875\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 151664.0938\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 156546.7656\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 159118.6562\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 148640.0938\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 152390.7500\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 155602.2969\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 152974.1562\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 150796.3594\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 156129.2500\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 151276.8438\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 150962.0938\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 155079.2344\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 155427.0312\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 148183.0781\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 160973.4219\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 152174.2812\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 145217.6875\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 148028.1875\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 155244.7500\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 151477.3125\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 150046.3281\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 153271.4062\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 151319.7500\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 156945.0469\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 146518.9375\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 151974.7656\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 148059.6094\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 149440.2188\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 155228.4531\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 149545.5000\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 149167.3438\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 153728.5156\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 152754.3438\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 146075.3125\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 149351.1094\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 148964.4531\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 153967.9375\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 144960.8750\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 147259.2812\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 148003.5312\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 154418.5000\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 147449.2188\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 150787.3281\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 148770.7500\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 149718.2812\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 150926.5938\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 149901.1875\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 147875.3125\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 146995.3906\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 144308.0000\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 155496.1250\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 147465.6250\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 148938.7969\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 147853.6562\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 155221.9688\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 155325.4688\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 147996.7656\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 154407.9219\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 142894.4062\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 144613.3906\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 150321.5938\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 146135.0312\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 144393.7188\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 149077.6562\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 150266.0156\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 142158.9219\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 144792.2500\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 143790.3438\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 148286.1562\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 147004.4688\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 138638.1875\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 148845.1562\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 148760.0312\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 147506.3281\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 149624.5312\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 152898.3125\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 142314.2969\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 143729.2500\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 151792.6562\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 154333.1875\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 148745.2812\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 149322.5312\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 146438.0625\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 142985.1406\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 146976.3750\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 150573.8125\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 138106.5938\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 150009.9375\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 146175.6875\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 142386.0938\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 152045.0312\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 143675.5000\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 148104.2812\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 138460.5156\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 146433.3438\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 149421.8906\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 147858.1094\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 150615.6250\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 140939.7656\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 144151.1562\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 142695.0000\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 144059.7656\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 144504.3750\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 149933.5938\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 146605.9375\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 147375.9219\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 147190.1406\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 143817.2812\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 149181.5938\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 150292.4375\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 143539.7969\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 144189.6719\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 151673.7812\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 141179.9688\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 144386.2969\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 142266.7656\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 144768.5312\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 147642.5938\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 141065.7969\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 147972.4375\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 148865.0000\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 146338.7812\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 136006.1250\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 153381.5312\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 145798.9062\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 153476.7500\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 148049.0625\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 141630.6562\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 148248.6094\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 139674.7656\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 140890.2188\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 141088.2500\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 149152.9375\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 147834.7656\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 148650.2188\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 143328.9375\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 135533.2812\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 140274.5312\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 144565.8750\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 146371.3594\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 144893.8438\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 147062.2812\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 149795.1562\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 138454.3438\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 146987.7812\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 143164.3750\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 142969.4062\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 139376.7969\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 142588.3750\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 135930.9219\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 141749.7812\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 140670.8125\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 139447.0000\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 151606.7188\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 147885.4531\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 145095.9219\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 134399.9062\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 142527.3750\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 140158.2188\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 142757.6562\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 139154.3750\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 141680.5938\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 143649.2031\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 148776.5469\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 147323.1719\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 136337.2969\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 139753.9062\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 144205.6562\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 142797.6875\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 136066.5938\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 140956.0312\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 142358.1562\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 139756.1562\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 139769.1562\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 148880.9531\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 138923.5312\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 139592.2344\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 139915.5781\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 137066.4531\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 138436.1562\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 139990.4688\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 139679.1406\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 145904.7812\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 142556.7969\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 144137.6250\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 141050.1094\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 139977.9531\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 135894.7188\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 136258.9531\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 141154.9219\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 139556.5469\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 141225.8906\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 138664.3750\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 142089.9062\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 138546.5156\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 139835.5156\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 136437.2500\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 136332.7812\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 138258.7812\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 135168.7656\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 132061.5781\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 136434.3750\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 141326.7656\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 139995.1875\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 132106.5781\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 145269.6250\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 142453.6875\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 139756.4531\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 133567.1250\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 139208.3750\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 141975.5781\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 144059.4062\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 143481.5312\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 138424.7344\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 137821.1250\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 138674.2031\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 137892.4688\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 131956.9062\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 131337.8594\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 138129.5938\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 134601.0312\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 139469.5000\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 132782.9844\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 138771.2500\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 131157.1250\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 135175.0312\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 133433.6562\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 139055.9688\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 143021.5469\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 133493.2500\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 133685.5938\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 139556.3594\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 140094.0469\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 142708.6250\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 138985.1250\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 131405.3438\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 141712.1875\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 138289.0000\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 138986.2969\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 135942.5000\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 139509.3438\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 132745.7188\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 139453.5781\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 132371.7969\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 137978.7969\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 142053.8750\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 143250.9844\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 127360.7656\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 139669.5156\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 132686.4219\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 135141.5625\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 135073.0312\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 134995.2500\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 137521.7031\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 132158.9375\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 139953.4375\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 134830.8281\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 134170.1875\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 143055.7969\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 137255.3594\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 132501.5312\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 130924.0859\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 133807.5000\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 134597.3438\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 131694.2188\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 139204.2500\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 134991.4531\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 137675.3125\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 132357.6094\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 136420.9688\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 135078.6250\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 130292.3047\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 134159.2812\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 136420.2344\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 140290.8125\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 136757.8750\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 138989.5000\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 132541.3750\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 134530.6562\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 130390.0312\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 135299.9062\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 130287.8750\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 133955.0781\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 136649.2188\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 127372.9844\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 134523.8750\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 136480.5000\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 135210.3750\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 137648.0000\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 133529.6250\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 134342.7500\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 130096.3984\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 140933.2188\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 134795.8438\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 131209.0938\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 137897.2656\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 125777.1953\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 137345.7500\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 140821.7812\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 138062.3438\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 134134.8438\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 130889.7188\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 129419.6406\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 132440.8594\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 135259.5938\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 129778.3203\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 126671.6250\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 135430.7031\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 141460.0625\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 127679.6719\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 128747.0781\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 129511.0469\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 131403.5938\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 128741.9297\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 134205.6875\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 127786.3516\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 133050.3750\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 131069.0625\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 127303.7188\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 134540.3438\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 128267.2812\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 133264.4688\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 128030.6641\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 131215.2812\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 136384.3125\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 126963.1875\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 128044.1250\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 126289.5156\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 127515.2500\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 132806.3594\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 129889.8438\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 138207.8438\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 131312.8438\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 125492.1484\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 131589.8125\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 130718.1250\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 128677.8750\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 129293.2656\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 129968.8594\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 133651.5000\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 130428.3203\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 130013.7344\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 128011.7500\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 130570.0625\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 129494.5000\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 136335.3594\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 133560.3125\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 130580.6953\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 134264.2031\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 131377.0156\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 129867.8906\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 125487.4531\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 125707.7969\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 127582.2812\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 131026.0859\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 126725.3516\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 128124.8281\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 128570.2266\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 128102.1094\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 131555.0625\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 122405.7500\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 129830.1406\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 127230.2656\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 128353.6562\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 133297.1094\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 128733.8828\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 128488.9688\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 127299.9062\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 126116.3906\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 136126.7031\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 123519.0469\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 127395.5469\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 134980.3906\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 129341.9375\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 122182.5312\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 126847.7578\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 127930.3125\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 122162.6562\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 123921.7969\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 122834.5938\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 128604.1094\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 129558.8594\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 133926.4844\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 130096.0312\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 130337.6094\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 127652.8750\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 124887.1250\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 119602.1250\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 129968.0781\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 122540.5156\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 126417.5547\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 123625.0234\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 128444.3516\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 124963.2188\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 123337.4141\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 123936.3594\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 126876.5000\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 131956.0000\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 122331.2969\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 124292.5156\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 124735.5547\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 124229.4531\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 127856.9844\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 130098.0000\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 121644.8750\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 132868.6094\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 136371.5312\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 120854.6406\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 122594.6094\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 125944.0000\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 123835.5469\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 127014.1406\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 132321.7500\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 125278.9062\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 122009.2031\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 126228.5156\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 127510.7188\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 123899.9375\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 126121.9844\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 122930.8984\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 129792.1953\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 122879.2344\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 126663.2266\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 125089.9219\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 129615.6562\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 125116.8281\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 121454.5234\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 126246.1797\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 116920.7422\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 121761.8906\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 123903.3047\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 124744.7344\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 127732.4375\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 130681.5781\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 125637.2500\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 125870.4375\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 120372.2188\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 126034.9375\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 124240.3594\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 121632.6094\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 120738.8281\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 121609.2969\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 118119.2344\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 119399.3281\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 119243.4453\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 123475.0625\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 127377.4297\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 129944.8281\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 125845.0312\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 123765.4141\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 126853.4062\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 124680.0469\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 123660.0469\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 126099.7578\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 122895.3516\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 127385.2422\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 117726.8438\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 116476.8906\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 121246.6875\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 124260.3750\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 125315.2031\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 127437.7500\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 122996.5000\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 125253.0703\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 119776.6406\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 120125.4453\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 121293.8281\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 123608.0000\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 126071.3672\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 129094.7344\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 120238.2969\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 125138.6719\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 123164.5312\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 118739.8750\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 123798.9531\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 118660.0781\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 127363.9375\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 115078.1094\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 127325.0547\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 123288.7188\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 119429.8516\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 123411.0156\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 120642.1484\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 129117.6094\n",
      "âœ… Epoch 1 Avg Loss: 179970.7228\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[496 624]\n",
      " [621 492]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4413\n",
      "\n",
      "ğŸ” Epoch 2/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 124646.6250\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 117851.9531\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 119207.4688\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 122316.1094\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 116765.3594\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 119696.4922\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 123539.1094\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 118500.7344\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 125129.6562\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 118776.4062\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 116891.2344\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 122241.0469\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 117392.1172\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 114527.7656\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 116154.7969\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 119953.9375\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 120364.5312\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 117033.9219\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 125928.1406\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 119510.7500\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 121961.1875\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 114554.0078\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 118015.2578\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 113068.2734\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 116427.7656\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 115679.7969\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 120435.4531\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 124773.2578\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 116764.7344\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 122945.0000\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 120600.0391\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 123317.2969\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 115961.4531\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 125426.0938\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 124533.3984\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 115263.3828\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 117201.7031\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 125871.3281\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 115485.2188\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 121026.7500\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 119331.8281\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 121642.8516\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 118468.3438\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 114149.8906\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 119289.4062\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 114920.4297\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 119116.9062\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 120191.1406\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 122004.9844\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 117830.1875\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 115655.2734\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 121682.2969\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 117821.9531\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 123457.8828\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 118295.8516\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 116303.8281\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 119948.1484\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 123402.8203\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 122265.5938\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 123533.8359\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 121154.3281\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 116319.0938\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 120069.6094\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 116590.7891\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 121954.0469\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 119948.3750\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 114002.6406\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 113607.5469\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 119293.3906\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 118464.5625\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 116695.0938\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 118832.5469\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 124010.7500\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 117176.4531\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 118498.7188\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 115146.3438\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 113803.1953\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 113931.6953\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 120531.6875\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 114165.0000\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 119134.6250\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 114282.8281\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 113243.3047\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 111501.3906\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 116203.1797\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 117339.1094\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 111899.5938\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 119371.0547\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 116750.8516\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 121347.2031\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 112835.9688\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 110672.3281\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 114451.6484\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 111036.4062\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 115629.0938\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 111451.0234\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 114677.7266\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 117201.5156\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 120686.1562\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 115104.1328\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 114340.7734\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 115448.2188\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 116570.8125\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 115953.5234\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 111916.8750\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 119697.7734\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 115261.2969\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 112879.0703\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 121756.4219\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 112281.8750\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 117844.4531\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 111828.2422\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 113213.5469\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 111305.4531\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 116931.4688\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 114104.8125\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 120762.3906\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 118562.5000\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 115435.4844\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 120929.0625\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 113047.6094\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 122774.9609\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 113988.4062\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 113856.4375\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 115077.1250\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 114731.9062\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 120247.2812\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 115589.4922\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 119975.6094\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 111199.0547\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 119590.5234\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 114244.5391\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 118942.7109\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 113278.1094\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 110639.6328\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 113306.8203\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 117774.8750\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 116286.9062\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 115346.1406\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 110934.0625\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 116060.6484\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 118021.2812\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 113799.0234\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 116862.9375\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 115695.4375\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 112559.8359\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 118779.7734\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 113695.0000\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 111955.2344\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 114819.6016\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 113354.6562\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 113340.9219\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 109905.3359\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 117214.2500\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 115092.3125\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 112508.1094\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 115442.7969\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 112772.6875\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 109714.9766\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 111558.2500\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 117830.4219\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 115058.0000\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 114407.1641\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 112560.0312\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 113210.6172\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 115884.7734\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 107178.5000\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 113470.0859\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 114205.3516\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 112178.6172\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 107975.0703\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 108176.8750\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 111661.1875\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 114427.3438\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 120643.8047\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 111201.2891\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 112741.8750\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 108574.5078\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 110354.6094\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 111580.4062\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 113298.7500\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 112765.5938\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 118161.5938\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 109643.6094\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 114272.1562\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 119100.3594\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 113851.5703\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 107209.0625\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 110613.3906\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 109913.0469\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 107858.0938\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 116285.1484\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 110770.5781\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 108778.5469\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 105455.0469\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 111726.1094\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 115972.4688\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 112665.3359\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 110876.8828\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 107577.2656\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 109113.2812\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 113579.3906\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 111330.1719\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 114867.0000\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 104679.3906\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 110867.3594\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 113817.1719\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 105584.7969\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 110114.6406\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 108253.5312\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 109793.1719\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 112433.2500\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 114723.2891\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 109067.0156\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 112261.4609\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 109919.2656\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 109209.1328\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 113169.4219\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 113460.5859\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 111429.2344\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 109488.2969\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 108787.3594\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 111918.3125\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 107937.3750\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 116052.7656\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 110506.3281\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 106555.5312\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 108394.6719\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 105279.7344\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 112709.9531\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 105066.6484\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 111115.0391\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 102142.6406\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 110927.7344\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 113784.7578\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 110680.4219\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 107992.8594\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 108968.4844\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 111296.6719\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 108333.2109\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 107504.8281\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 105453.6562\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 103597.0859\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 109791.2188\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 108491.3125\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 107342.8047\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 104509.8281\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 104890.8906\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 102722.7500\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 110707.8594\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 112755.4219\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 111332.1406\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 107476.1562\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 105639.4531\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 108665.5312\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 105947.7578\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 106387.1797\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 108578.3203\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 105298.6094\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 109881.1094\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 107511.6484\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 110084.7500\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 108455.1094\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 115259.1562\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 112725.3125\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 107656.5312\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 111426.9844\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 113304.9922\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 109024.4688\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 101992.6797\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 103961.0000\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 107613.7344\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 108303.9062\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 104961.5156\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 104501.7812\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 108556.7734\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 108197.0312\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 109174.6953\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 108880.4219\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 110285.6875\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 108386.9062\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 104313.7344\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 109857.9766\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 108537.5156\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 102553.0938\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 110775.4531\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 106602.2578\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 112408.7500\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 105739.7109\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 103354.3750\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 107177.4922\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 111005.9297\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 107278.7578\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 103098.4844\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 103469.0781\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 100743.7500\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 103479.9844\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 108205.1250\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 104109.1094\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 105240.9375\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 110776.9375\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 109187.9062\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 101351.6562\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 104336.4844\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 109918.2031\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 106453.4609\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 104083.7500\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 100972.4219\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 110458.5312\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 107546.4922\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 105532.5312\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 102618.5781\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 102268.5938\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 102793.8125\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 107980.4062\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 99697.8047\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 109429.2812\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 100448.5469\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 110553.5625\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 102857.2266\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 109846.6250\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 102224.5859\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 102894.5781\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 102531.3438\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 104744.6094\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 104822.5781\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 105559.9844\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 103740.2266\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 110840.3281\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 106478.3594\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 104632.3906\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 100278.0469\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 100203.0547\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 114390.4141\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 103074.5312\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 100231.7812\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 100164.3594\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 102515.8125\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 101669.4609\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 101323.8438\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 104372.1484\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 104076.3047\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 109259.7656\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 111360.3281\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 108165.1641\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 103981.4062\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 104222.0781\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 107643.4297\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 102067.4375\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 100175.8203\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 99583.4219\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 105267.5625\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 103255.0469\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 106871.9219\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 106229.8281\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 107324.0078\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 106570.0547\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 110495.4219\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 106460.8906\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 105981.8906\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 106289.9062\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 99719.7031\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 105748.2188\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 101320.2500\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 104560.7656\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 104404.0469\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 101099.9844\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 106070.3438\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 103368.2500\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 103994.4609\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 107057.2812\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 100463.7031\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 106072.8906\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 106069.5000\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 97182.1328\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 110301.3594\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 104500.8438\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 100986.1719\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 100111.9531\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 106230.9375\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 101225.9688\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 101148.0156\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 104835.8906\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 106653.3750\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 104543.9531\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 104179.5469\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 101225.1641\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 103575.6562\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 102829.9766\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 102804.1875\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 101226.5625\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 98874.6641\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 106159.8281\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 106056.3125\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 98090.9141\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 100429.4453\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 104506.7500\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 105656.8516\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 98516.7578\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 103938.0078\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 101968.9844\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 102974.0312\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 101596.6562\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 104511.4062\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 98972.2188\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 104991.0781\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 103092.2188\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 95203.4531\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 101475.1250\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 103850.3125\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 102107.7500\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 96800.7812\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 97708.4609\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 104136.7969\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 98463.3438\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 102836.5781\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 99929.6875\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 103939.2188\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 104872.9844\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 94011.6172\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 96588.4062\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 99923.6484\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 100655.4609\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 101181.0469\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 98425.4922\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 96885.6172\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 97087.7812\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 101701.8438\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 103429.3984\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 100976.7188\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 97868.6094\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 100740.8906\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 102215.1562\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 101966.0938\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 105048.3828\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 100311.9141\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 105784.1797\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 100110.6719\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 96042.2812\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 105524.5781\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 102984.5625\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 98025.8359\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 96656.5078\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 103174.5703\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 97972.4688\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 99296.3125\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 102920.9375\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 98012.2188\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 95356.6484\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 97993.5000\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 100689.7656\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 101177.5000\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 101577.6406\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 102926.9688\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 102230.6719\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 103193.2969\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 99735.9141\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 100486.6719\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 102176.1562\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 93405.3281\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 100520.4688\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 102356.0156\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 99076.8594\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 96732.1094\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 98932.9219\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 102444.2188\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 98218.8438\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 95335.7109\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 99935.9219\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 100300.7891\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 94758.9531\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 99337.4844\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 100970.5234\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 95038.3359\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 102520.6094\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 101182.4844\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 103076.6641\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 97692.7734\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 96975.0703\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 96819.8203\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 102002.2266\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 98051.1562\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 96018.9531\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 99375.3984\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 98873.8750\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 96602.5469\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 93663.4688\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 102027.0312\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 95705.7734\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 93095.6641\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 96733.8359\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 95875.9219\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 96930.4219\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 95420.1406\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 91866.8594\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 101029.0312\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 99562.8828\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 96541.7656\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 96875.3359\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 101615.5703\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 95024.5625\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 96404.1250\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 97040.2109\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 98098.9844\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 100270.6094\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 101304.6562\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 95475.7656\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 94882.1328\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 97555.9688\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 94451.0781\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 101890.2109\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 94108.0312\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 98214.0156\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 100015.4844\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 94978.1016\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 97294.2734\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 97000.0391\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 94032.9844\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 100522.6016\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 98092.5000\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 97581.8281\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 94173.4297\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 96530.2344\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 101453.1797\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 96008.5781\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 102868.9062\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 97622.3281\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 99364.3594\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 97374.5938\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 100573.5781\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 94457.7969\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 96617.9141\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 95351.1719\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 99228.1406\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 99184.4531\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 100389.1562\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 100031.0547\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 95143.9375\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 96002.1641\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 93788.4141\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 99157.4844\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 95334.8281\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 94004.9062\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 92797.1953\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 96300.9062\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 100028.1875\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 98973.3125\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 96201.8203\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 92685.9375\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 94444.0312\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 94601.9453\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 92554.4219\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 94173.6172\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 92864.4531\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 95860.0312\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 97580.8984\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 93457.8750\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 91486.2500\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 99398.7031\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 92279.1875\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 97066.1562\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 93723.7969\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 97100.7812\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 93019.3828\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 95606.3281\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 96489.1328\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 97639.0938\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 93855.5781\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 93602.5938\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 96594.4453\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 96322.4062\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 97032.9062\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 94197.3516\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 94464.9141\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 92746.5938\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 94287.3594\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 92724.5781\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 94178.2344\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 92585.4375\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 94917.8516\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 95301.8672\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 96494.9688\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 92118.0156\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 92180.5000\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 95041.2344\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 94625.7188\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 92758.0938\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 96413.0234\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 90683.4844\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 95623.9609\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 94217.7500\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 92004.2188\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 97920.5156\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 88650.5312\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 91690.9609\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 90402.8125\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 93751.0234\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 96090.8047\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 89566.4844\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 95730.3906\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 90469.5625\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 93960.6094\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 94621.0312\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 91101.7344\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 92707.7969\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 93318.8750\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 95499.7109\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 93661.1719\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 89766.0312\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 90611.9844\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 93443.6328\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 95111.9609\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 96612.9688\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 89836.3750\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 89744.4297\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 93930.5000\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 90760.0312\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 93987.6953\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 88047.7188\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 91821.6250\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 93846.0156\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 94964.7500\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 95812.4688\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 93388.7109\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 88869.1719\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 94360.9141\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 93332.9531\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 92143.4766\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 94144.5469\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 87938.7031\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 94289.1328\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 95456.9688\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 92596.1875\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 88702.3828\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 90286.1875\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 91620.6875\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 91987.6484\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 89421.0234\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 95053.0156\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 91532.3672\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 89168.8516\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 91280.3359\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 93803.5312\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 97897.1719\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 91032.0312\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 92255.2031\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 98411.2578\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 93088.9766\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 89932.1094\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 89514.0391\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 87067.7422\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 92988.3828\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 92601.8594\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 88288.2578\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 94471.5000\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 91187.1953\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 90797.7344\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 92582.7656\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 93813.0000\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 87615.7891\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 91827.9844\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 94272.5547\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 90438.7500\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 91390.3750\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 92258.0000\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 90111.3906\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 86483.4062\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 87835.4688\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 88322.0391\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 85309.6250\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 93370.6094\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 88036.4922\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 88778.1562\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 92176.0859\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 89834.7578\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 86593.3906\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 85586.0625\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 90905.5156\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 90122.9375\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 89834.7656\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 94479.5469\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 90363.1641\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 92685.1719\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 91220.7812\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 94409.7500\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 87864.5469\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 86848.3047\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 91573.9375\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 96330.9453\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 91859.1562\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 86946.4453\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 88708.1406\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 93921.0156\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 90611.0000\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 89917.9375\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 93132.6875\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 87762.5469\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 87469.8281\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 90866.0156\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 94944.4531\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 93383.5312\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 91827.7266\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 88590.4766\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 87719.0625\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 90105.3203\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 90560.6094\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 86898.5938\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 88739.8047\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 86274.8281\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 87171.8125\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 88925.1250\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 93509.9062\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 86306.2422\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 88341.3906\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 91714.3594\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 90350.3906\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 88699.6484\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 89677.7656\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 85909.9375\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 86144.6719\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 90149.5234\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 91344.5703\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 90797.7500\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 91751.5000\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 90936.6094\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 87924.8594\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 89105.0000\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 86389.1875\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 93948.4531\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 90220.1719\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 90440.5938\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 91187.0703\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 83223.2266\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 90328.9766\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 87591.8594\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 85999.8984\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 91916.1406\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 89618.1875\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 86379.7969\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 88906.5859\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 88346.5312\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 88426.4219\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 89295.5156\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 89766.4844\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 88087.5312\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 88860.2188\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 85577.5156\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 85162.6094\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 85188.5625\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 84310.9062\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 87641.4297\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 84032.9766\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 88947.1562\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 86398.5156\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 88141.2969\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 87768.4531\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 85593.7188\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 91506.7578\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 89676.0938\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 88421.9766\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 89642.3828\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 88758.7656\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 84604.5469\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 92887.9922\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 86424.5938\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 88367.4766\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 91130.8594\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 86652.9141\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 83776.6250\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 91206.9141\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 83355.3906\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 87302.8047\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 84673.5859\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 90997.4141\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 86542.2969\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 90870.2500\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 87286.5938\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 84613.6406\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 82470.2969\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 84738.3359\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 85982.3906\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 92694.1797\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 88281.9688\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 88399.9531\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 85920.1250\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 86964.5000\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 91518.5078\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 85539.6094\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 87149.3906\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 88740.7969\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 84224.6562\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 81881.2969\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 85274.2188\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 90100.0000\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 86157.8906\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 83937.1875\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 90036.7656\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 84038.5625\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 83207.1172\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 88305.7109\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 81795.5469\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 88207.5156\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 84333.6250\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 86893.0469\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 86188.0703\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 85291.0000\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 84959.4531\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 81786.2109\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 90594.1641\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 89760.1094\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 87066.6406\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 88046.3594\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 85079.5391\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 83495.4844\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 81463.3281\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 84236.6875\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 85101.7344\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 86377.2500\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 87313.6094\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 85590.7578\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 82645.8281\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 84752.3594\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 81824.7031\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 87775.8906\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 89970.6484\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 87976.3594\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 83064.0781\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 85382.4453\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 86354.9062\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 83301.5312\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 88062.5156\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 82919.8984\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 89956.9062\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 86985.4375\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 85407.9219\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 87021.7969\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 85125.7344\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 83529.9219\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 82483.4531\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 83882.1641\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 85799.6406\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 85615.4609\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 83296.2812\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 82330.1484\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 82405.8203\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 90023.3281\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 81955.1562\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 88348.8750\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 82101.1875\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 83895.1172\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 81795.6328\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 80496.3750\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 82410.2969\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 82742.1250\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 85992.2656\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 85059.2656\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 88585.4453\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 84126.5156\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 79735.5859\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 86343.0781\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 82457.1875\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 83554.2031\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 88549.9141\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 87577.1719\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 84767.7656\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 84630.9531\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 85560.8281\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 86892.3281\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 85079.7891\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 81357.3438\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 85020.7031\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 85080.6562\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 82305.9922\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 80051.0938\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 80416.8750\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 81518.3984\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 83790.3359\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 84941.6016\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 82242.3750\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 80323.6641\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 81602.4375\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 84556.3438\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 81833.1484\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 80010.4453\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 85012.4297\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 84012.4844\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 86652.6797\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 82564.6641\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 82853.2422\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 82360.0625\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 81142.8281\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 81534.9375\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 86231.6250\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 83385.7656\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 84819.1875\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 84880.4062\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 85269.3125\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 84548.1406\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 82661.4844\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 83611.5391\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 81069.3672\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 81560.4531\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 81282.6641\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 79828.5469\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 86094.1250\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 84436.7109\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 79674.5781\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 84620.7500\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 78854.6094\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 85624.5938\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 80125.0703\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 79372.6016\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 81469.8281\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 81693.7812\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 81340.3438\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 82183.3984\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 82791.0781\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 84461.6172\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 81032.1719\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 85300.5859\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 79810.2656\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 83467.6094\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 81957.6172\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 83215.2969\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 84898.6094\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 84800.8984\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 81998.5625\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 80457.9531\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 77517.5938\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 80445.0938\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 86144.5469\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 78188.5078\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 79723.0625\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 82093.5859\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 84064.9922\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 83462.3438\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 83319.9531\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 79253.3750\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 84116.5000\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 80137.0312\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 79642.0469\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 80559.1953\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 81226.1719\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 82204.9531\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 77700.6875\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 79084.1719\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 78896.7656\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 81015.8047\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 81969.8906\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 81256.4375\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 82156.9531\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 82668.3828\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 76000.7109\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 81078.2500\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 77885.8438\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 82932.7891\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 82044.3750\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 78163.3750\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 82882.5234\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 82416.5156\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 81592.0938\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 83263.3906\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 80913.7500\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 82145.3203\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 78444.3516\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 78698.9766\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 82411.5625\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 82423.6094\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 80193.9219\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 79041.4297\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 81045.1953\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 78428.6719\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 77686.6016\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 80600.8281\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 85423.5938\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 82666.7344\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 79800.9141\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 79773.3125\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 82395.7344\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 80125.4766\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 83303.1328\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 81369.1328\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 79376.4062\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 79131.8906\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 80423.7188\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 78646.6875\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 80246.7188\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 79562.0234\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 84481.8906\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 79045.6953\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 80199.4531\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 75401.8516\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 81971.9844\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 77503.1719\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 81516.3125\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 77143.4375\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 80010.4219\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 78198.3281\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 75643.0625\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 75389.6094\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 82466.2422\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 77387.5078\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 76478.0000\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 81242.3750\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 75795.9375\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 80960.9609\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 81432.6406\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 82852.6328\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 78719.8203\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 76576.9844\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 80182.4609\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 81332.9062\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 74966.4375\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 77014.2812\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 79036.0625\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 77940.6328\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 77760.1562\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 77477.7344\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 78856.7656\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 75257.3438\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 79772.2969\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 78812.7656\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 79029.0625\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 80475.5625\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 81771.0391\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 81770.5469\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 80126.3594\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 75578.2734\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 79861.5469\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 75681.8438\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 80397.7266\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 81050.9453\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 81962.9531\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 73854.2578\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 78796.1719\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 79291.4375\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 79740.2656\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 73360.3047\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 76218.4141\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 75526.8750\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 77147.3672\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 80600.2344\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 78122.8750\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 75471.4531\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 82444.7656\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 78198.2812\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 79504.5312\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 75113.6094\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 82638.6250\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 77537.9688\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 78399.9062\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 75331.7031\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 79499.5859\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 73495.0547\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 77575.4531\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 76746.0312\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 74649.5547\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 79034.3984\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 81931.6094\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 80655.0078\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 77191.6016\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 79022.7188\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 73452.3438\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 76417.5859\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 75509.8906\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 80370.6719\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 78248.5938\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 73674.4844\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 76631.1641\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 75199.2969\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 74277.2344\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 77881.3906\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 76255.1562\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 73618.1094\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 78078.0859\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 78464.5547\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 79120.8047\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 79225.6484\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 76938.5547\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 75995.1250\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 74942.0312\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 72294.3125\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 70662.8984\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 77612.0078\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 77615.8672\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 76148.5938\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 75984.2891\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 75940.5312\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 78805.6641\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 78407.9531\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 75346.2969\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 78343.5625\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 75286.2656\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 73783.7891\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 79134.0938\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 78203.6094\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 71927.5469\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 76914.1250\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 77433.1875\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 76843.7500\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 73232.6875\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 79661.0000\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 74929.0312\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 72155.3281\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 75199.2344\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 76859.0625\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 74658.8594\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 76196.4844\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 75820.2969\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 79401.5703\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 79281.0781\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 73292.6875\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 72965.4062\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 77905.9062\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 77613.9297\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 74255.0391\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 79258.9297\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 73370.3281\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 74920.7031\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 76168.2969\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 73270.7266\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 75051.6406\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 75374.0938\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 71384.7266\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 76059.1094\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 71796.8281\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 75514.0234\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 76907.1719\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 74160.0625\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 82097.7891\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 73904.7812\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 71598.5781\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 76326.0234\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 78051.1172\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 75226.9688\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 73961.0859\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 73972.6406\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 73804.9453\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 75018.8203\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 76266.4844\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 73151.7969\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 74395.4141\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 77067.3125\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 74626.1562\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 74726.4219\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 78223.3594\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 76692.6406\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 77156.8906\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 75048.8594\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 76234.7500\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 76376.5547\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 74957.8281\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 75152.5469\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 71637.4844\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 73482.7031\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 69946.7031\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 77566.4844\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 75063.5469\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 74839.3672\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 72276.4141\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 75938.2500\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 74596.7656\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 76559.3438\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 75407.3828\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 75047.6328\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 72166.9688\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 75263.1719\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 73144.7031\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 71885.2109\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 73784.1719\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 75534.1094\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 71988.0469\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 72841.4531\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 76696.6641\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 74841.2969\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 74519.3594\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 73547.1484\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 79068.6094\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 74174.6875\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 72420.3281\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 75716.3047\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 73232.7344\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 75063.8906\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 73112.9688\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 72040.3125\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 78001.3281\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 71788.2344\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 78101.6953\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 73676.2031\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 72240.6562\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 74666.7969\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 76543.9453\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 69807.4375\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 75092.6562\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 72771.6562\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 72776.0625\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 75732.4688\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 77065.4062\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 73422.1406\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 75947.3125\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 74835.0469\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 73443.0547\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 73301.6172\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 74751.0938\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 73530.8359\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 70693.3594\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 71709.6562\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 71670.8438\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 72141.4141\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 75512.6406\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 67502.3750\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 71185.6719\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 69263.1719\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 74555.9062\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 75083.4297\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 71912.2969\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 74059.7344\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 70960.9219\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 74429.4141\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 72033.7031\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 70601.1172\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 71985.4297\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 70210.1484\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 72687.9375\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 75179.5391\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 70706.0234\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 73703.0625\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 73201.8203\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 77096.0391\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 76866.0312\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 75644.1250\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 73444.8906\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 70309.5938\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 68383.3906\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 74728.3281\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 70879.6953\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 68646.0156\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 70161.4531\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 71962.3672\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 70932.8125\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 70237.5156\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 72678.1406\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 71805.6094\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 69379.4062\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 67047.6016\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 71611.0547\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 69873.0000\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 71013.8984\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 70111.3203\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 75440.4844\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 71484.4219\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 69168.2109\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 73200.1797\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 69469.1016\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 71445.6641\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 73276.1328\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 73013.9453\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 70851.8672\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 70070.4688\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 72126.2969\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 71971.9062\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 70827.1797\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 69187.8750\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 71690.0156\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 72453.9375\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 71560.8438\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 70488.8438\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 71519.6875\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 72679.1016\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 71929.5469\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 71485.5469\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 73169.8438\n",
      "âœ… Epoch 2 Avg Loss: 93562.6494\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[499 621]\n",
      " [618 495]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4458\n",
      "\n",
      "ğŸ” Epoch 3/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 69235.7656\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 71001.1953\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 71604.4453\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 65996.4062\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 70559.5234\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 75173.7734\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 72435.8359\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 74737.6875\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 67451.7656\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 68908.1328\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 68676.1562\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 71798.1641\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 72039.2031\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 69812.1719\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 68638.3047\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 70624.7422\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 70319.7578\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 71054.2500\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 71798.0156\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 73294.6484\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 67885.1875\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 71265.5859\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 70742.1094\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 67331.4375\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 68943.5469\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 71746.3203\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 73376.8906\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 68814.7656\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 72785.2500\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 69627.0938\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 70555.0859\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 71656.1406\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 69260.4844\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 70787.4688\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 73217.1719\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 69699.0547\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 68543.0391\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 71332.8906\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 70662.7969\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 67789.6641\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 68194.6094\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 71380.7031\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 70421.6328\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 67556.8750\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 71543.1953\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 70467.4531\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 68743.9297\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 69365.5781\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 69124.6562\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 67362.7969\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 66170.8438\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 70207.9453\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 72222.1641\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 67470.0547\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 72747.2500\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 68829.7344\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 70933.4609\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 70918.0938\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 71948.8047\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 70197.4062\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 70424.7422\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 69860.0469\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 66575.0781\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 72033.0000\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 67899.5234\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 70187.5859\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 67838.2109\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 65676.8125\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 70637.3047\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 70487.1641\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 70656.5781\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 69162.4922\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 70243.0312\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 69222.4844\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 68051.5469\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 72571.5938\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 66201.5781\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 69886.9141\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 67267.9375\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 71088.5938\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 70301.7812\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 69312.2812\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 70086.7031\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 66881.6797\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 69803.7812\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 68203.4297\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 65978.5156\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 67931.4297\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 69181.4219\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 68889.5078\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 71758.3906\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 67912.9219\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 70015.3906\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 69829.0938\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 69307.9688\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 68054.3438\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 68661.1875\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 68827.4453\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 64630.5820\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 70357.6094\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 73584.2422\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 65504.0703\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 67956.1094\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 69245.4219\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 67591.0078\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 67686.5547\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 64833.1953\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 64387.3906\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 68036.2500\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 66387.3438\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 67093.4375\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 67257.1094\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 68175.2734\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 67653.8594\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 72848.1641\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 67303.5469\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 68459.7656\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 67384.8906\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 70584.0078\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 66493.4219\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 66989.4453\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 64199.1328\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 66336.9219\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 68957.2969\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 64804.9375\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 67653.4453\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 69467.7656\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 65597.8594\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 69391.1562\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 65238.3359\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 69467.0000\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 66408.4297\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 64901.4922\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 69810.6484\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 66591.8984\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 67536.6250\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 66887.9062\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 66097.1875\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 67165.7188\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 69810.2812\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 67409.0859\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 66658.9219\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 67253.5625\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 65000.8086\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 71542.3359\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 64842.7344\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 66953.7969\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 69715.8750\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 69199.0781\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 68875.2812\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 70385.4062\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 68352.9297\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 65304.0547\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 64157.9727\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 66316.7344\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 71251.8594\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 67416.6875\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 69294.3906\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 69369.8672\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 64441.4922\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 65083.1484\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 63670.9609\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 65923.9688\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 70915.5625\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 67258.2344\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 70883.3828\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 63501.1719\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 65677.9531\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 67779.5000\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 64905.2227\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 66602.8281\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 67434.1562\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 66707.2188\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 69212.7031\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 67204.0469\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 65541.7812\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 66732.7656\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 69579.6016\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 69250.6953\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 64840.7070\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 65912.1094\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 64568.1328\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 66802.3594\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 67019.0938\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 69023.2891\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 66435.9219\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 69096.5000\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 66511.9297\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 66045.7344\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 67602.4375\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 64761.5000\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 62727.4062\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 63906.7695\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 66454.6172\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 64704.8945\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 63324.3203\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 66098.0312\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 63190.7812\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 67904.4688\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 67104.4219\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 65231.8281\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 67665.6797\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 64813.5547\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 64968.4062\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 65681.9688\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 67594.0859\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 65384.7734\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 64902.7969\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 67516.9922\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 70260.6562\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 64515.1875\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 66540.3281\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 62303.5742\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 64092.0625\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 64117.5977\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 64351.4297\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 64041.3984\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 62193.3945\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 64933.4961\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 63409.6602\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 67799.3438\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 66904.2031\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 62198.8477\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 66293.0312\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 65080.1641\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 65597.8281\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 62913.4688\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 64851.0352\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 67181.9219\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 66632.5312\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 61954.3281\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 66673.4531\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 67960.8281\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 62679.1484\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 63984.3438\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 63693.8086\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 62492.6602\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 64624.5547\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 68521.6719\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 64827.5312\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 65312.5430\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 64091.1641\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 65102.2930\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 65826.4219\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 63066.5469\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 65536.8281\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 66401.4219\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 65156.3672\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 65696.5938\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 63912.1133\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 65044.4805\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 66980.7812\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 67955.9922\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 63704.7734\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 63616.9062\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 63210.0625\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 62853.7812\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 64942.2109\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 63576.6758\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 64705.2109\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 59929.4375\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 64382.9648\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 63717.1914\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 66031.8750\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 61990.0234\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 62580.7930\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 67240.8125\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 61875.3203\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 62283.9492\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 64357.5859\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 66048.4219\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 61872.8477\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 68093.6875\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 64768.3281\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 62171.6602\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 64651.5938\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 64575.9453\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 65680.3594\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 60476.6562\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 62442.0859\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 68051.2969\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 60760.6289\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 62811.6523\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 64392.9258\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 61569.4453\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 63965.7344\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 62448.0000\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 62740.2578\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 65883.8125\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 63221.9922\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 65475.1133\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 62130.3750\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 62238.8203\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 64577.6875\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 61697.9922\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 60671.7383\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 63330.7227\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 62993.5156\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 59585.1641\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 65279.7305\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 64021.2109\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 65269.1914\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 65184.6484\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 65418.5000\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 62865.8906\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 63284.2812\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 65636.3125\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 61200.0469\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 61610.9844\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 63655.6289\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 61691.9141\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 63404.0078\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 64253.3281\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 62205.8477\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 65400.4844\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 64045.3164\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 63743.8242\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 62080.2109\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 62263.0156\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 61234.0781\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 62495.7969\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 64894.7773\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 64039.9297\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 64455.7344\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 65726.0156\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 65955.7812\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 63065.3047\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 61141.3711\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 63816.3359\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 59887.9414\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 61175.0000\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 62328.0156\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 62243.5234\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 61423.5469\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 62193.8047\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 59259.7070\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 59939.7266\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 62112.4844\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 63400.0977\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 63242.2734\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 60758.8398\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 61637.2969\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 61096.6562\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 64085.7539\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 60210.1406\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 61207.9102\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 62012.0742\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 61893.9375\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 60438.9141\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 61382.4023\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 59766.6953\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 61731.6484\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 61805.7734\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 61010.3594\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 59173.5391\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 62495.5352\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 60138.4297\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 61218.1172\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 64158.6953\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 61313.1406\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 59369.9688\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 59333.7031\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 60561.1719\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 65557.0938\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 63637.0664\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 62787.0000\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 59259.8789\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 60060.0430\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 61428.3906\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 61583.8047\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 60780.7773\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 62376.3438\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 64121.1445\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 61021.6836\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 61966.0547\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 61082.8125\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 61687.0664\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 63882.7461\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 58601.2578\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 59270.9648\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 65166.2031\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 62260.7656\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 63706.6406\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 64321.6797\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 60125.9961\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 65392.2422\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 62529.0078\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 60177.5156\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 63405.3828\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 61336.9805\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 60825.9375\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 64108.6172\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 58282.1641\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 60877.1797\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 64211.3672\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 60526.8984\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 59776.3984\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 64273.7891\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 58675.3047\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 62278.5078\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 61111.5977\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 63507.3438\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 62354.1641\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 60848.2734\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 62650.9922\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 60661.6523\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 63490.1250\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 58925.1406\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 59518.2734\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 60585.8242\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 58026.4531\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 62226.9453\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 61104.7734\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 62142.9297\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 56848.4688\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 60879.2227\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 57908.5703\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 61560.8281\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 59792.1875\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 61982.9922\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 63309.9922\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 63726.8906\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 62006.7852\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 59689.0039\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 63872.9141\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 61123.3281\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 61704.9844\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 61384.3672\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 61264.2930\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 62593.8828\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 58518.8984\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 60847.6953\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 60414.1992\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 58212.5312\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 58674.4922\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 60065.9141\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 58231.9297\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 57835.6250\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 57672.5000\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 57204.6367\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 58665.9531\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 59185.9023\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 60244.9570\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 62309.8828\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 58010.2734\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 61372.2344\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 62522.5430\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 58052.3828\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 58567.8945\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 59044.9336\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 61127.9531\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 57867.3516\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 59756.3672\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 61994.3281\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 64811.1016\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 57799.8281\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 57665.4102\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 58429.0000\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 61193.5078\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 60424.4180\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 60091.1875\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 59368.0039\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 57177.8438\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 58003.0391\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 60223.7500\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 61057.0547\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 59216.1797\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 61154.8320\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 58646.1016\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 59904.0781\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 58240.7344\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 57916.3750\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 58566.3984\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 58933.4375\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 58451.4922\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 58432.5156\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 57057.5156\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 60702.6562\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 58410.8164\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 54723.4844\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 60752.5859\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 56998.1055\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 59904.9766\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 61159.6328\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 59579.0234\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 57924.6641\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 60719.2266\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 55062.8047\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 56709.1953\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 60336.9297\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 56815.7109\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 58160.9023\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 60712.8008\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 59730.3750\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 56464.4102\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 59119.4453\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 58288.8203\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 58550.9609\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 61018.6797\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 57865.9062\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 61400.5078\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 58429.3125\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 59850.7227\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 59888.0039\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 57606.9141\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 60016.4375\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 57848.6250\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 57398.7422\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 59073.9492\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 58513.4609\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 56462.1719\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 57984.7031\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 56525.3594\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 57610.4609\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 61222.5312\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 59500.5352\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 57352.3398\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 57587.0234\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 60122.3281\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 61247.7188\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 59371.4258\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 61720.4180\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 60826.7578\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 59169.2188\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 55411.7266\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 57649.0391\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 54825.1055\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 58827.5078\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 56980.9570\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 58639.1406\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 56172.0234\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 59850.4766\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 53213.5156\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 60890.3242\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 55756.4062\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 60228.7461\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 60545.5625\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 57982.7422\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 58080.2656\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 60938.2617\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 59462.8594\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 59675.3203\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 56486.2422\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 54705.0391\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 59035.4531\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 60269.0469\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 56796.3359\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 58091.1562\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 55680.4297\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 58797.3984\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 56900.9062\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 55884.6172\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 59427.3750\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 55684.3516\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 56523.1289\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 57136.1797\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 61293.2031\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 57543.5547\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 57159.4375\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 55733.3125\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 58088.6172\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 57219.7422\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 57392.9766\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 54576.8984\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 53500.1680\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 59302.1875\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 57385.1875\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 54859.1875\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 60419.2188\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 58206.1562\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 57039.2461\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 55595.6641\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 54420.4453\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 55152.7539\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 59917.2773\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 57393.6016\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 59750.2656\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 58374.9062\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 55413.2812\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 56616.7969\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 54523.0625\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 59101.5234\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 57289.7305\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 54661.8711\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 54430.3398\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 56298.1719\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 53754.9844\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 55059.3125\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 55015.6211\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 56854.1875\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 56288.8164\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 57460.7852\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 58988.3242\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 56591.4805\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 56312.0430\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 55867.2617\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 58274.3828\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 53691.9297\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 57872.2109\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 58242.9805\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 57820.7148\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 57147.5781\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 55804.1602\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 55163.8516\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 54206.3906\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 57488.7578\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 57886.5977\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 55125.1094\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 54936.1602\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 55962.8789\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 56501.1133\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 55509.4375\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 56348.4062\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 58663.5508\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 55287.9531\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 53907.8672\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 59414.2031\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 58983.4141\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 55109.4297\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 54572.5781\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 54390.1836\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 57005.5000\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 54184.2578\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 53874.2500\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 55239.0156\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 57288.2500\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 56451.7422\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 56587.9297\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 58246.6562\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 54473.2656\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 57485.6641\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 55737.1406\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 55545.2734\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 57910.2578\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 51912.4492\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 56204.4453\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 56824.3828\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 55380.7031\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 55365.3672\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 57809.5430\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 57993.4766\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 53215.3125\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 53879.2852\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 55881.8672\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 54884.6328\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 58703.2617\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 55788.0938\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 57225.5312\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 53990.8516\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 55319.4141\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 53653.7812\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 56378.7188\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 51235.0625\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 58273.7109\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 53080.6680\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 54749.6484\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 54994.2383\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 54011.9609\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 56080.9219\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 53358.9609\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 56028.0312\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 54227.8789\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 57091.3359\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 53878.4453\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 55502.5781\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 55402.5156\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 55181.0078\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 54548.8125\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 54628.1992\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 54998.7031\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 56891.8750\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 55739.1953\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 56200.1523\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 54588.7852\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 53873.0195\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 54397.2812\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 55006.1250\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 56679.1719\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 52957.1172\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 53445.5078\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 56739.9961\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 53904.6328\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 54719.4453\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 53869.7266\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 57518.5312\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 57093.9297\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 55779.5625\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 52714.0547\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 56131.5977\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 55926.9492\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 55527.0391\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 54715.4141\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 52793.4805\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 51677.8828\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 53200.8242\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 55045.5859\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 54096.9609\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 53744.3398\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 53726.5625\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 53589.5508\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 57295.0000\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 55706.7891\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 53881.4609\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 55802.4219\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 52926.1680\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 54386.0195\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 51417.4219\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 54497.6914\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 56041.1641\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 52442.4922\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 52979.1016\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 53785.5625\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 57049.1328\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 53545.3867\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 55007.9062\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 53020.3594\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 53149.6562\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 53021.1484\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 52324.6406\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 53210.8242\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 53612.7109\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 54216.4570\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 53128.7305\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 54842.1016\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 52785.5938\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 54263.0938\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 51546.4141\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 54059.9453\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 55585.5781\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 55144.3789\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 55342.6016\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 54710.7344\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 54306.1953\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 56420.8867\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 53040.9414\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 53680.7305\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 56306.4180\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 52689.8906\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 54118.9766\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 53701.1406\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 54885.5859\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 53492.6484\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 51360.9961\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 54505.7656\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 53900.7852\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 55296.7266\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 52447.9375\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 50686.0234\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 52861.3125\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 53191.2109\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 55023.7266\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 54438.5742\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 50026.1406\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 54393.5117\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 51037.5977\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 50956.8008\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 55908.5000\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 53235.5625\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 53137.7344\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 53227.5078\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 53745.9375\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 54435.6094\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 51369.3281\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 55019.7656\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 53221.0508\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 54738.6211\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 54541.1289\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 54719.8750\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 52849.1094\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 51060.9609\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 51662.2812\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 51799.6406\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 53346.5664\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 53850.9375\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 53758.0078\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 53422.6406\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 52266.0391\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 53505.7812\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 52739.9922\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 53672.2266\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 52751.1406\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 53009.7812\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 55936.4688\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 53426.3906\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 52777.9727\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 52687.0859\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 53544.3594\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 50939.5781\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 53728.2188\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 52162.8594\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 52857.1914\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 52811.9688\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 53097.7891\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 54951.8945\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 51722.9336\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 54411.6094\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 52005.3828\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 50674.6562\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 52854.5391\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 51449.1797\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 50268.1016\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 51590.9062\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 53902.8086\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 52276.8672\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 51626.3281\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 53065.5938\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 53384.3203\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 52650.3281\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 51137.5859\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 50829.7266\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 52807.7266\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 50987.9219\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 50866.5312\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 51279.3203\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 51412.4141\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 52647.7070\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 53484.3672\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 53859.7500\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 51160.5820\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 49912.1289\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 54053.4766\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 51035.7891\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 50646.9766\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 54438.8359\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 51223.1094\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 49961.5703\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 50952.2812\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 52546.7695\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 52445.6250\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 48866.9766\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 51869.3984\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 53502.9297\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 51705.7070\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 51449.1250\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 48742.0703\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 52644.2148\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 53092.4453\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 52799.0703\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 49567.8047\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 51459.7891\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 50975.4062\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 52048.3672\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 51684.6914\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 52763.9180\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 50588.4766\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 51892.7539\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 51782.4609\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 51503.8242\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 49406.8750\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 51056.4414\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 51420.9766\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 53032.4453\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 51207.9102\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 52032.8633\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 48900.8203\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 53507.9062\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 53051.3164\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 50543.0156\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 52133.0547\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 50969.3438\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 50381.9141\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 50555.2773\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 49901.1641\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 53285.0000\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 49905.6719\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 52098.0391\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 50044.3203\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 54915.8359\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 50436.8008\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 49033.5000\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 48052.1992\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 51791.7305\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 49384.7969\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 52008.4531\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 50722.8828\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 49865.1328\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 49521.9844\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 52570.7227\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 50126.6406\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 50600.5898\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 52781.5547\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 51057.9844\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 49730.6523\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 50491.7500\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 49549.0781\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 47286.9336\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 48036.6914\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 51570.2695\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 48593.5000\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 52710.6719\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 53182.6211\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 50816.9141\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 53839.8672\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 51693.2656\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 50726.7500\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 49689.5352\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 52030.9922\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 48405.5820\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 50753.0938\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 51118.8711\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 49622.2500\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 49450.3359\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 52235.8516\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 51804.7617\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 50352.5625\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 51280.5391\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 51604.2930\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 49745.8320\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 49751.5312\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 49407.3594\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 52060.6836\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 50022.9922\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 52326.5781\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 48486.4688\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 49610.4883\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 50976.2734\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 52515.1602\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 50082.7578\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 49263.6914\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 49313.7461\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 47826.4922\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 52181.1719\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 49903.0000\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 50203.6797\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 50806.1367\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 49055.7109\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 51947.0781\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 51195.5000\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 48049.7578\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 48488.8359\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 48371.4219\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 49310.6836\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 48974.4062\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 50562.8984\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 50415.3906\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 48558.2031\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 49680.0430\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 52335.0000\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 51200.7891\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 48913.4648\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 49642.0391\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 50016.0977\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 48505.0391\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 47533.9453\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 50059.6172\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 49793.8828\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 47507.3672\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 49522.4609\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 51718.0312\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 47559.4062\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 47974.2695\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 47209.1680\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 51213.8516\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 54157.0000\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 47968.4844\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 49703.1797\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 48438.8438\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 48662.7422\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 47128.0625\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 49718.0195\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 49067.2812\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 49768.1797\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 45920.5078\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 49113.9336\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 47886.4141\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 51385.7031\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 48617.7109\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 48140.6094\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 52192.0234\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 47481.5547\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 48299.6094\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 46514.5938\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 49668.9062\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 50281.0039\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 48882.4062\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 47146.0430\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 49133.2031\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 51019.6680\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 49436.8789\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 49544.1328\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 50461.9102\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 48977.0156\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 48293.8672\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 48404.7109\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 49776.6758\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 47772.0391\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 49293.0117\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 45546.5586\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 48982.1328\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 49674.0586\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 48682.3125\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 45268.1484\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 47731.2578\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 50742.9336\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 48432.6641\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 47497.8086\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 49453.8906\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 48187.1094\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 46542.1094\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 49153.8438\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 47044.3711\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 49639.3164\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 48999.4609\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 47956.8008\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 45021.3945\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 47930.7344\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 46093.1641\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 47885.5391\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 47694.0859\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 47693.2656\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 47292.6016\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 49951.1172\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 49643.0898\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 46453.7266\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 49946.9453\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 47863.4258\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 47354.1562\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 50127.8320\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 48491.5234\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 45613.1641\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 47524.7734\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 50347.7070\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 49959.3750\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 48052.7422\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 47540.7891\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 49334.4883\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 46010.3203\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 49121.7188\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 49950.9883\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 47898.2969\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 47107.6797\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 44503.2305\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 46695.8711\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 49337.3750\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 49155.0898\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 49326.1289\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 46627.4688\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 48919.6641\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 49177.1094\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 47572.2109\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 49251.8516\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 46682.1680\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 46177.5352\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 48198.8164\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 46279.3867\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 46145.7812\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 47383.2383\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 46664.1172\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 46245.9688\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 47285.9922\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 47362.8242\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 46902.4297\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 46889.8359\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 45958.1094\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 46545.2734\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 50016.2148\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 46442.1250\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 48350.0312\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 45712.1836\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 47878.7188\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 45052.2656\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 47947.5234\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 47555.9922\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 47737.0508\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 49199.2461\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 47525.5898\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 48447.8828\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 50176.9297\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 48311.7969\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 45931.3320\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 49149.4297\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 49019.0547\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 45435.8164\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 48412.2812\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 48548.6094\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 46148.3125\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 46224.9023\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 49673.3945\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 49046.2812\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 49147.7188\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 46270.1484\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 44850.3320\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 46383.2891\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 45985.0078\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 47411.4844\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 47563.6406\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 46491.2734\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 47927.9922\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 45451.1016\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 46593.7109\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 45728.5078\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 45558.0898\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 44104.3867\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 45831.1562\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 47520.9258\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 45557.5508\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 45080.7734\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 48781.2734\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 45587.3789\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 48492.6016\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 46882.2500\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 45356.0625\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 46598.0859\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 46558.5703\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 48211.5078\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 48155.8828\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 45366.4297\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 47243.4062\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 47315.4023\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 46418.3672\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 47027.6406\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 45183.5234\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 46199.3281\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 45314.1797\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 48307.8789\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 46902.0547\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 45868.0273\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 47366.6484\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 46511.2891\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 48591.1953\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 45075.1133\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 46513.9258\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 48837.2344\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 45995.9805\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 46975.8906\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 48800.0469\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 47899.0781\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 47211.8125\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 47436.3438\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 44616.3477\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 46765.6953\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 43367.4453\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 44818.0938\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 46090.9844\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 45094.0938\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 48380.1836\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 44153.4219\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 45607.6172\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 45215.1602\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 45551.5664\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 46200.2969\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 45716.5547\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 45092.1836\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 45183.0898\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 47576.4844\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 48858.6953\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 45859.4766\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 44535.2500\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 45011.7188\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 47521.6641\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 45143.3945\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 45850.6914\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 45156.6406\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 43678.0508\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 44482.3555\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 44647.1562\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 45457.5117\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 46188.0547\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 47501.8555\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 44194.6250\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 45146.8281\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 44904.5312\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 45813.6719\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 44730.3047\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 45218.5625\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 47665.5234\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 44741.5078\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 45471.0156\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 44050.1562\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 44994.8359\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 44891.7422\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 44365.5508\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 43551.0195\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 44406.5117\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 45471.8203\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 43926.8086\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 45647.6797\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 45769.3711\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 47587.1016\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 46566.2734\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 46935.2461\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 47899.0703\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 45562.1562\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 44931.0859\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 43788.6875\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 45939.6016\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 46696.9844\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 44291.0234\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 42877.5312\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 45726.3086\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 43411.8516\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 46574.3750\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 44375.6680\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 48413.1172\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 47406.5820\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 45091.0391\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 45700.9062\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 43668.0625\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 45248.8906\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 45186.7109\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 43701.7930\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 44626.1055\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 43271.5547\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 45779.0859\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 43625.1094\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 47666.9062\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 46177.8281\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 43443.2812\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 47069.8438\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 45268.6602\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 44362.5234\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 45703.4922\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 43380.1133\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 43499.9844\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 42797.6445\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 44372.1875\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 45303.2930\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 45719.5352\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 46085.5625\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 44246.1914\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 43418.3125\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 43614.6914\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 44922.2734\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 43442.8164\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 43604.8984\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 43253.0156\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 45846.0820\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 44252.0156\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 44790.8789\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 42609.3008\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 41562.3164\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 45320.7109\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 44465.2812\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 45066.5508\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 44642.5469\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 43488.2031\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 42790.5859\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 43811.4062\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 44941.5586\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 45157.7031\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 43472.9453\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 46526.6133\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 47390.2578\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 45443.3672\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 44862.7266\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 43128.4258\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 43828.2852\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 43827.8672\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 45047.4727\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 44712.3555\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 45667.5156\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 42652.6094\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 45847.8438\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 45652.4609\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 43984.9766\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 43283.7578\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 43708.1719\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 46346.7969\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 43514.5664\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 47097.8359\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 45495.8633\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 47503.8281\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 44782.3047\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 44995.1836\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 44374.1758\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 43381.4609\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 45245.4766\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 47480.0703\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 44641.9688\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 44154.2656\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 42836.5586\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 41650.9375\n",
      "âœ… Epoch 3 Avg Loss: 56294.5932\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[507 613]\n",
      " [610 503]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4518\n",
      "\n",
      "ğŸ” Epoch 4/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 45078.8281\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 43204.5391\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 41697.7031\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 43688.1328\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 44929.3867\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 43550.3398\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 46032.8047\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 46214.2578\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 43830.3047\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 44160.5938\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 43214.2656\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 43374.1758\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 45143.6719\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 43914.2578\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 43702.9258\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 41314.4648\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 44109.6250\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 42224.8398\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 43886.1641\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 44303.8359\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 41668.8008\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 45121.4531\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 43255.4570\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 43710.2852\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 45712.7812\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 44717.8633\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 44393.9062\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 42865.6719\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 44965.6758\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 43109.4844\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 42019.3555\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 43844.5312\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 44215.6953\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 45025.4531\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 42056.5938\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 41722.0938\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 43397.8594\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 42153.2891\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 42468.9062\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 43467.1562\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 44718.3984\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 42132.9062\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 45449.9414\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 44501.6094\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 44289.1445\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 43175.7734\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 41734.5742\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 42420.9844\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 41944.2188\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 43603.7070\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 42050.8906\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 42788.2930\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 43328.6172\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 43457.9609\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 42628.2891\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 42963.5469\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 43871.1797\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 42445.0781\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 44136.2656\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 41418.1719\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 41666.2812\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 43516.6406\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 43970.5039\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 42906.1719\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 42897.1484\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 45448.0469\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 42565.6680\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 42875.0703\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 41808.4766\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 41775.5156\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 42339.9141\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 40298.8906\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 41233.8320\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 45784.5000\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 41612.0039\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 43607.6016\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 42040.7617\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 43680.3477\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 43189.4688\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 43213.6484\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 41555.4492\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 40733.6875\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 41382.0781\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 45297.6992\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 40715.6875\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 42120.1992\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 42275.5156\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 40575.0273\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 42925.6797\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 43441.8047\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 40382.7422\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 42609.5781\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 43566.2969\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 43819.2344\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 41656.0703\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 43461.3594\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 44194.5312\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 44232.2266\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 42442.9766\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 43225.3945\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 41977.1719\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 44204.7266\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 42843.7891\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 42308.2031\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 41257.3281\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 40709.3828\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 42517.7617\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 42536.8125\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 43385.7656\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 42316.3242\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 43486.8672\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 41962.2930\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 42113.6719\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 41543.0312\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 42459.4922\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 42885.0859\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 40828.1172\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 40594.4844\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 42162.9844\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 42780.3359\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 42963.1406\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 43380.8984\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 42569.7656\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 42102.4297\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 42295.7109\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 42669.2188\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 41324.3242\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 43719.9375\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 43179.3750\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 41452.9766\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 43354.0664\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 43449.0000\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 42635.0391\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 41289.0195\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 41016.6562\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 42073.9375\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 43059.0195\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 40946.5078\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 40463.6484\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 40884.7695\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 41129.5391\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 43388.6562\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 42839.5391\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 41033.3281\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 43014.5078\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 42272.9883\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 41610.6055\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 42162.6641\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 42098.5781\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 39684.0859\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 42870.7617\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 42060.0664\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 41999.0859\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 42105.0469\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 39774.5430\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 42781.7500\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 39913.4023\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 41518.3789\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 40968.5938\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 41373.0781\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 42425.7461\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 41477.2461\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 39838.3320\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 41298.5469\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 41482.3164\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 42666.7383\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 40427.4336\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 42163.2031\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 40941.7422\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 41381.6250\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 39220.3672\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 40664.0703\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 42678.5469\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 39493.3281\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 38956.7500\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 43404.3438\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 41731.8945\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 42652.5391\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 41065.7344\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 39725.5430\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 41437.8984\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 42318.1328\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 42188.5703\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 40939.9570\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 40031.3984\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 41257.9375\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 41970.3945\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 38547.6719\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 41731.1016\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 41446.3203\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 39437.2148\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 43200.9062\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 39543.8984\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 41991.9062\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 39905.4570\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 40343.8281\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 41138.8438\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 41692.2812\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 40948.8867\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 42461.2969\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 40825.9375\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 41226.3594\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 40381.6953\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 39464.8594\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 42172.9492\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 40974.8164\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 40887.0547\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 39620.0430\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 39235.5391\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 42677.4531\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 41472.9375\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 40731.5586\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 42294.8438\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 41395.8828\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 38846.7734\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 40199.5625\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 39696.9258\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 39792.7891\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 39314.2109\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 40591.0742\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 40179.9219\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 41153.3320\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 38814.7500\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 38846.9219\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 40172.2344\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 40278.8125\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 40085.4609\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 40944.1914\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 41935.3828\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 41068.3047\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 39985.7578\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 39730.7344\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 40615.5469\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 41751.5000\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 39736.5859\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 39613.4883\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 40673.3477\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 40285.5859\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 41292.6133\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 39116.6211\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 41073.4844\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 39962.6133\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 40741.4883\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 42489.6992\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 40722.8594\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 40272.1016\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 39976.1797\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 41505.2891\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 41279.0547\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 41786.2344\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 40941.7344\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 42592.9766\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 38479.4531\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 38695.7695\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 40974.6914\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 40886.9766\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 40435.3438\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 38768.3281\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 39856.2656\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 39147.1562\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 37701.1445\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 39843.0547\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 39823.8125\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 40702.8047\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 41279.3984\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 38320.6406\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 39652.3359\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 39580.1016\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 39249.3164\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 39120.5000\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 40858.5859\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 41091.2031\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 39512.8984\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 40028.7773\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 40447.8984\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 39544.9961\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 40268.0781\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 39413.5352\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 41425.0273\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 40082.6992\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 40795.6953\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 40103.4883\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 38804.0547\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 40276.7031\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 39401.9336\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 40129.5000\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 41312.5977\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 38546.6797\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 40167.2266\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 38144.2500\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 39011.3867\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 39026.4688\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 38176.2422\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 41277.2031\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 37901.2461\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 41516.7578\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 38794.2812\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 37906.5195\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 37541.7695\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 39331.8281\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 38749.3867\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 39346.9375\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 39756.7656\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 39141.8633\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 38110.0742\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 38819.6875\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 38277.7539\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 38622.6406\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 38411.6484\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 37827.5469\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 39398.3984\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 39528.8750\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 39063.6562\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 36988.0703\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 39869.9883\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 37635.8633\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 39917.7070\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 39904.7500\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 39209.6016\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 39402.6094\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 38542.4609\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 39228.5156\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 37064.9844\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 38471.9609\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 40582.6953\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 38767.1562\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 38416.5039\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 38388.7383\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 40670.8984\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 41115.3203\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 38008.2930\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 40145.6992\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 37997.8359\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 37762.1562\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 41520.7773\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 36343.6562\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 39259.7383\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 37789.6367\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 39111.3594\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 37097.1367\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 38487.1328\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 37153.7891\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 39765.3086\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 39479.5977\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 38980.3438\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 39425.9727\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 38993.2539\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 38681.6250\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 37621.7422\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 39349.8281\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 38017.5312\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 37782.5234\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 37652.1953\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 40852.7383\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 40185.6094\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 37588.2422\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 38929.5234\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 37336.8281\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 38148.1328\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 37922.2773\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 40306.8711\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 38356.6055\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 38902.8750\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 39268.3438\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 38283.6953\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 37618.9414\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 37351.2305\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 38021.2266\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 38582.1875\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 37401.9648\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 40271.4609\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 37994.4023\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 39366.6641\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 39017.7344\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 38364.5469\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 38094.8750\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 38910.3281\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 36885.6797\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 35942.7266\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 36720.0625\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 39073.6016\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 38693.4766\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 37797.3164\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 39141.5195\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 36942.3594\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 38663.6875\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 37148.0977\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 38448.8828\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 39684.6797\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 36934.4453\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 38421.6562\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 39174.7266\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 37407.8633\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 37843.7422\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 38163.7344\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 38528.0352\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 38193.3906\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 36721.1016\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 36607.2734\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 36508.0977\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 38432.7422\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 37682.2344\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 36671.1953\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 37007.6562\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 38347.1484\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 35680.9180\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 38018.7578\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 40199.4844\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 38010.3828\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 37279.5547\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 36590.2969\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 39473.4688\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 37528.8125\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 36967.8438\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 38107.1758\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 38435.7930\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 37989.5312\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 36453.3008\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 40279.2266\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 37135.5430\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 37445.7031\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 37494.6133\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 38588.0273\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 37545.4922\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 36444.7969\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 38313.1055\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 37363.4453\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 37810.7344\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 38073.6172\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 35945.2539\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 39563.3633\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 38177.0469\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 37557.7227\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 36847.7734\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 37886.6562\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 36719.6641\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 36226.6836\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 38179.8711\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 38324.4375\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 36835.3516\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 35895.0469\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 37281.0781\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 36639.9766\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 38735.2344\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 35436.3906\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 37964.1328\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 37311.3359\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 38800.6719\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 36899.9453\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 36302.6914\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 35327.4336\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 35398.9570\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 39422.0820\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 37308.1562\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 37140.3047\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 36097.6953\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 37428.2500\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 35854.0547\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 37062.6875\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 36737.9531\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 37307.3281\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 37007.2500\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 37129.6406\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 35353.8984\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 36275.6133\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 37076.3906\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 38622.9141\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 37180.1562\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 38683.3906\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 38536.4688\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 38374.2422\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 35114.3359\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 37102.4062\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 36625.6641\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 38530.3594\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 37522.0781\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 36646.7227\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 37574.0312\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 37555.7578\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 38656.7070\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 37462.8945\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 36345.1016\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 35954.3125\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 35125.3125\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 36096.8047\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 37056.7031\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 36041.8438\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 35800.5195\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 35776.3906\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 34953.7344\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 34570.1172\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 36993.7188\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 39195.7852\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 35794.0391\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 37659.6406\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 38328.2773\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 37578.0664\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 37585.4219\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 35464.1719\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 37226.6875\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 36754.5781\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 36664.4922\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 36631.4180\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 37579.3594\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 36125.0547\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 36349.1016\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 37746.9062\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 37705.5352\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 36331.3203\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 37949.3750\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 36808.9688\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 37306.9062\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 35200.6875\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 35446.4219\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 35977.3438\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 35676.7812\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 35273.4375\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 37608.2383\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 35037.3828\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 37011.5547\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 35521.5469\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 35325.1953\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 36613.8438\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 34750.5938\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 34746.2188\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 35166.9922\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 36720.2188\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 36176.2656\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 36553.5273\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 35894.7969\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 36102.3984\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 36992.0938\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 35786.9102\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 37747.6016\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 36404.4648\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 36499.0625\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 35565.4766\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 35118.2578\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 34579.4609\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 36887.2422\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 35177.1562\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 35386.1680\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 36001.2773\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 36488.5391\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 36145.7734\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 35447.2891\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 36631.7109\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 36338.9297\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 35605.0430\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 35557.1797\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 35812.0156\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 36265.5625\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 37700.2422\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 36454.6484\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 37662.1953\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 35181.4805\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 37163.5625\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 35144.4531\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 35628.1875\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 37342.9922\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 34864.7500\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 36626.1797\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 34950.8398\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 35171.2266\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 35360.3750\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 35065.2188\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 37539.7500\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 35042.8203\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 36316.0742\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 35667.2031\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 35645.4766\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 35780.0859\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 36652.3438\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 34174.7227\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 36044.1055\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 34547.6328\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 33586.1719\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 34783.3438\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 36994.7500\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 38129.1250\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 37275.2188\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 34230.4961\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 33736.9688\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 37411.6914\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 37201.8828\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 34761.9062\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 36633.0586\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 37402.5312\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 35300.6172\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 34110.6094\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 37998.3359\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 36130.4922\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 35467.0977\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 36303.3867\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 35718.8477\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 35064.8320\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 34410.9141\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 36521.8047\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 36480.9844\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 38454.9961\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 36101.0703\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 35195.3047\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 34800.5898\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 34797.8828\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 36144.1641\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 34696.4219\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 36669.7578\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 35257.5547\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 34461.1094\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 34601.1953\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 34974.7383\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 35379.7500\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 35484.3633\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 37089.7500\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 36276.9453\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 34429.5156\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 35574.3203\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 36072.4219\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 33860.8633\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 34224.1562\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 34930.5820\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 35860.5391\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 34661.9727\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 35172.7578\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 35074.4141\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 33722.4336\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 36031.3906\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 34343.7266\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 34751.2656\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 34174.4609\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 34316.6758\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 36607.8359\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 34280.7305\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 34120.0000\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 35184.0977\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 33377.8867\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 36558.3281\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 36602.9844\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 33990.2344\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 35442.2070\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 33686.3594\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 33959.1133\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 33782.3438\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 34503.8438\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 33828.0391\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 34377.1328\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 34872.8906\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 34016.9023\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 35269.0000\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 35472.3555\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 34126.6914\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 33383.7656\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 36161.1172\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 33620.9062\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 33307.9023\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 36706.2852\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 35131.3828\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 34396.7266\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 34329.8906\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 35922.2578\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 35104.1953\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 34140.9102\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 34507.1172\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 32376.9961\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 33741.1719\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 34901.1406\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 36215.7734\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 36401.0938\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 34682.8359\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 34554.4141\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 35724.7266\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 34306.8281\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 33855.3750\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 35264.7383\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 34369.7031\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 35868.2656\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 32500.3320\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 34246.6875\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 32869.4453\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 36583.3281\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 32459.5449\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 31465.9531\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 33698.4062\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 33781.6953\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 33057.2305\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 35101.7969\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 34392.3516\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 33538.4922\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 34498.8555\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 35283.3984\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 35569.6953\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 32081.8359\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 32799.9336\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 34657.0312\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 33629.3047\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 34027.6328\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 34808.9609\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 33306.5312\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 33545.2031\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 32623.6738\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 33204.9141\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 34891.2812\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 34181.4219\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 34253.1406\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 35191.7773\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 32939.0469\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 35261.8906\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 34447.5547\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 34142.3633\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 32853.7734\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 34278.8906\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 32381.9609\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 34000.7031\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 32965.5430\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 34965.9453\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 33368.7500\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 32368.2969\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 32504.5527\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 34540.1484\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 33540.4570\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 32042.1328\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 33134.4531\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 34738.4414\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 33646.4453\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 32448.2305\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 33436.2031\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 34568.7969\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 33569.5000\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 32146.5996\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 33567.6289\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 33474.2188\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 33630.9062\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 33542.8750\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 34450.3086\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 34070.4766\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 32442.5957\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 33976.2227\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 33656.1953\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 32494.7812\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 34769.8594\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 33700.3633\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 32591.8828\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 34558.7812\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 34865.2500\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 34026.4180\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 35525.1484\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 34503.8359\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 33143.9375\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 34205.2266\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 32715.1406\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 33508.7773\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 33811.2891\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 32411.3633\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 34268.4531\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 34760.3945\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 35096.4922\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 32190.1270\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 33571.6367\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 34248.2188\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 32394.8340\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 32453.6133\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 32327.4609\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 34953.1250\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 35199.1836\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 33126.2422\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 32064.8848\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 35981.7500\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 34458.6602\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 31873.1406\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 32171.4316\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 32057.3691\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 33975.9375\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 35198.1445\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 32669.3105\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 35152.9805\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 32877.4922\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 34101.9766\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 33887.4375\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 31914.3047\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 32192.6016\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 32039.7520\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 31466.1992\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 34121.3828\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 33650.1094\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 32069.1016\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 34070.5000\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 31049.8047\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 34274.9531\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 32883.1641\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 33772.4141\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 33354.2695\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 31884.3848\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 32872.4141\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 33165.7070\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 33913.2656\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 31581.3516\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 32193.1660\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 33847.4922\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 33236.1641\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 32154.1289\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 32158.2188\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 33525.4766\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 34160.7227\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 32301.6816\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 33720.3125\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 32079.3633\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 32445.0586\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 34275.5625\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 32692.3242\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 33067.7930\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 33109.9062\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 33293.3516\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 34050.9609\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 31945.3945\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 32175.0273\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 32308.0391\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 32182.9512\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 31573.6172\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 34489.2109\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 34268.6250\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 31583.5312\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 32671.0781\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 32426.8047\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 33960.3438\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 32196.9062\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 33014.2266\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 32742.4473\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 33553.5742\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 32425.6172\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 31595.7891\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 32294.2812\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 32760.0977\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 31275.2656\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 33445.8203\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 32400.7539\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 33301.2461\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 31725.3516\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 33609.0469\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 32210.5352\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 31468.8320\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 32837.2031\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 31610.0742\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 33224.7852\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 33462.2266\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 31242.6543\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 31988.0117\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 31814.1016\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 34675.6289\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 31458.1406\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 33578.6250\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 31702.8945\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 32617.5137\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 32226.5977\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 32360.5898\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 31378.1816\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 30928.2363\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 34584.4844\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 31509.6758\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 32318.6074\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 32564.9043\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 32662.9219\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 31986.9141\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 31856.6289\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 33177.7812\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 31665.9355\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 31333.4160\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 30498.4941\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 30696.7676\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 33622.8477\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 31618.8633\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 31170.6875\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 30188.8359\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 32481.3203\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 32301.3965\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 30188.9688\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 32481.5234\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 31757.3750\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 33416.3086\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 30702.4844\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 33083.7969\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 31228.3027\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 29943.7168\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 32499.3398\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 31165.9492\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 32449.7344\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 30093.4043\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 31223.9375\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 31051.9766\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 31008.2051\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 33437.1875\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 31829.4551\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 30909.6328\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 29748.8008\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 31781.8320\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 31662.7852\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 30959.1074\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 31284.2422\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 31685.8477\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 31238.9492\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 32998.8477\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 31585.0195\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 30234.5000\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 31613.6914\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 30368.3320\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 33074.3984\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 29872.4570\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 31735.3594\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 31316.7422\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 30999.0605\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 30069.4336\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 31481.0254\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 30245.0742\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 32399.3457\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 30187.6309\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 33310.7656\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 32266.2910\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 32997.6484\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 32600.9766\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 30063.3281\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 31248.3242\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 33252.1953\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 30996.7539\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 32278.6211\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 32188.4180\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 31521.2363\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 31512.1270\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 29932.0664\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 33234.6289\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 31235.3965\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 30763.7656\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 29619.0996\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 33166.2812\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 32244.1914\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 30738.6914\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 31249.4922\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 30960.8125\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 30482.2461\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 31046.7500\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 30770.2305\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 31435.6504\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 32143.4805\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 29817.3594\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 31157.6953\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 30094.6387\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 32286.4336\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 31493.1367\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 31926.7461\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 29995.4023\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 30738.7246\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 32758.0859\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 32260.1367\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 30496.0625\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 32102.4648\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 30751.3477\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 31091.4570\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 30075.0312\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 32742.5898\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 30916.7891\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 31126.2461\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 31546.5664\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 32368.7695\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 29883.3887\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 31352.8516\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 31180.0840\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 30489.8301\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 30650.9727\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 30512.0977\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 32387.5352\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 30156.2266\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 30882.4863\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 32282.1641\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 30459.8770\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 31250.7578\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 31738.1035\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 32513.8301\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 30136.9121\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 32087.3730\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 31953.4355\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 30062.0586\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 29982.5195\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 30446.0977\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 30235.2148\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 31379.6211\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 31661.8398\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 31543.7500\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 30210.9902\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 31114.0840\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 29709.0430\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 31193.1914\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 30546.4980\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 29821.4727\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 31213.7246\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 31987.7930\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 31504.7012\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 31847.0293\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 31266.0703\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 31151.1875\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 31855.8945\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 31289.6914\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 31530.7500\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 29389.5977\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 30286.5000\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 31375.1484\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 30447.7754\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 30978.4805\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 30876.1582\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 31440.4629\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 31254.4434\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 30967.3516\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 29959.6094\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 30712.9336\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 29225.0820\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 31551.8672\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 30947.0957\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 31242.5820\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 30724.7148\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 31672.6992\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 29636.4727\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 31532.8359\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 30459.7500\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 30924.0508\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 30547.0469\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 30827.7871\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 31660.5879\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 31930.7266\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 29628.0781\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 29961.4434\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 31440.5996\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 31596.5352\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 30814.8750\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 30938.5000\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 29283.4141\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 29045.6914\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 30860.4121\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 30379.9414\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 29574.6484\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 31564.3184\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 29057.4141\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 31727.6895\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 30680.8027\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 29441.9883\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 29547.9883\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 31356.6641\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 31288.7109\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 31543.4180\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 29665.6094\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 28673.5977\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 30740.0703\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 30361.4785\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 31233.2695\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 28571.4863\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 30136.7617\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 30221.9766\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 30613.9629\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 29482.5469\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 30637.0684\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 29161.9355\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 31640.7012\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 28182.0117\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 29385.5918\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 31192.9355\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 31030.2539\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 30476.0547\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 29560.9434\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 29161.6797\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 29316.1484\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 30991.0742\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 28501.6172\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 29207.2852\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 30584.4453\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 28929.3262\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 30774.3359\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 28539.3086\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 30553.3281\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 30608.9355\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 29227.9395\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 29793.1641\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 28486.8281\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 30010.5469\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 29869.3281\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 30139.5703\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 30958.7168\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 30595.2754\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 30070.6699\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 29546.7676\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 29950.3906\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 29261.6406\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 30816.8379\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 30225.7852\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 29611.9727\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 30967.4180\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 30466.5469\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 29598.8008\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 28489.8047\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 28011.8652\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 30368.3828\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 30164.0078\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 28768.6719\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 29479.8242\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 28890.9102\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 29896.0273\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 29903.3848\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 29321.2656\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 28931.5430\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 30727.8750\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 29684.5820\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 30001.3984\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 28560.5391\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 30185.8926\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 28094.1641\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 30076.8926\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 29914.1016\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 29873.0547\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 30115.7090\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 29970.0508\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 28668.0547\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 29609.3906\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 30666.1797\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 29555.3848\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 29450.7656\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 29193.9980\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 30037.9961\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 28284.4141\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 28442.2324\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 29903.0586\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 30729.3242\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 30612.4512\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 28906.8438\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 29289.8691\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 28603.8242\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 31129.3457\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 29118.0332\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 28764.6797\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 29333.6094\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 30342.4609\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 30220.1523\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 29368.1055\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 27684.3398\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 29397.9805\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 29476.9336\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 28635.2266\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 28695.0352\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 27421.6758\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 27669.3008\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 28992.1816\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 28327.3027\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 30290.2305\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 28922.5645\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 29652.5625\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 28656.0625\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 29035.1406\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 30542.0898\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 30103.5996\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 28704.8867\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 29974.5762\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 29254.0703\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 28438.8242\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 28415.9727\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 27913.9023\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 28984.4863\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 28073.5820\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 29671.6816\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 29368.8984\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 27391.3105\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 29281.9922\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 28230.3906\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 29709.0137\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 29288.6074\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 28993.7070\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 28424.6445\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 30255.3262\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 29594.7812\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 28434.9414\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 29461.7480\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 28819.2422\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 28435.4609\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 28453.7168\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 28913.4648\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 27838.1523\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 28781.3086\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 28583.4258\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 27894.6523\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 30569.5000\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 28128.3477\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 29390.3555\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 29246.1680\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 28604.7969\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 27778.2109\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 29578.0703\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 29648.8086\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 29004.0469\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 28970.9590\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 28911.9375\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 27950.4043\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 28431.2656\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 28822.9180\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 29476.2988\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 27614.2129\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 28117.8828\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 29776.1172\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 28181.2461\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 28962.6465\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 27876.9766\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 28399.4023\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 28613.5820\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 28616.6484\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 29639.3633\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 29516.2812\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 30347.8105\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 27553.4570\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 28527.8477\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 28935.1016\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 29762.5566\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 27295.8906\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 28913.0605\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 29851.3164\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 26934.7070\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 28230.8477\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 28029.4062\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 29352.4961\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 28825.7969\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 27607.5391\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 29347.5039\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 27973.0098\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 28435.6035\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 27571.6680\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 28283.9199\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 29096.9297\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 28359.8008\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 27747.0430\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 27581.4277\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 28725.9082\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 27575.9375\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 27743.5859\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 29219.7812\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 29523.6758\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 28629.5000\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 28756.2402\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 26944.4121\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 28992.5176\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 27825.7578\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 29305.2891\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 28670.2891\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 28945.0977\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 26642.2695\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 29048.0625\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 27925.7539\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 27390.7617\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 27092.9297\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 27603.2168\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 29351.5195\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 28276.8281\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 28181.5781\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 28445.1953\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 27760.7617\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 28134.3281\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 26774.0469\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 27357.1328\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 27459.9082\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 28058.7676\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 28228.0703\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 26282.0254\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 26027.9551\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 27563.8359\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 28011.8164\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 26851.5508\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 28470.8711\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 28266.6875\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 29275.0977\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 28124.7852\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 28116.5625\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 28389.9180\n",
      "âœ… Epoch 4 Avg Loss: 35234.6329\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[509 611]\n",
      " [608 505]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4523\n",
      "\n",
      "ğŸ” Epoch 5/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 28527.4336\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 28383.2656\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 27707.1250\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 28675.6914\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 26522.2031\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 27591.7422\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 28238.8086\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 26891.6680\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 28620.9434\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 27703.3086\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 27852.7461\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 28148.1738\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 28402.7441\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 27256.6191\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 27927.7148\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 28918.5781\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 28112.8750\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 26296.3223\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 27162.8164\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 28236.0977\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 28017.7031\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 27404.0078\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 27881.3281\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 27147.5898\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 28548.6035\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 28415.8613\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 27153.4922\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 28592.9629\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 27937.5098\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 27956.8613\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 27140.6875\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 28259.8340\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 26185.4492\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 27849.0938\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 27429.7793\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 26443.4414\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 26042.7734\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 26426.1172\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 26329.9727\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 27991.0566\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 27258.6406\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 27492.0977\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 26902.2422\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 27576.4258\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 25982.0547\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 27400.7578\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 28013.4922\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 27476.5000\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 28922.0586\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 26941.5586\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 27072.5449\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 27279.7266\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 26247.0332\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 26514.2363\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 26580.0820\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 27892.2383\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 27591.9922\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 26453.5781\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 27235.4531\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 25966.4805\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 27355.8574\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 27682.9375\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 28545.0078\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 28168.1719\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 27402.7305\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 28081.4238\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 27234.9258\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 27277.5625\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 25604.2090\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 26564.4434\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 28612.2617\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 26842.0312\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 25880.3555\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 27102.2031\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 26286.1367\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 27630.7812\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 26425.2227\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 27052.4297\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 27409.8047\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 27656.0117\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 27398.2578\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 27024.2734\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 26193.3457\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 27176.7734\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 26345.2910\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 26537.1191\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 27607.8047\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 26637.5820\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 26529.5703\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 26816.9004\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 27439.9238\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 25482.6719\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 27863.6602\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 26373.0215\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 27480.3184\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 27333.3379\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 25838.6250\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 27638.9336\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 27490.1836\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 26616.5273\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 26604.3379\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 28656.0117\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 26094.5234\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 25885.5586\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 26781.7773\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 26640.2949\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 26478.8477\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 28253.7930\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 25477.1367\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 25947.7363\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 27183.5859\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 27094.0234\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 27311.2148\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 25562.2676\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 26297.3262\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 26232.9863\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 25732.0547\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 26589.0996\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 26212.3828\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 25588.3164\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 26901.1875\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 26732.5508\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 27325.9922\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 26222.0586\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 25779.2070\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 26133.6445\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 26799.8320\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 26071.8359\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 26496.0859\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 27824.3613\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 26481.4844\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 26633.5039\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 26618.3848\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 26335.2070\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 28038.9297\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 26762.6094\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 26766.3828\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 26682.6699\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 26491.3242\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 26250.4414\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 26850.5801\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 26356.8281\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 27816.0996\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 25853.0703\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 26429.9414\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 26089.8711\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 26588.3047\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 25849.9102\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 27513.3867\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 26907.2617\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 27156.4219\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 27095.1641\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 27251.1875\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 25747.6562\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 25791.4922\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 26748.8906\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 26280.6309\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 27034.7070\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 26511.0938\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 27136.3887\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 26386.5918\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 25825.5820\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 25866.6543\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 26006.6758\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 25511.8145\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 25912.7480\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 26315.7773\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 26568.6680\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 27161.0352\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 26256.6270\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 26493.0410\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 25806.3008\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 26218.8203\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 25949.2695\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 26255.0664\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 26072.9453\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 25586.7617\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 24657.5391\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 26051.5566\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 25791.0703\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 25196.0664\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 25446.8945\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 26542.9883\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 26291.6445\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 25474.2422\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 27029.3750\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 26323.1328\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 25689.7891\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 25426.0293\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 27365.6582\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 26549.1602\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 25387.4180\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 26487.7266\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 25704.4688\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 26213.3438\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 25373.7129\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 26578.2656\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 24207.5723\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 25963.9766\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 26541.2441\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 27081.6602\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 25739.5430\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 25482.2539\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 26578.4570\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 24555.1367\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 25944.3477\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 25977.3125\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 25057.4316\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 26084.3594\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 25636.0703\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 24764.4785\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 27669.9043\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 26368.6680\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 25066.0293\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 26348.3203\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 26833.8945\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 26610.9375\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 24348.6484\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 26519.4844\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 25130.5449\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 26476.5137\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 27997.0312\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 25507.9922\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 25943.2070\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 24868.2305\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 26419.4844\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 25596.1055\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 26056.8496\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 26586.4980\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 26027.4570\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 24791.9297\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 24999.7461\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 25446.4258\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 24601.2695\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 25033.1074\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 26297.0508\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 26323.8086\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 25790.9727\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 24708.8086\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 24964.5820\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 26023.3496\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 25457.7617\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 25737.3262\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 25363.4805\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 25434.4609\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 25053.4766\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 25437.4316\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 24188.1758\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 25343.8125\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 25448.7930\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 25186.5508\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 25454.2461\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 25193.2852\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 25146.6973\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 26068.0664\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 25458.3203\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 25188.9277\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 24944.1719\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 25893.6484\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 25552.1641\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 26300.9102\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 25887.7422\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 25446.9922\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 25964.6348\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 25101.1504\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 26501.3613\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 25036.7012\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 26827.5938\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 24820.3906\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 25880.6484\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 25431.7695\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 23933.0195\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 25074.4199\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 24540.7188\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 24537.3828\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 25445.9316\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 25871.7773\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 24361.0195\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 24334.6465\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 25153.0938\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 25501.8984\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 24587.1445\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 26364.3984\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 24893.7578\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 25109.1094\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 24514.3750\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 25797.8066\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 25923.4375\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 23982.0820\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 24457.2070\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 24275.4844\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 24500.6152\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 25884.3633\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 24448.0742\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 24584.9609\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 24403.9453\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 23918.4043\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 24310.2363\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 24585.6055\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 24795.0625\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 25098.3457\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 23105.9492\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 24692.5840\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 25831.3125\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 23684.8047\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 23976.4863\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 24597.7871\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 25175.9785\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 23852.5078\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 26574.2930\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 25035.0996\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 25456.9141\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 25033.4473\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 23912.2891\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 24842.4414\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 24797.5234\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 25503.7051\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 25098.7891\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 24014.0234\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 24294.8828\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 24996.3945\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 24863.1484\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 25718.0020\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 23660.1465\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 24519.2617\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 24654.1504\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 24859.7109\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 26018.8691\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 24305.8184\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 24660.9023\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 24531.8320\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 24202.9004\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 24612.5957\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 23817.1797\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 25257.6035\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 24389.6523\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 23141.9961\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 24675.5605\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 25389.9062\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 24621.1895\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 25192.9180\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 25849.0469\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 25831.3730\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 24463.8711\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 24229.5273\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 25886.7383\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 24522.5898\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 24915.4531\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 24767.8945\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 23518.3457\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 24125.8223\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 23371.8008\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 25048.7539\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 25301.4219\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 25333.9766\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 23794.4062\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 26217.2188\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 24648.2930\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 24154.1562\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 25376.0000\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 24932.8242\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 26084.3379\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 23691.9336\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 24946.8008\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 24642.8281\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 23866.9375\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 24436.3711\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 24275.9043\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 24795.8223\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 24856.6562\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 25257.5684\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 24370.2617\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 24049.4023\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 25518.7715\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 24160.0234\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 25147.7656\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 24969.2754\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 24388.2422\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 24541.7578\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 24981.6074\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 25655.3516\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 24908.2363\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 23706.8457\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 23217.6465\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 24180.4258\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 23726.2129\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 23854.4648\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 23981.2109\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 25794.6875\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 24089.7402\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 23734.1445\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 23619.1074\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 26248.8711\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 25216.3164\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 23832.2637\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 23993.6367\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 24714.4180\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 23990.8477\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 24199.2188\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 24633.3066\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 25217.3613\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 25208.1309\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 23607.9316\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 23810.3164\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 23146.8633\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 23621.0195\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 23115.1797\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 24467.9414\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 23851.9355\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 23996.7559\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 25644.6406\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 24221.8828\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 23848.4023\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 24334.6191\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 23068.4492\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 24226.3203\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 24385.0781\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 23928.5039\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 23403.7695\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 23390.5859\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 24489.9102\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 23945.7988\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 24112.6367\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 23890.9648\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 24331.5547\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 24679.8008\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 23685.3555\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 23202.9766\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 23362.8242\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 24775.0195\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 23804.5000\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 23749.5059\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 24612.7656\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 23414.4102\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 23287.5703\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 24097.4297\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 22817.9160\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 24036.1914\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 24328.6426\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 23704.2617\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 23542.2930\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 23877.4336\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 23540.3672\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 24417.3965\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 24070.6836\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 24158.0781\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 22941.8242\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 23170.6953\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 24235.9492\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 22944.5820\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 23846.0332\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 25003.8887\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 24792.4609\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 22975.9258\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 23463.8398\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 23023.3496\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 24242.0391\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 24063.8164\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 22738.1660\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 23766.9961\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 23850.8164\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 23334.3828\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 22748.3105\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 24700.7109\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 22837.5215\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 23843.4727\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 24246.7441\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 23556.0391\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 23746.9453\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 24220.5605\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 22266.7383\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 23008.8164\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 24114.0938\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 22848.4766\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 23780.0684\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 23881.5938\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 23959.6602\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 23921.1895\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 23400.2754\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 23980.7852\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 23819.1328\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 23888.3359\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 23141.9805\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 24114.1953\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 24107.4609\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 24501.0117\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 24121.8008\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 22618.6387\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 23251.6758\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 23752.6562\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 23563.2871\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 24309.5410\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 24085.3750\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 23449.0391\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 23170.1055\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 22956.5781\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 22966.2812\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 23171.0078\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 22623.5254\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 23323.3789\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 22968.1406\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 24940.8750\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 23139.4922\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 23553.0684\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 22217.2559\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 24014.1309\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 24362.7148\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 23102.4141\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 22688.5273\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 22459.8340\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 23321.3750\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 24556.0391\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 23109.0488\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 21958.6094\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 23754.2012\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 23103.7148\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 22753.1230\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 23781.1602\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 22946.2344\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 22233.2305\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 23294.1406\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 22417.3555\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 23376.0547\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 23857.8320\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 23672.9492\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 23893.9473\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 22816.8594\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 21232.1992\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 22946.1055\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 23002.5879\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 23415.9922\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 22653.3867\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 22119.6660\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 22407.8320\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 23093.2695\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 22190.8145\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 24623.7832\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 22145.4219\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 23807.9648\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 23250.1074\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 23151.6270\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 23424.4512\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 23292.9180\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 23094.5859\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 23096.7266\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 22170.4648\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 23362.1172\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 23020.2266\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 22447.7305\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 23632.5742\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 22953.1562\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 22043.0078\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 24437.5156\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 24031.8887\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 22580.9844\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 22361.1621\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 21763.1973\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 23236.3633\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 23626.7363\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 22649.3535\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 23335.5508\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 23455.9902\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 23124.6445\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 23247.3164\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 22797.5117\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 22870.4590\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 23040.3125\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 24345.8320\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 22950.6602\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 22259.5918\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 22409.2969\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 23379.5781\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 22546.5293\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 22055.3262\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 22080.0430\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 22060.1484\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 22431.4883\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 22226.6855\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 23738.8496\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 22772.7168\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 21885.2266\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 23124.7305\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 22143.5781\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 22877.8828\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 22760.8750\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 21746.7988\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 21599.5645\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 21505.5625\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 22262.6504\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 23845.3301\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 22430.5762\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 22726.4824\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 22740.4805\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 25311.1484\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 23694.9062\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 22250.4688\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 23355.3691\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 23207.4219\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 22054.9180\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 21892.9258\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 22898.4121\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 22121.6289\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 21974.7188\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 22750.3340\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 22077.0078\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 22671.4766\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 22076.1992\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 22902.0664\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 21838.1953\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 22380.2500\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 22601.4590\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 22875.1094\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 22823.7246\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 22238.4453\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 23340.1562\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 21230.7129\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 23205.9648\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 23232.4766\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 22293.3809\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 22601.4629\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 22927.3242\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 22540.5156\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 23146.4453\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 22498.2090\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 21891.6562\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 21977.7148\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 22819.1953\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 22790.4219\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 22672.9297\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 21464.0938\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 21692.5215\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 22907.1602\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 20929.9531\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 23022.7461\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 22867.9961\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 22676.3457\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 23023.2324\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 21468.8555\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 22407.8477\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 22373.0430\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 21767.2031\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 22484.9336\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 20880.8105\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 21418.0840\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 22809.1484\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 22476.6797\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 21928.1289\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 22244.0254\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 21343.1562\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 22410.2734\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 22984.3516\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 21160.4414\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 21489.4688\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 22877.1738\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 22259.5938\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 22364.9688\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 22106.3594\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 20949.7969\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 21696.1445\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 21651.0137\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 21010.9141\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 21013.9414\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 21420.2148\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 22536.3984\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 22929.8398\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 22855.3242\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 23123.6367\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 21321.9766\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 23373.6504\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 21744.5469\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 22592.8750\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 22375.2559\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 21993.5273\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 23212.0469\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 21242.9648\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 22167.4746\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 21277.4570\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 21920.2344\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 21534.2656\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 22249.2441\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 22655.7070\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 22043.9277\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 21491.5938\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 22768.4219\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 22464.3906\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 21275.1680\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 21328.5508\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 22491.9336\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 21502.3164\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 22353.9375\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 22100.9297\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 23265.9688\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 22084.7852\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 21770.1797\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 20919.8359\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 22579.0840\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 22423.2930\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 22600.4316\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 22576.5332\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 21026.5352\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 22212.4609\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 21525.9531\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 21531.9863\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 21781.6797\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 21645.1641\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 21727.1992\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 21643.9023\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 22402.2109\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 22542.1699\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 21552.8711\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 21221.3301\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 21438.9219\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 20523.9434\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 21746.6758\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 20896.2812\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 21044.1875\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 20439.0352\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 21903.7422\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 21670.2305\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 21400.2422\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 21174.2227\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 21216.1152\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 21208.3164\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 20687.8008\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 20002.1523\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 22527.5156\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 22881.5254\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 20937.7695\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 22412.3457\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 20695.8496\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 21473.6367\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 20761.9746\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 22525.4609\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 20770.5020\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 21134.5391\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 20968.2500\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 21307.4160\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 20940.9746\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 20329.6328\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 22226.1504\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 22291.4023\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 21710.7266\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 21794.6465\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 22310.2988\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 22181.5586\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 21244.4180\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 21796.2227\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 20586.5859\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 21441.6367\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 20933.7695\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 20777.0078\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 20474.7461\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 21247.2773\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 20556.6035\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 21750.0195\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 21705.0039\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 21667.9648\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 21586.0508\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 21559.9844\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 22563.9023\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 21980.5391\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 21698.2266\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 20745.9297\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 20668.4570\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 21337.2949\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 21384.4727\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 22193.0977\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 20779.8906\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 21640.5312\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 22312.2461\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 21700.6992\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 22075.5605\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 21737.4434\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 20514.8223\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 20801.0469\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 21037.9258\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 20558.1328\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 19969.8066\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 20834.3672\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 19216.7891\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 20088.1328\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 20867.3477\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 22284.2031\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 21612.4648\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 20383.7422\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 21718.5215\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 21114.9355\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 21169.2637\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 21194.3770\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 20956.5898\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 21669.2832\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 21046.0781\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 21094.9297\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 21078.5098\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 20387.1172\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 20758.0391\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 21486.4648\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 21067.4961\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 21992.1309\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 20290.1328\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 20650.3242\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 19818.0508\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 20185.2227\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 20812.4766\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 19938.5820\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 22459.7305\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 19918.2480\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 20595.9688\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 21087.5195\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 20793.3555\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 20743.9375\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 21420.7910\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 20177.8809\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 20859.0605\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 21342.6914\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 21065.9492\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 20438.3164\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 20636.7031\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 21576.8906\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 21438.2266\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 21259.3105\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 20279.3320\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 21053.3672\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 20597.8262\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 20538.2148\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 20475.8281\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 21925.9648\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 21004.2559\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 20577.3789\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 20797.5430\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 21268.5156\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 20584.9023\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 20269.1406\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 20538.8359\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 20075.0547\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 21071.6289\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 20805.0820\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 20095.4961\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 20000.8906\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 20178.9297\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 19431.9492\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 19518.5938\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 20918.3477\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 19855.1602\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 20399.4453\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 20304.4648\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 21091.5938\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 20985.9492\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 21467.2832\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 20933.0371\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 20208.4375\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 21405.8555\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 21179.0273\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 20221.4062\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 19949.4434\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 20620.7148\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 21107.8340\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 19957.0039\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 20676.5488\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 20218.0391\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 20854.8984\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 20700.0156\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 21629.9727\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 20368.0039\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 20824.8848\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 19306.4141\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 20524.2227\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 19810.8438\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 20243.3945\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 20088.3809\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 20805.9473\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 21231.7168\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 21560.6172\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 20695.7695\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 20392.1719\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 21172.2539\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 20199.1992\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 21403.1836\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 19835.7109\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 19715.0234\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 21148.5703\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 20490.6777\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 20789.5625\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 21065.5508\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 20129.1641\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 20812.1875\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 21147.8477\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 20448.0664\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 20464.5488\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 21825.1992\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 20259.2031\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 20901.4961\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 20141.1230\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 19236.8516\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 20948.2383\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 20053.6211\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 20832.0918\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 19561.2402\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 20203.0508\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 20465.6562\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 19991.7500\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 20274.9766\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 21285.7480\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 20001.4785\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 20659.1250\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 19754.6250\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 19593.9414\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 19450.3770\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 20260.5664\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 20416.0430\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 20554.4707\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 20070.5273\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 20228.7148\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 19665.2949\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 20516.1934\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 20771.4609\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 20471.2617\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 20423.4766\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 19991.6035\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 20853.4922\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 19305.8574\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 19987.7891\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 19398.1641\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 19828.0254\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 19555.6621\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 20455.5312\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 19027.7695\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 19523.2715\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 20419.9883\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 19846.5000\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 20634.6641\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 19104.4863\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 19965.1855\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 19485.4180\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 19436.5859\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 19549.0391\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 20979.2773\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 21504.1367\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 20566.5664\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 19754.1055\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 19979.6016\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 20636.8398\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 21008.4258\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 19977.3164\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 19736.1973\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 21073.8379\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 20867.3047\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 19757.6406\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 19583.1250\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 20895.0137\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 19409.0996\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 19544.2246\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 20980.9590\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 19060.9121\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 20483.3398\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 19180.0527\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 19161.7969\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 19975.7852\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 19749.5078\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 19635.6523\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 19881.5488\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 19161.8652\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 20082.2969\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 20945.6133\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 19738.3770\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 20154.1426\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 18811.5859\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 20263.8770\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 19547.3516\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 19510.1309\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 19351.0039\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 19748.2734\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 20120.6797\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 20373.0566\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 19840.6387\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 19173.8438\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 18974.2109\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 19881.6699\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 18710.3867\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 19710.5918\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 19885.1406\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 20665.4238\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 20480.0234\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 19108.4727\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 20306.0664\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 19653.1406\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 19569.2812\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 20414.0059\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 19593.4844\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 20716.6797\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 20129.8594\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 20890.4492\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 20301.1172\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 20136.8438\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 19829.2422\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 19363.0312\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 19001.3809\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 18826.9004\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 19817.5273\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 20345.7520\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 19750.4414\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 18774.8828\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 19310.9336\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 19692.9395\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 20333.8008\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 20055.0977\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 18870.8125\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 19456.2031\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 19484.2578\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 19141.6836\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 18827.3359\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 19220.6953\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 20309.7832\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 19533.4102\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 19237.2188\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 18938.4141\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 19138.4766\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 18268.9805\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 18897.7109\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 18937.0039\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 18811.0547\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 19892.4375\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 20558.4414\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 19628.0820\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 19244.8594\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 18651.3906\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 19009.4785\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 19634.6055\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 18549.1191\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 18961.2422\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 19018.1367\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 20006.6562\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 20622.2871\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 19953.2148\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 19270.0469\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 19380.9180\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 19066.7637\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 19021.6328\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 19948.5586\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 19260.0742\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 18348.6016\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 18533.7637\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 19460.6680\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 19088.4102\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 19575.8809\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 20462.8320\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 20247.7266\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 19367.3184\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 19051.5781\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 19230.6406\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 19280.2793\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 18673.8320\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 19283.3711\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 19249.8496\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 18149.7578\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 19297.5117\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 19393.0020\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 19208.8516\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 18308.2383\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 19555.7383\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 20085.8047\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 18609.4219\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 19789.2422\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 17891.1504\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 18789.4883\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 17836.2578\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 18327.1465\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 18291.4453\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 18633.0371\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 18617.1562\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 19131.0977\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 20665.1426\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 19721.3438\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 19802.5898\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 19548.5195\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 19144.2617\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 18411.3984\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 17997.0527\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 19534.9180\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 19701.0938\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 18730.8066\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 19514.6172\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 19725.9492\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 19078.3086\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 19280.7637\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 19432.5391\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 18642.1406\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 19674.6133\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 18756.4062\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 19327.8320\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 18355.3516\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 19108.7773\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 19361.9004\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 18139.9219\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 18307.4922\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 18761.9805\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 19139.5078\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 18836.0312\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 18961.3086\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 18626.4219\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 18333.6113\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 18575.4004\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 18627.6523\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 19663.8535\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 18548.8438\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 19019.8320\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 19573.8672\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 19548.8047\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 18275.0918\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 17918.1133\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 20079.6270\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 18270.1172\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 18604.9141\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 18386.7422\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 19462.2207\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 19281.3203\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 18632.6113\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 19334.3086\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 19049.4883\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 19451.4844\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 19384.1230\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 19396.3672\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 19153.8066\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 18941.4043\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 18103.2070\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 18242.0195\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 19504.2266\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 19773.9629\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 19665.8926\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 19299.2324\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 17941.5781\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 19414.2344\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 19062.7617\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 18871.2285\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 18749.1992\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 19354.9727\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 18339.0469\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 19027.8594\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 18885.5039\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 18554.4492\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 18941.3945\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 19339.1289\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 18596.0352\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 18237.2617\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 18266.5625\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 18625.3145\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 18768.3398\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 17798.2188\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 18122.0000\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 19745.3477\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 18278.3984\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 18271.4922\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 19232.4336\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 19433.8516\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 19017.7031\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 18694.4180\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 18904.5391\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 18180.1367\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 19096.2500\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 18675.4609\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 17916.6406\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 18389.2012\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 18911.4551\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 18645.6523\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 18838.3398\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 18412.9648\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 18314.6641\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 19203.4023\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 18522.8613\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 17815.5469\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 17984.5723\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 19069.4766\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 17590.7246\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 18743.0430\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 18119.3438\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 17527.8945\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 18790.4102\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 19283.2500\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 18430.1484\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 18015.8789\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 18415.9453\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 17864.7617\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 18225.5352\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 18739.3359\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 17835.2422\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 17934.2734\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 17465.3574\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 18196.4375\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 18034.7969\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 18910.8633\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 18510.8984\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 18415.4062\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 17618.9297\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 18735.3516\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 18909.8086\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 18833.7734\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 17843.3008\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 18116.3047\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 18572.6289\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 18140.3320\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 17844.7422\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 18020.2207\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 17973.4883\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 18911.5586\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 18425.5742\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 18671.2461\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 18643.4805\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 17513.1230\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 18515.4414\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 19458.8086\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 18017.9023\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 18198.5586\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 17621.5430\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 17461.4570\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 19072.0762\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 18220.1523\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 18204.3164\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 17224.2129\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 18614.5078\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 18288.4102\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 18589.2852\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 18313.7539\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 18335.6641\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 18474.3125\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 18542.8457\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 17789.0098\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 19377.7520\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 17165.3086\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 18675.3965\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 18055.3281\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 17152.1758\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 17197.6094\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 18061.8008\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 16785.1738\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 18062.8359\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 17669.7793\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 18538.6914\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 17613.6094\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 18026.7598\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 17822.4043\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 18652.2656\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 17773.6035\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 17848.8457\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 18339.7832\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 17683.6074\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 17159.0273\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 18793.0508\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 18230.6094\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 17817.5234\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 17822.5391\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 17389.7520\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 17133.9102\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 17263.0781\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 18435.9961\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 18216.1602\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 16778.4648\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 17028.3887\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 18742.8965\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 18683.2344\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 17596.1797\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 17845.7168\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 17428.1973\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 18551.4141\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 17444.5625\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 16683.3125\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 18815.5508\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 17567.5586\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 18486.1484\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 17467.2812\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 18126.8047\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 17760.7324\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 17543.8945\n",
      "âœ… Epoch 5 Avg Loss: 22441.0690\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[517 603]\n",
      " [600 513]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4551\n",
      "\n",
      "ğŸ” Epoch 6/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 17773.2695\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 17188.5195\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 18237.0703\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 17753.6250\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 16938.3672\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 18559.6367\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 17603.3867\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 17186.2109\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 18232.5391\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 17888.0312\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 17654.9082\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 17251.1602\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 18083.7070\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 17218.0312\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 17451.2812\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 17838.7324\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 17392.7969\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 17356.1641\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 17945.2168\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 17951.3750\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 18388.9062\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 17415.8867\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 17798.3711\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 17402.6426\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 17997.6914\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 17230.9766\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 18063.6602\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 18331.3047\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 18630.5430\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 17944.1191\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 17738.0840\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 17398.6777\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 16962.8750\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 18141.8418\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 17601.6660\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 18364.0195\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 18735.4941\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 17624.6035\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 17848.4434\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 17471.1641\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 18037.8770\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 17115.3359\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 17600.8906\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 17386.4492\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 17982.9453\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 18234.8672\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 17280.1758\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 17320.2168\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 16962.1328\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 17360.4414\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 18230.5820\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 17092.0273\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 17262.0664\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 17856.4453\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 17728.9688\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 17018.0781\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 16926.8438\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 17352.5742\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 17036.5625\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 17426.8848\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 17558.9609\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 17458.5898\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 16959.0488\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 17133.3125\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 16531.0625\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 17521.4922\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 17382.6797\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 17555.4844\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 17196.9180\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 18253.2070\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 17483.8770\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 17162.6875\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 17822.0938\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 17321.4609\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 17643.9844\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 17479.7500\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 17635.6055\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 17564.3496\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 17861.1270\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 17712.2422\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 17401.8555\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 17022.4512\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 16973.6562\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 17815.1836\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 17001.0215\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 17420.0664\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 16906.4258\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 16572.5703\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 17928.2051\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 17918.6016\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 17882.2617\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 17419.6758\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 17400.6328\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 17479.2832\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 17010.7910\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 16912.8730\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 16648.6133\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 16776.3555\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 16646.3145\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 17272.7656\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 16573.0430\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 16749.7617\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 18220.4082\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 16424.0117\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 18094.0312\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 16724.1211\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 16915.6680\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 18117.0117\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 17030.3945\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 16486.2031\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 16285.2969\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 16520.3457\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 16950.7949\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 16840.8789\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 17374.3379\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 16365.6289\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 16944.6797\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 17175.8691\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 17775.9844\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 17918.6934\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 17605.7031\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 17033.8965\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 16971.7930\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 16674.7969\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 16422.3516\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 17087.3164\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 17282.6465\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 17156.8594\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 16650.3086\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 16903.2188\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 17462.0898\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 17244.5156\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 17605.1094\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 16993.8125\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 16978.5449\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 17849.1191\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 16264.6582\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 17216.5273\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 17846.2422\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 17174.3594\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 17180.7070\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 16634.0977\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 17645.2617\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 17001.5449\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 17042.4414\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 16276.8779\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 17209.3730\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 17233.7324\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 17792.8594\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 17725.9805\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 17441.3867\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 16275.6963\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 16172.6367\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 17400.9805\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 16990.5781\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 16763.1836\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 17287.5137\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 15932.2041\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 16987.6367\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 16851.0352\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 16584.4102\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 16879.7715\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 16827.7129\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 16995.2656\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 17409.3164\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 16615.5605\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 16598.4023\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 17238.9727\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 16994.9629\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 16750.5117\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 16037.6777\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 16297.8789\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 15801.0449\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 16112.1914\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 16840.3203\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 16464.8203\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 16851.1797\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 15653.1191\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 17549.2695\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 16903.2773\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 17231.5859\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 16168.4160\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 17090.5137\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 17135.5977\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 17045.3984\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 17127.9258\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 16668.1562\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 16847.7129\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 16093.0957\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 16766.8066\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 17199.2090\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 16998.4844\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 17374.3652\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 16501.0664\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 16114.9346\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 16518.7344\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 15762.2676\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 16872.5703\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 16456.2500\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 16762.9824\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 17643.5195\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 17063.7734\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 16835.7383\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 17664.4023\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 17152.2656\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 16307.3223\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 17193.6094\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 15867.7070\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 17041.5391\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 16734.0820\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 16582.0176\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 17052.0977\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 17457.4941\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 16867.0898\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 16832.9492\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 16874.0156\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 16968.2773\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 16935.1191\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 17032.5664\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 17180.4414\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 16495.5078\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 17304.6484\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 16275.8379\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 16653.2344\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 15943.5293\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 16251.4248\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 16013.6709\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 16363.7793\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 17346.0996\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 15962.7656\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 16684.7500\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 16952.8984\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 16153.8350\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 16464.8438\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 17099.1035\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 16170.3984\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 16894.5547\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 16307.2158\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 16159.2207\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 16028.2666\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 16066.1641\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 16740.7539\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 16107.2324\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 16109.4395\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 15496.8906\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 16832.8984\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 15333.0078\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 16310.1992\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 16557.6445\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 16252.3652\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 16162.5049\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 16906.3633\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 16573.7891\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 16195.2441\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 16611.9141\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 16259.6367\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 16226.4980\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 16381.6973\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 16905.9961\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 17048.4805\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 16917.1230\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 16653.5664\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 15709.8262\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 16610.0000\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 16660.3555\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 16831.3613\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 16093.5059\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 16809.1133\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 15870.3027\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 15885.9639\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 15544.7510\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 16070.7246\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 16244.0498\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 16524.1641\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 16410.9336\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 16277.9863\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 16010.8574\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 16606.5586\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 16416.4961\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 16298.5654\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 17276.8867\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 16263.4219\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 16558.0312\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 16147.1592\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 15732.5605\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 16309.6523\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 15608.7529\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 16415.6914\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 16845.6875\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 15994.8311\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 16337.9004\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 15515.9707\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 16593.6133\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 16416.8867\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 16445.6406\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 17134.6426\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 16835.0996\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 16006.8115\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 15763.1641\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 15730.2363\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 16485.8359\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 15771.3105\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 15952.7559\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 16245.7168\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 16703.9805\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 15844.1025\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 17141.1992\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 15846.0020\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 16064.8799\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 15747.7881\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 15388.4434\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 15098.4541\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 15493.0215\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 16176.8174\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 16389.5957\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 16121.5615\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 15783.6338\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 16526.2930\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 15820.5996\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 15490.8184\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 15335.4141\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 16753.5234\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 16250.5625\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 15807.7559\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 16018.0215\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 16661.0254\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 16386.4766\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 16512.0020\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 15584.4082\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 15824.9463\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 15765.5098\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 16028.9121\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 15774.8711\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 15392.7246\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 15947.2354\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 15902.4453\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 15222.1709\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 15605.3857\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 14747.5898\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 15998.8984\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 15950.9131\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 15452.8936\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 15134.3057\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 15380.8174\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 14930.0371\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 16114.3770\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 16008.4561\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 16207.4150\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 15634.0889\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 16145.0703\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 15375.3613\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 15245.9395\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 15542.8477\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 15826.2109\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 15394.0605\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 16029.0645\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 15095.8799\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 15015.6211\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 15733.3896\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 16140.1445\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 15293.1094\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 16773.1836\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 15415.2812\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 15347.2168\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 16088.7930\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 15108.5117\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 16286.7002\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 15289.0898\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 15126.1016\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 15816.8271\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 16070.8037\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 14608.4463\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 15959.3545\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 14859.7559\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 15845.6670\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 15736.6719\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 15832.5674\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 15884.2773\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 14823.4414\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 16794.0664\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 15071.9258\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 16543.1582\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 17130.7031\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 15701.8320\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 14504.6074\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 14984.4648\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 14945.7637\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 15721.9082\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 15376.7148\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 15738.9082\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 15921.3076\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 16066.9814\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 15291.9004\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 15474.9766\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 15208.6299\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 16032.2139\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 16600.1289\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 15293.4141\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 15144.6396\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 14848.7285\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 15245.3770\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 16731.2852\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 15426.3887\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 15807.1289\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 15416.6660\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 15076.2959\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 15033.5664\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 15051.0703\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 15598.5000\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 15572.0723\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 16662.9805\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 15899.9590\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 15046.7891\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 15371.5029\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 15175.8008\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 15167.2773\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 15543.3066\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 16148.5859\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 16491.0117\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 15254.4697\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 15884.2734\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 14983.1689\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 16008.8398\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 15706.9805\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 15626.1504\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 15528.0781\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 14912.4199\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 16171.9004\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 14772.5801\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 16151.5566\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 15432.9844\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 15281.7979\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 15551.4980\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 15902.2480\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 15498.0713\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 16338.0371\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 15266.2930\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 14641.8799\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 14943.8828\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 14882.9141\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 14713.8301\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 15253.1045\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 14928.8789\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 14655.8691\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 15332.2676\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 15231.4160\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 15384.9863\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 15300.0361\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 15171.1875\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 16089.4629\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 15950.7461\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 15015.8184\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 15461.7383\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 15648.0879\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 14707.1797\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 14918.5537\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 15132.3311\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 14912.2656\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 15510.4922\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 15549.4531\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 15360.3379\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 14471.8320\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 15966.2988\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 16002.1719\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 15493.9307\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 15287.7051\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 15605.4658\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 15271.2148\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 15538.9766\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 15692.3848\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 15116.1787\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 15533.1699\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 15130.5605\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 14753.4316\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 15318.7422\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 14227.4590\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 15372.3184\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 14479.6162\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 15405.5664\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 15703.0430\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 14928.8672\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 15618.0137\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 15690.3125\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 14685.6992\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 15310.8125\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 14228.2080\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 14474.9121\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 15868.0918\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 15355.4551\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 15487.6348\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 14540.8789\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 15911.7266\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 14967.1162\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 14902.3730\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 15124.8193\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 14831.9854\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 14716.4971\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 15064.0264\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 15055.6416\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 15589.6660\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 16028.7832\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 15604.2666\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 15399.0186\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 14713.1875\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 15846.7480\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 14516.7363\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 14829.7666\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 15173.8438\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 14757.6504\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 15249.8633\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 14746.6748\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 15166.8672\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 15908.6133\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 15005.0293\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 14669.7900\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 14977.5723\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 15250.2344\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 14680.8672\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 14741.5723\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 14478.6621\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 14563.0859\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 14799.4902\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 15328.7598\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 14686.7773\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 14923.7812\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 15571.8457\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 15170.8320\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 15336.4668\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 14363.6943\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 15374.2461\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 15130.8516\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 14974.9668\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 15037.5488\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 15450.1816\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 14704.1055\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 15105.7363\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 14226.9326\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 15405.1211\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 14415.4170\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 14979.8223\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 14074.7109\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 14754.3945\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 15370.6572\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 14627.7695\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 14518.7471\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 15162.3242\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 14921.5918\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 14902.3652\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 14778.1426\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 15472.8730\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 14300.9766\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 14975.8447\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 14282.1992\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 14717.1641\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 14787.3047\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 14359.2842\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 14818.9453\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 15054.0254\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 14696.8691\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 14399.5957\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 15011.9668\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 14996.1738\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 15079.4678\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 14752.2168\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 14440.8916\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 15009.7666\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 14776.4541\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 15206.9678\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 14421.6289\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 14341.9219\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 14933.4102\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 14067.2109\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 14074.0078\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 14121.1016\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 15012.8730\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 14483.0186\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 14571.6973\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 14743.2031\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 14895.8594\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 13991.0791\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 14118.2002\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 14090.2988\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 14606.0381\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 14310.1367\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 15359.8906\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 14649.8066\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 14262.7656\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 14543.9668\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 14515.2686\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 14833.1445\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 14869.6299\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 15072.6279\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 14737.7041\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 14540.1562\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 13924.5449\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 13700.4521\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 14357.6855\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 14725.2129\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 13877.3115\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 14137.7568\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 14039.3789\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 13722.5176\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 14507.3721\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 14831.4326\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 14081.8438\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 14809.9502\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 14424.3223\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 14489.4531\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 14203.7051\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 14185.7637\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 15115.2246\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 14510.6348\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 14511.4199\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 14520.0039\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 14515.7812\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 14203.8174\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 15075.3857\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 15323.8789\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 14052.0449\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 14363.6758\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 14087.0898\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 14215.7861\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 14275.3359\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 13941.4551\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 14244.9570\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 13939.9316\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 14558.3340\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 14108.6348\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 14537.0215\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 14033.8574\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 15266.4727\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 14351.0811\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 14178.2041\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 14217.9746\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 14319.8945\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 14792.5244\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 14868.6328\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 14847.4971\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 14953.2949\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 14870.7480\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 14188.5381\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 14909.0029\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 14205.0498\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 14478.7656\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 13454.2129\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 14769.3086\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 13929.4004\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 13757.4453\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 14901.2197\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 13801.0166\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 14430.5488\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 14371.2246\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 14252.0762\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 14307.2188\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 15015.0176\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 13953.5664\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 13560.0176\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 13603.6738\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 14762.3105\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 14815.2129\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 14188.3037\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 14687.2246\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 13930.9023\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 14298.6875\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 13686.7812\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 13971.3184\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 14133.1445\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 13562.7891\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 14819.1875\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 13650.3223\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 13955.8438\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 14018.1533\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 14561.9277\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 15247.4199\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 14373.7607\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 14376.1270\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 13963.6670\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 13901.4023\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 13830.9043\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 13895.4131\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 14667.5010\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 14700.8691\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 13499.4023\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 14114.1201\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 14300.5830\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 13892.1631\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 13868.9297\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 13150.6992\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 14432.0918\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 13802.1543\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 13910.0137\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 14738.7979\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 13795.9209\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 14529.2793\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 14102.6572\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 14227.5967\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 14688.8730\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 14189.1436\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 13359.9697\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 14013.8174\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 13941.7715\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 13392.6260\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 13453.9121\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 14373.7246\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 13926.6348\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 14716.3398\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 14627.3311\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 14005.2070\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 14730.6074\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 13623.0391\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 13611.4375\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 14068.1484\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 13434.9941\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 14226.0586\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 13400.1699\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 14193.1016\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 13948.4736\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 13683.0264\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 13787.4238\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 14112.8398\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 14957.8730\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 14098.4980\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 13506.7842\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 13611.8633\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 14164.1230\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 14363.0527\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 14022.8086\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 13676.7002\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 14361.0879\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 13516.2422\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 13715.5938\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 13609.9121\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 13406.7754\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 13604.5820\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 14242.2578\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 13785.5000\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 13010.1934\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 13785.6455\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 13691.2275\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 14086.5898\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 13960.5156\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 14052.2324\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 14160.7344\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 14191.2852\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 13288.8398\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 13235.6465\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 14378.1924\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 13528.5547\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 13464.7910\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 13179.2383\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 13901.5898\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 13446.9775\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 13971.4414\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 13566.3359\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 13758.7441\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 13529.0410\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 13887.7246\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 13106.4922\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 14580.5898\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 14000.0605\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 13539.6211\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 14199.1338\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 13204.3906\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 13299.9082\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 14189.2578\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 13553.6367\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 14465.0762\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 13000.4512\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 14008.9805\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 14069.0146\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 12880.7715\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 13922.2295\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 13532.5176\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 14090.8223\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 14391.0576\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 13361.8535\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 13566.0312\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 13380.1445\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 13868.9219\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 13669.8291\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 13971.7080\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 14028.6211\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 13110.2002\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 13812.8594\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 13895.4512\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 13183.5479\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 13729.5918\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 13949.6973\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 13383.1357\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 13376.7080\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 13391.7812\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 14254.3115\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 13772.4248\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 13751.7930\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 13492.7188\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 13071.6982\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 13166.5781\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 14335.6406\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 14347.9199\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 14266.9688\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 13119.9570\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 14009.7178\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 13066.3506\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 12955.2070\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 13295.7695\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 13442.0762\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 13971.0332\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 13315.8750\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 12827.4795\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 13763.8965\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 13850.5117\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 13502.2012\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 13200.3516\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 14042.6875\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 13471.0049\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 13111.0127\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 13460.7949\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 13639.0752\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 13216.0430\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 13414.5381\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 12728.0195\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 12842.1699\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 13986.9590\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 13802.7725\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 13647.1074\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 14209.2734\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 13511.4766\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 13046.0020\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 14124.8311\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 13511.1572\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 13546.6230\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 13258.0967\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 13128.5635\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 12870.3467\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 13627.4375\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 13993.1680\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 13654.1309\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 13770.2295\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 12948.9199\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 13078.0000\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 13397.2617\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 12605.4258\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 13890.4609\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 12964.0146\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 13534.0225\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 14067.1504\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 13286.1777\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 12821.6846\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 13516.1533\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 13441.7812\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 13086.1836\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 14029.0771\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 13094.9219\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 13754.5439\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 14115.1973\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 12974.7676\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 13333.9277\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 13112.1113\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 12917.3750\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 13210.1602\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 12896.8066\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 13049.7246\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 13643.6230\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 13019.4453\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 13298.9863\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 14545.0635\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 13734.6113\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 12914.1172\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 13348.7314\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 13758.9160\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 13608.6758\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 13830.5566\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 13449.9180\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 13456.3867\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 13090.9922\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 13075.7363\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 13377.8926\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 13333.8613\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 12855.2012\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 12693.7754\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 13307.5176\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 13575.7793\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 13266.0254\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 12993.7129\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 13349.2051\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 13930.2480\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 13468.1562\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 13052.9414\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 12940.8926\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 13169.6250\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 12674.2002\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 12852.1689\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 12515.6328\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 12984.3379\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 13340.3301\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 13480.2852\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 13590.0000\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 12220.1406\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 12911.8213\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 13164.4541\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 12374.1875\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 13249.0879\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 13328.9102\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 12464.7148\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 13149.8926\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 12767.6201\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 13017.9395\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 13131.9336\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 13491.4463\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 12892.9326\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 13614.4355\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 13595.9648\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 13021.2188\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 12620.2871\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 13269.3984\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 13092.0684\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 12870.1133\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 12749.6914\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 13081.5508\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 13350.9434\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 12793.5918\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 12420.0654\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 12972.1562\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 13025.8652\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 13354.5117\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 13220.9336\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 12725.8340\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 13024.2705\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 13128.6914\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 12804.3945\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 13301.8320\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 13194.2754\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 12873.4180\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 12541.0029\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 13279.6533\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 13439.6758\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 13092.9678\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 13226.4531\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 13349.3691\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 13057.6426\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 13045.8008\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 12247.8604\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 13111.9170\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 13696.0039\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 12531.7217\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 13385.3008\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 12357.8438\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 13038.6006\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 12516.3359\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 13264.2578\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 12818.6104\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 13557.0273\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 13155.9951\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 12943.8018\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 13602.3086\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 12537.3496\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 13499.7197\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 12401.2559\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 12614.0527\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 12628.6240\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 12096.3604\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 12478.2715\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 13870.1465\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 12331.4092\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 12743.0664\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 12864.1592\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 13211.0449\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 13437.6250\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 12811.0186\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 12789.6465\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 12963.0166\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 12787.7637\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 12781.6855\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 13222.2168\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 12929.2266\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 13168.5234\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 12180.1074\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 12635.0020\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 12817.7090\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 12691.7324\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 12816.5215\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 13240.3164\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 12416.0332\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 12740.7969\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 12244.5566\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 12707.1895\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 13015.7441\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 13200.6270\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 12207.2676\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 12921.4219\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 13303.5410\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 13012.6816\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 12836.1699\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 12304.1699\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 12749.9277\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 12466.6406\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 13009.3311\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 12554.6025\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 13276.7510\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 12039.4668\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 12750.1455\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 12317.4258\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 13427.1260\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 12577.2266\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 12070.0947\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 12095.3525\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 13010.4590\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 12291.9111\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 12429.1797\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 12069.3213\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 12515.3848\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 12320.0518\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 12882.7598\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 12102.2363\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 12899.0840\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 12682.5645\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 12581.3447\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 12525.6436\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 12483.1729\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 12721.1680\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 12785.6299\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 12307.4102\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 13304.6270\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 12718.8926\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 13265.5381\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 12554.7871\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 12215.7188\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 12502.6816\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 12755.4512\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 12873.8496\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 12766.5312\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 12836.1699\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 12740.6367\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 12392.8379\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 12857.0664\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 11833.6895\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 12350.3770\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 12273.6816\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 11901.9639\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 12464.0742\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 12235.0723\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 11673.6348\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 12848.6943\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 13126.0117\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 12146.3438\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 12767.3857\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 12637.1514\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 13119.7646\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 12991.2295\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 12103.2393\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 12311.6211\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 13065.7715\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 12555.7402\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 12008.5400\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 12873.6768\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 12147.2148\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 12262.6045\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 12166.2227\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 12772.2109\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 12024.5635\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 12512.5957\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 12379.1758\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 12628.4346\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 12287.6602\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 11838.2617\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 12094.1543\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 12307.4014\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 12733.3262\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 12416.8809\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 12501.1797\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 12564.8379\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 11815.3750\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 12423.1211\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 11675.9121\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 12362.0352\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 12502.5430\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 11769.2578\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 12152.5908\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 12461.0518\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 12251.9297\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 12551.9980\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 12058.7725\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 12469.7314\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 12042.3643\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 11779.5537\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 12430.8164\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 11658.0596\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 12528.3916\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 11690.7627\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 11778.7461\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 12435.9150\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 12275.1230\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 12352.0645\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 11973.2969\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 12262.4043\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 12894.4336\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 11711.0469\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 12168.6289\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 11674.7822\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 11671.0811\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 11688.0098\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 11386.9980\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 11850.5127\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 12437.7129\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 12482.3535\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 11754.2734\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 12075.6777\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 12764.5020\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 11983.5762\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 12312.9688\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 11681.8848\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 12505.2266\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 11930.9023\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 12278.4375\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 11861.4717\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 12968.2598\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 12233.1875\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 12394.3340\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 11944.4883\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 11878.8887\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 12333.6455\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 12259.9355\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 11930.5879\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 11806.4824\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 11510.0020\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 12576.8066\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 11627.6006\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 12365.5371\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 11629.7617\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 11708.1367\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 12482.3164\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 11584.1416\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 11610.6562\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 12265.2266\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 11419.7812\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 12698.8818\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 12059.4219\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 12047.4229\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 12299.6318\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 12414.5713\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 11930.9629\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 12000.7832\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 12164.6289\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 11624.7627\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 12537.6904\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 11486.9980\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 11526.2998\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 12310.2041\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 12344.5742\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 12309.6533\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 12650.1953\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 12153.0615\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 12136.1963\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 11667.4688\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 12035.4766\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 12145.5977\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 11506.7695\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 11925.0977\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 12076.2793\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 12444.0498\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 11812.8447\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 12342.8027\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 12042.8574\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 12671.3262\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 12317.1445\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 11644.8213\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 12243.2607\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 11385.3672\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 11698.0098\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 12041.4160\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 11472.6445\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 12100.2910\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 11802.0801\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 12174.9961\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 12305.0684\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 11462.5244\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 11809.8047\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 12507.2627\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 11683.1074\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 12015.4697\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 11585.3896\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 11954.7676\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 12035.6250\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 11541.0781\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 12094.2441\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 12356.4502\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 12010.0840\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 12568.3936\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 11907.2764\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 12150.9219\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 11630.1270\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 11312.4785\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 11956.4756\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 12136.5879\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 11915.6348\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 12177.5352\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 11353.8789\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 11831.0820\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 12124.0645\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 11625.6895\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 11492.7549\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 11741.5146\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 12024.6309\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 11964.8301\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 11505.9502\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 11641.5264\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 11982.1621\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 11342.1533\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 11803.6787\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 11389.8867\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 12048.0977\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 11433.9121\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 12246.6934\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 11635.2920\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 11631.0371\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 11448.8721\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 11302.4082\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 11598.4023\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 11461.1699\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 12086.8750\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 11322.0332\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 12311.6328\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 11757.1289\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 11250.8887\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 11309.9043\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 11910.1699\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 11800.2275\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 12290.2676\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 11847.6270\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 11923.4521\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 11676.5664\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 11885.0586\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 11096.7002\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 11317.8154\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 12559.6562\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 11255.5879\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 11995.9980\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 11480.1309\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 11593.0762\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 12204.1045\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 11745.4932\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 11477.3828\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 11071.5811\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 11627.9580\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 11165.5000\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 11523.3105\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 11986.9492\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 11197.7441\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 11829.9395\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 11537.7676\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 11949.3008\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 12145.3633\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 11523.8818\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 11096.2734\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 11772.3213\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 12110.0430\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 11360.7979\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 12183.5742\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 11542.3965\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 10955.5684\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 11758.2578\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 11585.1504\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 11246.8223\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 11638.9326\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 11376.7891\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 11728.9902\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 11247.0410\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 11867.7158\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 11181.3027\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 11379.2695\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 11252.7949\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 11782.7529\n",
      "âœ… Epoch 6 Avg Loss: 14463.6669\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[521 599]\n",
      " [596 517]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4608\n",
      "\n",
      "ğŸ” Epoch 7/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 11400.6631\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 11179.9990\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 11506.0723\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 11083.2207\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 11490.3223\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 11150.1133\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 11400.3066\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 11240.4277\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 11060.6992\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 11280.2705\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 11548.0166\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 11339.0117\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 11822.1445\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 10764.9863\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 11045.0195\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 11716.0479\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 11615.1875\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 11134.9072\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 11356.6328\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 12052.9277\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 11570.6611\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 11773.3994\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 11826.6504\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 11480.9277\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 11355.8926\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 11635.4072\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 11068.2090\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 11838.8105\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 11324.0547\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 11428.9102\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 12043.8408\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 11969.0742\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 11376.6621\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 11464.5547\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 10965.8428\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 11090.0908\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 11121.5635\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 11766.6943\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 10828.6201\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 11029.0078\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 11255.2012\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 11581.8750\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 11298.4922\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 11726.7441\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 11690.2969\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 11597.8213\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 11303.1777\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 10948.9971\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 11218.6484\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 10662.8164\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 10704.7266\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 10869.1250\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 11672.2393\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 11209.6797\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 11323.8477\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 11403.0166\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 11625.8770\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 10972.7129\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 11478.1328\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 11507.0234\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 11158.3457\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 11621.1729\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 11303.6709\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 11595.9551\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 10918.2539\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 11758.6582\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 10688.6602\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 11323.7148\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 11230.7344\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 11476.0137\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 11424.5557\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 11456.8633\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 11412.4277\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 11411.9912\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 11348.3994\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 10987.1836\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 11410.9316\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 10789.1992\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 10922.8945\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 11319.0605\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 11214.7227\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 11769.0049\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 11105.3398\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 11560.8184\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 11011.6152\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 11280.5703\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 11140.6826\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 10884.0078\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 10848.7871\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 11321.4834\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 10809.6826\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 10854.0596\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 11424.5879\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 10760.7842\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 11199.0107\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 10978.0146\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 10895.3223\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 11039.2773\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 11459.8096\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 10818.3691\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 11770.3125\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 10946.0820\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 11192.8447\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 11289.3633\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 10532.2783\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 10956.2188\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 11402.2539\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 11271.6504\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 11404.2607\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 10934.5742\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 10252.1289\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 11124.6191\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 10756.0898\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 10400.7100\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 10872.6045\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 11035.4355\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 11135.6270\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 10907.9707\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 11241.2178\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 11386.3594\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 11248.3818\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 11364.3506\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 10954.0723\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 11186.2734\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 11291.6270\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 10968.9668\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 11051.7881\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 10928.6895\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 11103.3887\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 10724.3398\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 10829.2529\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 10828.7236\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 10448.4238\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 10397.0664\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 11707.6133\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 10449.8652\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 11077.2246\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 11239.2930\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 11046.5615\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 10462.7129\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 11398.0723\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 11270.5195\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 10705.7285\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 11563.8525\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 10841.5107\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 11229.5117\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 10610.0898\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 11064.4277\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 10969.5801\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 10828.9668\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 11443.6055\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 10986.9160\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 10845.9365\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 11311.1035\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 11240.6582\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 10937.6797\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 11018.0879\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 11424.5439\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 10626.8701\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 11234.3789\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 11391.7549\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 10819.1758\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 10957.4922\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 10920.9121\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 10649.9199\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 10635.0986\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 11125.6396\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 10760.6582\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 10307.4492\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 10462.0498\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 10700.3320\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 10816.8438\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 10819.5889\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 11074.0342\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 10332.7969\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 10549.6768\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 10882.5098\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 10392.8115\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 10604.9033\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 11156.6895\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 10879.4805\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 10617.9590\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 11092.7041\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 10404.3926\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 11229.6631\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 11591.1875\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 10344.0410\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 10639.1182\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 10667.5566\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 10795.8223\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 10148.5107\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 10392.8535\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 10785.8604\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 10529.4092\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 10499.9316\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 11403.1465\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 11066.8135\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 10822.5078\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 10543.1172\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 10792.6885\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 10603.6934\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 11014.8809\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 10472.5703\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 10711.4277\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 11073.9053\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 10631.4482\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 10343.6240\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 10734.2559\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 10708.5938\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 11079.5469\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 10211.0127\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 11064.3203\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 10621.6797\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 10847.8730\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 11078.5049\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 10847.5332\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 11097.7275\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 10535.6064\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 10759.4434\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 10035.8887\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 11212.7510\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 10711.1934\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 10686.5010\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 10321.7344\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 11174.7812\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 10866.9795\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 10604.7227\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 10507.6201\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 10711.6992\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 10632.9033\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 10387.7812\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 10673.5674\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 10308.4795\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 10874.5059\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 11240.4912\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 11146.9258\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 10113.4297\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 10235.1348\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 10093.3486\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 11127.3691\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 10809.1211\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 10305.1543\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 10595.1123\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 10210.6768\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 10781.0879\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 11243.7148\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 11123.0840\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 10405.8516\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 10385.8301\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 10361.9883\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 10831.5039\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 10692.3018\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 10289.0703\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 10165.4668\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 11017.1709\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 10894.3027\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 10279.4863\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 10281.8691\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 10642.2812\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 10970.0801\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 10682.0840\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 10294.6807\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 10941.7383\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 10482.2773\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 10418.9434\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 10575.7500\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 9870.4014\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 10781.4170\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 10496.0205\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 10882.9062\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 10736.9395\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 10433.6992\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 9746.0273\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 10354.9492\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 10652.4766\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 10284.9766\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 10260.8545\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 10547.6865\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 10214.1436\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 9968.6211\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 10595.1406\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 10323.4648\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 10207.7686\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 10597.9180\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 10466.3711\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 10315.4561\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 10759.1279\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 10244.2266\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 10372.5742\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 10944.9277\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 10819.5303\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 10522.1084\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 10616.5293\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 10559.9883\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 10358.7158\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 10323.0361\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 10203.4219\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 10403.8701\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 10632.9688\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 10206.5508\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 10382.8457\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 10617.7461\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 10514.3604\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 10939.9688\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 9908.7861\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 10370.9668\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 9935.5039\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 10224.1777\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 10776.2793\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 10234.6641\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 10483.3740\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 10688.2520\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 10213.0400\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 10534.6240\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 10339.5117\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 10811.3848\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 10217.1602\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 10088.6826\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 10197.7734\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 10241.2373\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 10359.2646\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 10019.4561\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 10474.6553\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 10160.2256\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 10689.0547\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 9981.7207\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 10227.5391\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 10146.9453\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 10011.2061\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 9962.2617\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 10246.3203\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 10377.2500\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 9888.4092\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 10831.3096\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 10058.3613\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 10472.0957\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 10103.4131\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 10998.3965\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 9781.2695\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 10757.3066\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 10099.7754\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 10383.1074\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 9953.2598\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 9864.1523\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 10035.2803\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 10153.2725\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 10418.9980\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 10195.1436\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 10159.0654\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 10333.1855\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 10085.4189\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 9760.2930\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 10747.7354\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 9845.3691\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 10725.5967\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 9967.6484\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 10375.3184\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 10035.1289\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 10113.7324\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 10608.2783\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 9908.4062\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 10272.8730\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 10154.8359\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 9972.2402\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 10087.6465\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 10312.2773\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 10164.3691\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 10009.5361\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 10486.8066\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 10290.5449\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 10234.5996\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 10554.2363\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 9775.1289\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 9835.8516\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 9926.5898\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 10301.5234\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 9830.3262\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 10357.8877\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 10050.5654\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 10019.2324\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 10624.2910\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 9915.6494\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 10551.2764\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 10261.2490\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 10604.0479\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 10014.8242\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 10158.3350\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 10313.9707\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 10736.2012\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 10004.3145\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 10223.8750\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 9553.4277\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 9621.4785\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 9889.9707\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 9472.5205\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 10015.6416\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 10191.7266\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 10032.0352\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 10160.3418\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 11010.5742\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 10113.0811\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 10073.8984\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 10673.9268\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 10295.3818\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 9664.4072\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 10026.0537\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 10690.1230\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 10405.3457\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 10071.0371\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 10454.1592\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 10084.1025\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 10083.0537\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 9872.0449\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 10118.1240\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 10244.5430\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 9822.1875\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 9933.6650\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 10439.7070\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 10694.7275\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 10151.1162\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 9786.3770\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 10086.6826\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 9712.8613\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 10034.1221\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 10272.6875\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 9951.6367\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 10133.5508\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 9225.0586\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 10378.8047\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 10132.3105\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 9685.0840\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 10077.4824\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 10062.9336\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 10338.7754\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 10333.5195\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 9484.1846\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 9744.4434\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 9975.9727\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 9420.4062\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 9912.1328\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 9766.5205\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 10313.1914\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 10066.9336\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 9684.8135\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 9992.9590\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 9528.2695\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 9903.1445\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 9395.8203\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 9673.8027\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 9295.5146\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 9881.0098\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 10263.1338\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 10171.7207\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 10201.2471\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 9916.9268\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 9221.9785\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 9885.5508\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 9942.9219\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 10338.6650\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 10125.2227\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 9992.5195\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 10057.8467\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 9681.2520\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 10629.0898\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 10110.7334\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 10168.6797\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 9988.5674\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 9846.6250\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 9914.1660\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 10119.7188\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 9763.6191\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 9883.1523\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 9709.5664\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 9566.6279\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 9796.6719\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 9651.9912\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 10147.7090\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 9441.4590\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 9313.1416\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 9634.3613\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 9653.4756\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 10111.9766\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 9855.4609\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 9701.7021\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 9507.7148\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 9418.4668\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 9300.0352\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 9394.1191\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 9754.5410\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 9803.7520\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 9902.5820\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 9552.9590\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 10361.5225\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 9476.3838\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 10529.4824\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 10114.8604\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 9593.2227\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 9235.0381\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 9813.8594\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 10149.7754\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 9891.1348\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 9433.3125\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 9742.4307\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 9666.6494\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 10012.8291\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 9899.7012\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 9379.8408\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 9958.1602\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 9580.9287\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 9686.1387\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 9471.9766\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 9753.8877\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 9458.6328\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 9702.6660\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 9852.8242\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 9232.1035\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 9221.0371\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 9357.1260\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 10177.1973\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 9713.2129\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 10036.6748\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 9735.7188\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 9889.5693\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 9343.3115\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 9454.0547\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 9660.2979\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 9626.1113\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 9506.0771\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 9215.2598\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 9881.9258\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 10050.5430\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 9812.8984\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 9721.6768\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 9337.0781\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 9079.5098\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 9222.6445\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 9362.4727\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 9582.7871\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 9932.9258\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 9460.7949\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 9674.7520\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 9207.6270\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 9223.1953\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 9306.0957\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 9623.3711\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 9664.1436\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 9828.1035\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 9751.8799\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 9958.8486\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 9942.4922\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 9374.2988\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 9892.9688\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 9655.4316\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 9624.9932\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 9868.4863\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 9551.4307\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 9718.9014\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 9522.0996\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 9461.7354\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 9531.3721\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 10026.4053\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 9211.0234\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 9490.9980\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 9413.3438\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 9543.9043\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 9288.1865\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 9295.9834\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 9345.8477\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 9227.7168\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 9343.7656\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 9036.7441\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 8949.6982\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 9594.3262\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 10054.7725\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 9438.6895\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 9064.6309\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 9331.6953\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 9659.8643\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 9554.5059\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 8991.7383\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 10018.2119\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 9297.5449\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 9400.7148\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 9401.6543\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 9294.8984\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 9156.5508\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 9816.8965\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 9561.3076\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 9368.5996\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 9441.8047\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 9815.6963\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 9155.6289\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 9446.5107\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 9642.0293\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 9519.0215\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 9670.5352\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 9359.1660\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 9863.7119\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 8727.9531\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 9276.9209\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 9012.3457\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 9085.3203\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 9217.0273\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 8901.7266\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 9830.7988\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 9171.3516\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 9629.6699\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 9887.6787\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 9876.7812\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 9259.0820\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 8907.3975\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 9103.6328\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 9014.8867\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 9314.1094\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 9588.5840\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 9923.8164\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 9535.3145\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 9441.8164\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 9130.1309\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 9347.7969\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 9442.5020\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 9501.1992\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 9641.3945\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 9281.8008\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 9570.8057\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 8987.7568\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 9776.6299\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 10066.5518\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 9201.0918\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 9202.8711\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 9434.7754\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 9354.1826\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 9319.9609\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 8931.3057\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 9347.5117\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 9646.5918\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 8890.5645\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 8992.1836\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 8923.8789\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 8893.1562\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 9778.0811\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 9014.9785\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 9512.0596\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 9192.2520\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 9452.1191\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 9177.2090\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 9669.6357\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 9095.0996\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 9584.1504\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 8935.5107\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 9265.8916\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 9379.0801\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 9091.7461\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 8833.2285\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 8822.6621\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 9517.0547\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 9062.9414\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 9313.6631\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 9209.7715\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 9285.3652\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 9474.3623\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 9009.0146\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 8722.5586\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 9362.3066\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 9361.5439\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 9672.4707\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 8956.6270\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 9857.1152\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 8952.7842\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 9310.4092\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 9104.8301\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 9463.2930\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 9318.7168\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 9514.5547\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 8793.9375\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 9314.3818\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 9382.4277\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 9196.3564\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 8617.8467\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 9521.1504\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 9333.0586\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 9067.3516\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 9982.5684\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 9523.1875\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 9426.3604\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 9451.5654\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 9132.6191\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 9205.0000\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 9506.9531\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 9403.1230\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 9325.6387\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 9502.0703\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 8985.9961\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 9611.0986\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 9335.6748\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 9431.5566\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 8964.9707\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 9254.0234\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 8915.6426\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 9576.5215\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 8808.7305\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 8886.6641\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 9061.6826\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 9047.1367\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 9009.9971\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 8779.1768\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 9297.4961\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 8836.8125\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 9133.9355\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 8947.3496\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 9102.3828\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 9001.7842\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 9384.2168\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 8723.2236\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 9626.2832\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 9477.7080\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 9168.6758\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 9032.5840\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 9393.2285\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 8862.1025\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 8855.0889\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 8886.7012\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 8753.6191\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 8994.9268\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 8907.7168\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 8571.8838\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 9240.7129\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 8581.6270\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 9186.4277\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 8872.5703\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 8673.7627\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 9109.9990\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 8750.2959\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 8898.1230\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 8760.0312\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 9321.4336\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 9402.7461\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 9060.7773\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 9186.1113\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 8903.9434\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 9106.3203\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 8807.1328\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 8910.2930\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 9090.4736\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 8975.3740\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 9067.6318\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 8901.4902\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 9172.2188\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 9094.4375\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 9133.1484\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 8897.9619\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 8725.3662\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 8715.8584\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 8785.5195\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 9421.2900\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 8610.1113\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 8975.5234\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 8849.9043\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 9254.4766\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 8953.6318\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 9224.2744\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 8878.6299\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 8951.8457\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 8859.9883\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 9280.2646\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 9002.6982\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 8913.7803\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 8934.3262\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 8926.0488\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 9088.3008\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 9134.9297\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 8862.1484\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 8983.7441\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 8367.8223\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 9172.7168\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 8971.6738\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 9190.7246\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 8514.6826\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 8876.6094\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 8546.3613\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 9129.1738\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 8738.7119\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 9000.9531\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 9193.6025\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 8864.6270\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 9087.7354\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 8837.4531\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 8504.9277\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 9028.6240\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 8831.8047\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 8385.7344\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 8892.7617\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 9061.4297\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 9306.4121\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 8975.7041\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 8865.9316\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 8605.8789\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 9149.4609\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 8442.4902\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 9113.9814\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 8989.7168\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 8963.3379\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 8648.0107\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 8939.1680\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 8926.6562\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 8951.0723\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 8920.1230\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 8635.6426\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 8715.5879\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 8911.0898\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 8850.0469\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 8511.0000\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 8755.5977\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 8871.6797\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 8903.8965\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 8643.0684\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 8968.1523\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 8610.1699\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 8294.5098\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 9008.7236\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 9092.4883\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 9136.8740\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 8787.8535\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 8703.8467\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 8534.9131\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 9064.5967\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 8597.5137\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 8774.7344\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 8595.5947\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 8410.5479\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 8884.6602\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 8617.5371\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 8284.2373\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 8210.3740\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 9180.4434\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 8454.2812\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 8989.6709\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 8598.1699\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 8683.2656\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 8563.4414\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 8486.0332\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 8382.1084\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 8678.6924\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 8750.5371\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 8767.2930\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 8432.7129\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 8610.5146\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 8425.7178\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 8968.9629\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 8511.3301\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 8783.7002\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 8677.1641\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 9072.9727\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 8802.4785\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 8861.8828\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 8650.8457\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 8443.5713\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 8873.5527\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 8402.8867\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 8970.5117\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 8339.1865\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 8447.7344\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 8957.3379\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 8830.1172\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 8423.8564\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 8373.4785\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 8505.0752\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 8897.1875\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 8688.6641\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 8640.9473\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 8580.8320\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 8676.1074\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 8578.3262\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 8846.3594\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 8405.9570\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 9043.4072\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 9187.9238\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 8896.3906\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 8520.4141\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 8757.7109\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 8207.6357\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 8542.0625\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 8718.0430\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 8386.2109\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 8576.3330\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 8685.1064\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 8109.6006\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 8498.1719\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 8580.1152\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 8852.4805\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 8947.9746\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 8249.0586\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 9153.7715\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 8377.0312\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 8395.3301\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 8596.8184\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 9107.8926\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 8317.7402\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 8542.5762\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 9149.5107\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 8358.1621\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 8765.2344\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 8456.5215\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 8285.1787\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 8226.8145\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 8126.7983\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 8350.3027\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 8705.9883\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 8441.8184\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 8567.1367\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 8201.0605\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 8219.8242\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 8452.3125\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 8769.7373\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 8246.6152\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 8241.1562\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 8205.9180\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 8512.7012\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 8387.2930\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 8673.1299\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 8650.1855\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 8203.8555\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 8819.6680\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 8705.4141\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 8830.4121\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 8635.2031\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 8238.4893\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 8281.2842\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 8261.3936\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 8500.3457\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 8449.1768\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 8070.0566\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 8731.3760\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 8628.7314\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 7897.1992\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 8568.5547\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 8581.3350\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 8800.9297\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 8201.8389\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 8877.5137\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 7916.5869\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 8300.6172\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 8469.5176\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 8405.6836\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 8337.4033\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 8134.2764\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 8302.7207\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 8495.8340\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 8123.7935\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 8303.5078\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 8669.8672\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 8137.8516\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 7825.9043\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 8706.7734\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 8274.3066\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 8776.1172\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 8631.3535\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 8292.8750\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 8173.5410\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 8478.8613\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 8570.3525\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 7914.1143\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 8481.1826\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 8049.3608\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 7967.2178\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 8541.5312\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 8210.3379\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 8183.8945\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 8429.5527\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 8122.2817\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 8433.1904\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 8674.5938\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 8420.9707\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 8115.8574\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 8550.5410\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 8696.7246\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 8415.2627\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 8190.1401\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 8389.7764\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 8472.0527\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 8137.9062\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 8551.1895\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 8295.9092\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 8468.4395\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 7917.0010\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 8260.0576\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 8180.1191\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 8037.6709\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 8153.2925\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 8253.2412\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 8409.6240\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 8157.4790\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 8092.4404\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 8211.7412\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 8707.5625\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 8076.9722\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 8498.4531\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 8160.9028\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 7960.0376\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 7859.0801\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 8336.6641\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 8623.2930\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 8306.2676\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 7749.3979\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 7838.1826\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 8481.4531\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 8317.4414\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 8356.3945\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 8005.9692\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 8353.1523\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 8044.7065\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 8178.9736\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 8558.1426\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 8209.7100\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 8142.1143\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 8365.3574\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 7981.1182\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 7794.2227\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 7941.3027\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 8494.9092\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 8127.4404\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 8170.1382\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 8128.3877\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 8470.9531\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 8345.0879\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 8608.4678\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 8032.7432\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 7885.1270\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 7797.0332\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 8071.6592\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 7769.5010\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 8367.4326\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 7690.8770\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 8042.9365\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 8072.1201\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 7836.3564\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 8154.2031\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 7733.6362\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 8349.2891\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 8468.9180\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 8316.8770\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 8342.9453\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 7981.7109\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 7930.6787\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 7775.9917\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 8409.8936\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 8425.5957\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 8073.3945\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 7909.1680\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 8494.4443\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 8250.3428\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 7922.9580\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 8222.4531\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 8075.2949\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 7874.8193\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 8043.0293\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 7954.9873\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 8186.4170\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 8317.9746\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 7654.2656\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 8289.9883\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 7939.9385\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 7684.0342\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 8448.6846\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 8138.5630\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 7688.2622\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 7983.3818\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 8361.8145\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 8381.1250\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 8181.2700\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 8622.4873\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 7794.9917\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 8177.1582\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 7948.8184\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 8901.9062\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 7620.1299\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 8401.0840\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 7905.8560\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 8437.0244\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 7717.9785\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 7812.0889\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 7788.6528\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 7724.7744\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 8051.2373\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 7889.6641\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 7872.0820\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 7691.3145\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 7938.0811\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 8136.8857\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 8043.2021\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 7603.2446\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 7606.8984\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 7843.6406\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 7845.5459\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 7595.1006\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 8160.2617\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 7938.5635\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 7826.1855\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 7756.4785\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 8142.2583\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 7805.4980\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 7910.5303\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 7828.9453\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 8079.3359\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 7918.9668\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 7820.2012\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 8253.5703\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 8145.8711\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 8147.8867\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 8471.5547\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 7386.3062\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 7813.1777\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 8248.7754\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 7422.6655\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 7895.6304\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 7910.5459\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 9183.2715\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 7829.9473\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 7621.1943\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 8123.4482\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 8007.4971\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 7730.7100\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 8114.4756\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 8137.9800\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 7640.8076\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 7867.2915\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 7509.2515\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 7868.1694\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 8262.2793\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 7772.4902\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 7928.5996\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 7541.8911\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 8149.0737\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 8000.0449\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 7932.1572\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 7835.7817\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 7623.4004\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 7840.0474\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 7716.9424\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 8011.9927\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 7434.3398\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 7491.5117\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 7540.0435\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 7572.8252\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 8247.3613\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 7749.1611\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 7414.5654\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 7706.5908\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 7781.2280\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 7887.9131\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 8099.4697\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 7591.7832\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 7603.8535\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 7519.3740\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 7996.2461\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 7942.0942\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 7834.1509\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 7793.3145\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 7523.2402\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 8334.1836\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 7880.0488\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 8189.5767\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 8022.6553\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 7982.8477\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 8052.6592\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 7730.2812\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 7581.0156\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 8038.3584\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 7711.5596\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 7786.2773\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 7783.3706\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 7841.9087\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 7563.8408\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 7601.1924\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 7668.4648\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 7289.1646\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 8099.1909\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 7718.9795\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 7972.4238\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 7707.4307\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 7800.7197\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 7984.4834\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 8040.9199\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 7531.7842\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 7918.1992\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 7703.4570\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 7365.5796\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 7751.1216\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 7685.2349\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 7377.6919\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 7854.0684\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 7563.7310\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 7743.2866\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 7522.4897\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 7648.8418\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 8018.0186\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 7813.1958\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 7402.6738\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 8107.9482\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 7492.4424\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 7314.3599\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 7579.4302\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 7805.1758\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 7675.6851\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 7672.1724\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 7765.8965\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 7813.5161\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 7595.8125\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 8081.3418\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 7817.7627\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 7243.5903\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 7504.9268\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 7743.0322\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 7657.1958\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 7728.9229\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 7539.6270\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 7888.8076\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 7881.8232\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 7473.3745\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 7368.8877\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 7535.5352\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 7578.1816\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 7520.8311\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 7055.5840\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 7848.0332\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 7549.7983\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 7541.4238\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 7835.5166\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 7714.3076\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 7260.7544\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 7628.1250\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 7517.5107\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 7547.9932\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 7097.9766\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 7949.0264\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 7632.2456\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 7661.0176\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 7565.6709\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 7420.9595\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 7331.6655\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 7335.2944\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 8146.2598\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 7829.5684\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 7336.5542\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 7494.6025\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 7757.8730\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 7725.5605\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 7398.2168\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 7721.3232\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 7913.0781\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 7668.2041\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 7219.2021\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 7664.6572\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 8137.3428\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 7833.3916\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 7863.1782\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 7346.4707\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 7439.5801\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 7521.7246\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 7693.6963\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 7863.3047\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 7302.1807\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 7407.9209\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 7570.9043\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 7679.8403\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 7503.0000\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 7717.7144\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 7649.5854\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 7484.3784\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 7445.8838\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 7350.3267\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 7676.3496\n",
      "âœ… Epoch 7 Avg Loss: 9373.3511\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[526 594]\n",
      " [591 522]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4610\n",
      "\n",
      "ğŸ” Epoch 8/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 7642.3872\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 7233.9482\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 7738.7334\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 7581.6587\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 7374.3047\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 7309.9438\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 7589.7861\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 7440.3838\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 7639.3066\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 7538.9033\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 7805.7695\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 7410.8213\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 7549.0127\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 7650.4990\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 7547.8369\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 7357.0518\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 7521.1738\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 7457.0132\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 7424.6528\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 7380.3506\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 7772.3311\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 7712.3613\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 7751.9277\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 7538.5864\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 7409.7207\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 7642.8872\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 7467.7666\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 7497.7817\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 7755.3193\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 7266.9121\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 7435.4038\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 7411.6953\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 7481.9951\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 7608.5205\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 7141.7744\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 7665.2217\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 7272.6514\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 7587.1084\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 7323.5664\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 7659.1226\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 7525.5366\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 7450.5889\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 7405.3013\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 7825.3262\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 7880.2192\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 7098.0112\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 7296.3564\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 7741.0552\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 7420.3516\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 7272.2964\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 7884.7559\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 7294.1372\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 7666.7651\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 7391.9629\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 7174.9307\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 7310.1611\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 7281.5425\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 7088.4639\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 7637.5781\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 7384.2158\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 7256.8281\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 7272.5869\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 7386.3809\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 7163.9541\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 7002.7041\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 7204.5137\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 7306.3276\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 7243.7295\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 7639.7568\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 7405.9165\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 7220.9668\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 7519.7295\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 7267.4854\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 7051.1055\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 7102.2109\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 7028.6094\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 7032.0620\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 7195.9326\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 6855.7949\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 7427.4756\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 6851.4043\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 7337.6582\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 7256.7183\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 7578.0444\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 6952.2354\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 7596.3438\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 7616.1621\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 7318.6191\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 7100.5669\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 7044.6523\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 7024.0244\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 7423.3086\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 7508.0850\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 7670.2900\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 7364.5723\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 7144.4087\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 7362.0557\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 7233.3784\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 7554.9961\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 7191.7549\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 7438.7100\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 7764.1221\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 7192.5400\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 7314.0391\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 7581.0933\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 7135.3877\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 7469.7432\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 7073.2314\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 7613.4272\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 7039.5820\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 7581.6660\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 7242.2217\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 7141.6689\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 7510.2944\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 7049.0444\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 7226.3340\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 6997.3008\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 7026.9326\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 7276.7979\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 7334.2383\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 7037.7593\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 7138.4062\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 7007.8936\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 7015.6382\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 6966.9131\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 7335.1655\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 7317.7764\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 6812.9717\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 7060.1074\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 6794.7510\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 7191.5342\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 7260.4258\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 7305.3408\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 6881.4404\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 7495.4473\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 7150.2852\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 7014.2002\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 7033.9478\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 7730.5176\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 7168.3545\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 7414.3730\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 7375.3105\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 7269.4194\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 7079.3330\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 7231.4033\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 7021.3105\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 7259.0322\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 7257.1260\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 7134.0195\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 7196.7769\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 7076.6230\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 7160.4736\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 7182.1157\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 7192.1191\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 7001.4199\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 7190.4482\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 7233.9863\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 6745.8384\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 6830.5059\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 7226.8369\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 7087.7402\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 6823.5103\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 7270.4038\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 6616.0176\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 7259.4346\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 7322.4673\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 7269.9365\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 6709.2705\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 6808.9053\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 7003.6099\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 7390.6816\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 7250.2524\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 6771.2197\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 6787.9531\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 7087.1572\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 7321.5977\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 7310.3105\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 7442.9258\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 7079.5410\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 7228.3037\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 7219.1367\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 6850.5176\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 6890.6191\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 7104.3560\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 7167.8550\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 6762.5479\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 7068.7446\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 6899.8633\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 7303.2681\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 7219.5835\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 6583.0737\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 7309.8887\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 7115.9351\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 7454.9590\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 7093.9453\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 6950.7490\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 7010.8755\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 7039.6113\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 6891.0576\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 6958.9072\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 7110.5410\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 7181.7422\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 7110.8975\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 6950.7207\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 7095.5581\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 6997.3911\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 6737.5518\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 7044.7559\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 7115.3809\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 7149.5947\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 7039.3672\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 6819.4570\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 7245.8828\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 7068.4922\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 6706.8398\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 7081.4048\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 6800.0635\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 7084.6836\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 6932.7373\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 7096.4351\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 7023.2466\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 7226.8198\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 7041.8306\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 7081.8472\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 7181.2373\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 7202.3535\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 7128.0166\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 6829.4375\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 7038.2134\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 6841.8906\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 7040.5132\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 7311.7363\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 7051.9512\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 7252.6665\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 7104.9727\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 6692.2046\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 7107.3247\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 7158.7539\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 6904.1729\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 6978.2524\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 7056.9272\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 7152.3325\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 6778.9043\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 6751.5967\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 6923.9336\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 7085.8271\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 6593.9053\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 6785.7158\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 6915.3491\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 7093.7896\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 6712.8936\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 6572.3765\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 6724.1772\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 6627.4570\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 6888.1455\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 7103.9902\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 6747.9541\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 6968.5830\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 7277.3789\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 6725.5542\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 6976.4014\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 6689.7490\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 6991.8682\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 6852.0283\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 6744.9619\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 6434.7871\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 6757.3711\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 6720.3857\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 6775.3213\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 6781.0889\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 6832.4922\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 6934.5586\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 6792.1865\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 6704.7334\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 7256.1855\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 6849.6860\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 6680.3203\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 6702.7373\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 7284.9697\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 6629.9287\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 6344.6221\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 6967.7656\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 6525.9111\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 6709.0483\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 6724.0957\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 6455.4375\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 6922.7432\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 6759.5601\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 6997.9990\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 7051.1509\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 6627.2832\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 6780.5918\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 6938.8828\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 6857.2109\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 7198.1499\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 6856.6748\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 6364.6055\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 6501.1533\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 6866.3691\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 6721.7261\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 6917.0088\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 6529.6318\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 6753.0215\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 6673.9346\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 6638.6094\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 6961.3354\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 6684.6250\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 6488.3535\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 6541.3984\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 6962.5908\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 6618.0264\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 6564.1699\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 6764.4038\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 6796.6724\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 6510.7949\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 6868.6992\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 6919.7549\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 6752.4746\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 6633.6953\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 6787.4307\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 6950.9248\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 6504.0205\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 6676.8657\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 6759.0176\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 6534.7002\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 6665.4873\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 6607.0591\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 6747.2275\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 6630.7344\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 6841.5352\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 6810.5293\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 7216.0420\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 6866.0898\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 6657.0137\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 6496.7686\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 6717.4805\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 6639.1479\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 6715.4619\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 6939.2759\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 6765.9678\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 6910.6816\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 6778.2256\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 6593.0186\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 6615.8394\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 6635.7803\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 7096.1504\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 6662.3965\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 6646.1104\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 6361.0693\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 6517.3086\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 6995.6899\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 6384.0537\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 6989.3101\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 6728.7178\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 6732.3022\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 6585.8213\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 6698.8022\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 6749.8535\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 6525.9766\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 6557.2686\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 6527.5576\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 6708.8027\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 6916.9824\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 6921.4277\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 6805.8984\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 6506.5811\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 6350.6104\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 6540.2910\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 6820.8022\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 6559.4473\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 6302.6660\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 6337.1055\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 6546.3896\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 6943.0806\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 6984.7778\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 6852.0225\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 6539.9521\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 6192.6851\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 6265.1196\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 6379.7266\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 6637.1118\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 6351.9497\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 6579.2397\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 6319.2856\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 6513.8691\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 6499.4775\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 6414.1143\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 6750.8975\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 6467.0205\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 6563.8218\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 6498.9175\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 7245.9639\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 6687.6338\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 6715.9077\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 6970.5723\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 6580.0605\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 6579.9189\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 6300.7070\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 6503.5303\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 6433.1387\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 6339.6221\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 6908.8291\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 6492.7539\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 6716.4443\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 6592.5312\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 6500.3047\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 6625.5986\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 6643.7021\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 6543.5576\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 6771.8179\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 6077.1885\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 6515.6338\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 6496.5054\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 6639.1973\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 6311.7368\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 6782.5986\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 6235.4077\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 6578.7095\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 6589.9717\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 6421.7461\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 6549.8022\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 6579.1426\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 6252.0830\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 6350.4790\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 6786.7163\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 6504.3887\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 6787.2642\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 6511.8364\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 6548.7974\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 6395.9956\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 6416.4604\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 6787.9404\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 6459.5786\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 6462.6406\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 6355.5098\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 6534.6924\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 6504.6211\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 6496.3779\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 6544.0757\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 6163.5303\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 6208.2334\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 6825.0986\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 6716.2925\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 6556.4395\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 6485.3564\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 6558.0317\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 6477.7627\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 6390.2222\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 6462.4258\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 6482.6660\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 6395.3115\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 6315.7891\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 6483.4043\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 6204.0034\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 6573.2803\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 6148.6143\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 6034.0244\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 6674.6465\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 6437.2842\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 6688.5049\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 6189.6523\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 6512.5771\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 6428.2954\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 6262.5781\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 6685.2642\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 6237.8926\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 6475.7900\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 6518.8350\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 6564.5732\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 6468.3984\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 6821.2070\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 6295.1338\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 6358.5166\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 6077.0684\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 6669.1787\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 6232.8799\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 6541.2412\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 6438.1836\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 6259.5752\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 6629.3281\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 6571.6567\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 6748.5537\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 6435.0068\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 6458.3984\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 6424.5869\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 5937.3208\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 6608.2842\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 6524.4619\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 6354.8115\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 6324.9199\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 6319.5859\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 6407.6807\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 6635.8467\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 6213.0239\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 6506.0117\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 6262.5977\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 6147.1631\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 6320.8652\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 6123.7637\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 6260.0801\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 6298.9824\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 6050.8389\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 6292.3110\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 6378.4878\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 6312.1719\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 6332.9355\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 6451.7251\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 6564.0122\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 6337.2070\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 6228.7822\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 6573.9668\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 6295.3604\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 6297.5039\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 6153.1943\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 6252.9961\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 6120.1162\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 6256.8408\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 6377.3823\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 6141.8652\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 6191.6572\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 6557.9839\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 6567.4121\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 6213.1011\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 6220.0703\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 6465.2739\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 6207.3037\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 6412.8457\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 6307.8486\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 6212.1914\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 6230.6221\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 6202.0176\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 6229.6992\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 6218.9326\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 6443.9229\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 6261.1050\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 6108.3955\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 5967.2451\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 6349.8936\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 6184.7744\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 6338.7275\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 6442.9204\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 6434.9766\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 6142.5811\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 6039.1914\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 6477.7891\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 5922.6963\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 6026.7085\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 6340.7715\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 5967.1636\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 6257.4121\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 6128.9780\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 6150.9717\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 6302.3662\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 6043.1763\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 6224.4814\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 6068.0254\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 5962.6836\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 6381.8159\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 6382.7300\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 6162.8223\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 6282.5176\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 6460.3535\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 6234.2988\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 6134.4658\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 6068.0386\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 6081.8823\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 6228.8271\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 6122.0938\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 5906.5273\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 5871.3213\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 6258.7031\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 6209.0044\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 6167.0557\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 6100.9697\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 6187.8730\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 6259.4453\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 6219.1943\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 6364.9424\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 6107.6899\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 6126.2568\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 6226.7222\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 6370.8477\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 6538.0635\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 5952.6504\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 5919.3682\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 6275.0190\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 6345.6084\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 6183.5200\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 6108.2661\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 6270.2217\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 6194.7642\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 6154.0146\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 6086.6138\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 6006.5688\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 6582.3721\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 6046.9443\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 6383.1738\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 6274.2007\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 6053.0723\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 6027.0342\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 6344.9443\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 5971.5312\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 6393.7036\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 5895.0601\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 6233.6250\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 6405.8379\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 6368.1504\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 6249.0562\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 6302.9404\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 6176.7090\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 6090.6587\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 6411.7373\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 5734.6211\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 6608.0488\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 5926.8789\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 5791.9585\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 6189.3589\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 6179.2783\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 5961.8896\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 5721.5869\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 5886.3467\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 6033.1934\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 5947.5723\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 6268.8115\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 5956.9912\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 5975.5283\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 5981.5146\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 5884.2520\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 5895.7109\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 6082.3774\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 6058.6035\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 6211.2607\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 6046.5000\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 6069.2046\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 6118.8379\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 5989.7290\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 6198.1802\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 5817.5439\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 6276.5762\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 6128.3052\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 6059.1621\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 6097.2227\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 5984.8638\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 6105.1562\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 6076.1396\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 5744.0117\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 6122.0020\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 6207.4951\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 6060.6060\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 5788.1035\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 5999.1270\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 5900.9927\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 6253.2939\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 6219.3916\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 6182.5977\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 6137.2407\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 6230.7578\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 5925.8457\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 5784.9966\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 5905.8862\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 6208.7412\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 6041.5244\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 6296.0312\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 6201.8213\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 5813.1201\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 5672.6250\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 6127.7324\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 6121.1504\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 5994.5513\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 6054.4033\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 5972.9297\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 6333.1035\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 6128.2764\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 6209.2319\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 6083.2500\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 5706.4331\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 6071.6104\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 5968.2822\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 6064.7124\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 5749.0176\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 5947.4253\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 5876.1704\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 6003.0259\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 6090.6318\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 5763.8125\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 5741.0400\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 6137.6045\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 6136.5977\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 5875.1797\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 5955.0049\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 5896.8926\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 5840.8730\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 6000.7476\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 6098.9199\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 6073.1182\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 6316.0801\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 5798.6982\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 5555.6382\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 5670.3091\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 5976.9336\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 5821.0664\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 5662.0312\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 5866.1680\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 5937.5293\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 5900.9038\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 6041.1270\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 6262.2593\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 5864.4155\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 6105.5244\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 5737.2129\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 5979.1284\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 5956.7744\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 6043.4473\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 6133.1904\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 6162.8887\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 5840.6416\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 5768.9224\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 6259.5801\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 6211.8330\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 5773.6455\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 5945.5811\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 5814.2881\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 5780.7998\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 5962.9648\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 6006.3311\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 5522.5410\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 5975.4375\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 6075.4365\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 6057.8818\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 5856.4585\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 5720.9355\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 5639.8633\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 5752.7393\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 6004.4365\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 6145.8926\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 5874.3599\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 5985.9688\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 5693.9604\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 5957.1533\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 5995.1631\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 5944.7588\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 6032.6250\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 5824.5649\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 6009.6445\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 5888.6377\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 5699.4150\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 5981.4302\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 5989.4614\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 6050.9629\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 5979.4536\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 6037.8564\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 5648.8027\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 5753.0469\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 5623.0469\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 5746.4473\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 5973.8462\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 5656.4404\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 6097.0947\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 5650.8657\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 5674.6113\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 5917.0234\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 5556.2549\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 5754.7422\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 5806.4473\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 6002.8154\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 5758.5127\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 5453.6543\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 5810.1187\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 5870.1436\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 5766.7588\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 5866.1387\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 5590.8672\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 5408.1675\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 5593.6572\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 5607.6309\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 5519.0029\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 5908.6646\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 6017.3389\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 5555.4414\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 5903.5762\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 5982.7959\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 5727.7021\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 5723.8179\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 5886.8657\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 5865.1914\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 5682.6387\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 5665.0415\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 6143.5581\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 5897.7969\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 5685.0728\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 5517.9907\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 5886.9434\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 5680.5181\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 5649.0420\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 5982.1260\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 5845.2090\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 5713.5562\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 6041.2588\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 5929.9307\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 5675.9580\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 5415.3369\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 5868.3740\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 5766.2627\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 5782.0000\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 5868.1201\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 5804.7090\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 5556.8965\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 5739.9072\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 5760.4551\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 5576.9316\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 5621.0557\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 5588.8164\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 5987.9863\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 5606.1543\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 5814.6260\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 5798.3789\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 5574.9771\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 6059.8691\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 5534.9097\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 5405.6914\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 6199.0640\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 5433.4678\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 5628.7266\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 5567.9990\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 5880.6880\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 5814.8311\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 5846.9082\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 5380.8232\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 5533.2676\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 5995.2466\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 5573.7764\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 5596.0264\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 5957.9204\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 5759.8628\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 5597.9590\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 5238.0498\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 5743.0254\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 5655.6025\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 5859.5137\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 5480.8105\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 5488.5645\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 5419.0918\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 5776.2837\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 5634.4092\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 5537.0176\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 5555.5244\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 5559.5068\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 5633.7939\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 5984.8013\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 5741.5420\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 5374.5415\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 5534.2358\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 5626.7021\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 5905.6855\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 5514.4336\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 5372.5469\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 5493.2720\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 5668.4380\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 5638.4839\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 5560.3096\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 5442.5674\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 5579.0889\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 5647.3525\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 5414.3564\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 5417.1709\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 5332.5132\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 5718.9878\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 5612.9678\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 5310.0571\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 5732.2764\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 5547.0723\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 5945.4697\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 5484.2935\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 5624.0923\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 5643.0117\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 5383.8442\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 5598.9102\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 5514.4736\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 5539.1572\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 5510.3667\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 5367.3125\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 5376.0425\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 5388.7319\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 5706.0059\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 5713.5864\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 5507.1465\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 5682.6689\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 5529.5483\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 5441.3530\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 5870.8652\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 5695.2012\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 5741.5767\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 5524.3364\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 5640.5801\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 5642.5405\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 5698.3506\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 5350.8286\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 5583.2588\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 5182.9756\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 5531.7603\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 5272.3154\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 5767.3740\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 5521.4541\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 5391.4463\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 5515.0542\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 5443.5581\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 5570.1729\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 5202.9976\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 5439.1992\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 5411.2554\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 5324.5127\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 5492.6230\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 5542.0117\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 5315.0283\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 5532.3535\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 5434.7349\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 5645.4004\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 5650.4204\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 5600.2700\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 5525.9795\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 5847.5537\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 5680.0479\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 5650.4756\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 5460.1714\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 5336.7837\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 5476.7920\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 5430.4688\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 5576.4121\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 5298.7002\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 5657.4209\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 5761.4365\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 5326.3706\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 5454.7100\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 5407.6533\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 5453.5791\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 5460.2417\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 5426.1851\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 5450.6367\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 5570.2705\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 5388.3906\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 5767.1133\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 5446.8291\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 5325.9443\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 5186.7363\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 5200.4443\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 5448.4751\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 5582.2490\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 5612.7349\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 5568.6211\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 5248.1108\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 5597.0601\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 5527.5508\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 5645.3242\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 5273.4600\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 5278.0693\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 5574.0845\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 5203.1001\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 5438.8486\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 5317.0405\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 5435.1274\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 5548.8320\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 5268.8706\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 5526.9277\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 5204.8691\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 5449.7773\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 5325.2148\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 5219.3398\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 5439.8379\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 5441.5967\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 5482.4614\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 5486.1816\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 5334.0137\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 5374.4404\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 5270.0210\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 5202.2979\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 5582.8301\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 5330.5479\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 5663.6133\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 5229.6050\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 5149.6914\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 5320.7275\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 5258.0645\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 5509.8311\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 5458.6846\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 5483.5508\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 5290.8125\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 5348.6006\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 5647.5889\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 5586.8267\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 5586.2969\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 5341.6504\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 5439.5117\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 5395.0630\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 5410.9741\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 5675.2290\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 5433.7744\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 5188.7617\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 5487.9189\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 5232.8027\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 5199.0063\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 5386.6914\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 5453.0928\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 5170.5283\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 5354.9961\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 5553.1328\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 5440.1973\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 5304.3267\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 5438.4639\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 5172.7061\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 5553.0825\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 5248.0889\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 5093.5107\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 5127.1646\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 5341.2217\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 5124.7256\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 5298.8760\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 5186.8047\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 5340.0513\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 5308.7769\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 5237.1694\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 5214.6050\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 5287.8320\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 5396.3110\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 5006.2148\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 5125.6240\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 5242.5352\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 5246.0381\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 5485.4146\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 6183.9028\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 5524.4360\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 5572.4980\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 5383.5825\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 5325.2520\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 5305.9443\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 5359.2603\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 5612.2124\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 5222.7393\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 5110.9839\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 5405.5850\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 5072.2856\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 5143.5186\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 5394.2373\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 5152.2832\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 5243.1353\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 5341.5630\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 5492.9092\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 5123.5254\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 5355.6411\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 5181.6172\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 5439.2695\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 5306.1699\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 4980.6221\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 5122.4771\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 5241.0156\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 5178.5576\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 5292.3096\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 5111.3467\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 5405.2100\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 5416.6406\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 5583.6670\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 5095.5098\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 5255.1997\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 5100.8633\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 5152.0371\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 5131.5034\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 5128.9238\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 5488.0303\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 5301.5254\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 5296.0776\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 5208.6953\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 5142.2139\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 5333.5059\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 5402.4312\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 4918.6865\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 5150.8291\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 5072.1235\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 5350.1064\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 5474.1270\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 5346.8936\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 4960.8433\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 5024.1943\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 5444.3916\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 5133.1240\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 5060.8540\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 5051.2749\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 5265.9512\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 5068.1934\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 5490.6445\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 5124.3271\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 5135.4307\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 5157.9282\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 5131.9795\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 5682.9629\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 4987.9917\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 5354.3442\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 5125.5181\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 5406.6094\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 4982.4277\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 5284.4785\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 5211.7783\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 5388.9951\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 5026.5112\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 5337.1523\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 5122.7197\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 5174.1367\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 5367.9043\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 5013.5186\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 5279.8447\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 4909.0781\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 4994.3740\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 5012.1792\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 5024.9844\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 5309.4614\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 5173.2920\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 5005.1577\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 4920.0830\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 5205.1084\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 4793.0264\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 5191.0264\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 5336.6099\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 5410.6504\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 5117.9170\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 5297.1660\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 5176.6270\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 5129.5142\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 5438.6045\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 5081.8682\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 5137.0371\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 5024.9688\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 5132.0547\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 5059.9019\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 4969.2231\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 5012.5952\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 5301.3389\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 5143.1670\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 5083.8242\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 5042.7266\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 5319.0928\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 5174.9746\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 5172.8848\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 4923.3174\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 5035.5815\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 5174.1396\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 5128.0742\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 5196.1299\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 5021.2124\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 5095.7632\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 5063.2998\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 5077.6729\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 5059.2812\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 4849.0928\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 5009.4844\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 4899.1387\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 5057.8154\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 5092.7480\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 5312.4888\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 5064.8633\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 5104.2988\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 5333.0664\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 5197.0303\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 4928.6704\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 4937.2314\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 5082.9248\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 5204.7080\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 5139.0322\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 5127.0254\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 4819.0215\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 5245.7847\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 5055.2241\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 5280.8159\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 5153.4023\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 4792.9189\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 5150.0576\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 5352.5830\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 5285.3130\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 4759.1426\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 5148.7021\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 5148.8501\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 4706.9429\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 5071.9976\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 4871.7974\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 5194.7915\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 5097.1787\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 4863.1958\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 5114.6006\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 5478.9858\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 4982.5342\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 4908.1631\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 5161.9121\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 5063.1133\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 5051.5181\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 4886.7534\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 4877.8896\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 5464.1260\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 4970.6948\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 5080.3564\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 4948.6396\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 4940.7632\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 5035.7173\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 5354.3994\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 4796.0488\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 4797.4546\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 4918.3115\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 5068.5137\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 5184.3359\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 4873.0566\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 5054.8037\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 5152.1992\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 4961.8555\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 5212.3892\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 4987.4600\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 5143.6738\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 5079.7969\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 5083.1367\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 4896.2725\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 5164.8613\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 4769.0928\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 4934.8301\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 5055.3789\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 4962.4775\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 4990.1318\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 4983.9414\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 5123.7422\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 5121.7939\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 4975.5557\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 4878.1172\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 4821.8438\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 4834.0391\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 5149.4556\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 4986.0430\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 4755.3564\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 5015.3799\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 5018.1582\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 5000.6387\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 4892.3760\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 4881.8799\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 4915.5166\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 4964.1416\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 4869.7905\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 5053.7773\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 4798.6289\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 5118.3955\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 4916.8540\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 4883.8145\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 4720.3906\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 5242.3418\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 5186.8496\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 4795.4336\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 4878.5044\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 4788.3765\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 4699.6611\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 4998.6392\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 5017.5269\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 4908.9355\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 5155.7505\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 5094.2788\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 4792.6792\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 4797.4160\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 5092.0088\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 4892.6152\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 5036.5537\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 4807.0044\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 4926.6309\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 4635.4834\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 5016.3882\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 4875.5005\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 4912.8672\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 5048.5474\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 4839.7881\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 4994.5273\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 4838.1133\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 4797.2407\n",
      "âœ… Epoch 8 Avg Loss: 6109.1657\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[528 592]\n",
      " [589 524]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4627\n",
      "\n",
      "ğŸ” Epoch 9/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 4756.6670\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 4850.7012\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 4796.1450\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 4973.1797\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 5024.7524\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 4798.2651\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 5181.4336\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 4688.1592\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 4760.0400\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 4807.7456\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 4798.6631\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 4950.7744\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 4698.4512\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 4949.6050\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 5067.4199\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 4836.9043\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 4862.0723\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 5073.9102\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 4629.6108\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 4996.2739\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 5034.5586\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 4971.0625\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 4777.5049\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 5014.2515\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 4986.4854\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 4778.2305\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 4902.6963\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 4986.9531\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 4558.6172\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 4951.8472\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 4849.4897\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 5113.5811\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 4800.0361\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 5017.2246\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 4906.3291\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 4874.2275\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 4988.7656\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 4704.0264\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 4761.7051\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 4668.9697\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 4976.0283\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 4757.1274\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 4885.0410\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 4640.7358\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 4951.2812\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 4802.0571\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 4841.6621\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 4921.2207\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 4790.5791\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 4807.0664\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 4910.8193\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 4942.7383\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 4859.3623\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 4835.4629\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 4504.4673\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 4851.7622\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 4699.2559\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 4896.2671\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 4588.8066\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 4738.4033\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 4850.9751\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 4994.3555\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 4941.1265\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 4753.2759\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 4708.3086\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 4751.8052\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 4554.2700\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 4909.2930\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 4700.7319\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 4568.3633\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 4830.3418\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 4689.0547\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 4606.2109\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 4615.1113\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 4848.3691\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 4559.1826\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 4667.2881\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 4696.2334\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 4558.0811\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 4966.4775\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 4612.7705\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 4644.3018\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 4890.7715\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 4549.9409\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 4822.7568\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 5030.3589\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 4795.8105\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 4989.8657\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 4775.3145\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 4732.9658\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 4664.2373\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 4639.4971\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 4869.0444\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 4913.5029\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 4523.1382\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 4823.6426\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 4633.0293\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 5032.2495\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 4592.2354\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 4588.9771\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 5145.5166\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 4446.9189\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 4607.0933\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 4816.9092\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 4538.8379\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 4738.7275\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 4845.0083\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 4789.1807\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 5035.1021\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 4607.2344\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 4705.1768\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 4731.8652\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 4725.5283\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 4727.9556\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 4814.5684\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 4692.2705\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 4742.9727\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 4860.0400\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 4512.8892\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 4600.5254\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 4795.9297\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 4751.5264\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 4751.3340\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 4624.7178\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 4625.4712\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 4871.0264\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 4580.8574\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 4464.2817\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 4879.7998\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 4474.1660\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 5054.2871\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 4713.6406\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 4563.0527\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 4593.2988\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 4669.7319\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 4976.4487\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 4776.9404\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 4847.9326\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 4829.2959\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 4869.0752\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 4529.6650\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 4506.4775\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 4917.2529\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 4609.9980\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 4696.2539\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 4765.3398\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 4637.9351\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 4673.8867\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 4706.8037\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 4350.0796\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 4653.3604\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 4726.5420\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 4539.3945\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 4507.1021\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 4617.2861\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 4477.9727\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 4774.3818\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 4415.7803\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 4620.1636\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 4363.4268\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 4751.2251\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 4746.6699\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 4871.4126\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 4637.2910\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 4667.3809\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 4507.3042\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 4755.4873\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 4490.8208\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 4802.5811\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 4496.0610\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 4417.4414\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 4614.0791\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 4503.1758\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 4781.8008\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 4475.6357\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 4720.7710\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 4813.9365\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 4802.8203\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 4268.6606\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 4481.5298\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 4742.4854\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 4447.8350\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 4566.1562\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 4448.1577\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 4841.1157\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 4478.7197\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 4810.1826\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 4746.7378\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 4587.6963\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 4639.8027\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 4726.5005\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 4530.7793\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 4465.9712\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 4388.3188\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 4686.4229\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 4654.1807\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 4518.9819\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 4479.9287\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 4586.3955\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 4667.3574\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 4647.6782\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 4464.8706\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 4519.2598\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 4695.6318\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 4509.7256\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 4555.2788\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 4545.6484\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 4626.1353\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 4568.5049\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 4610.7217\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 4533.9795\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 4707.1782\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 4330.1353\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 4575.3398\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 4555.2275\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 4655.7822\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 4741.2197\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 4533.4023\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 4366.1865\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 4545.4360\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 4354.3350\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 4465.4165\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 4666.4727\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 4560.6748\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 4247.8843\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 4517.4414\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 4456.2720\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 4704.9727\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 4519.4863\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 4358.3052\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 4408.9790\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 4634.9106\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 4629.9971\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 4572.1372\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 4538.6001\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 4424.9023\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 4548.5527\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 4361.2441\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 4570.2373\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 4608.2461\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 4515.6377\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 4513.2041\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 4392.9873\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 4419.0664\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 4399.2588\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 4575.4478\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 4600.0229\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 4238.6816\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 4300.8271\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 4611.1904\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 4326.2334\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 4561.3149\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 4332.5791\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 4542.3096\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 4374.8325\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 4443.3730\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 4384.9541\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 4730.4453\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 4304.2983\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 4243.5322\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 4624.4971\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 4506.8804\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 4402.6763\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 4427.5527\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 4708.4004\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 4566.5684\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 4331.2852\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 4348.3369\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 4492.9502\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 4617.3828\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 4603.3184\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 4527.0889\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 4844.4131\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 4657.5947\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 4373.3467\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 4527.8213\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 4303.4927\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 4346.3540\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 4317.2832\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 4571.2236\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 4538.9419\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 4405.2891\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 4279.9072\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 4353.9243\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 4533.0488\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 4639.0898\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 4573.9258\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 4545.8184\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 4346.7480\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 4365.0605\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 4527.5737\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 4396.1709\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 4636.5850\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 4759.6479\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 4530.1548\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 4421.4751\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 4423.2695\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 4566.3242\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 4577.4922\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 4334.3364\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 4270.3535\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 4295.9023\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 4280.8911\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 4229.0527\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 4267.0654\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 4177.5693\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 4549.5176\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 4514.4067\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 4387.8682\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 4253.5713\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 4124.7070\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 4502.4697\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 4442.5547\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 4572.9023\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 4259.9663\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 4391.9287\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 4398.7109\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 4424.2808\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 4425.5596\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 4456.3608\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 4680.4941\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 4309.2129\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 4546.0508\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 4440.1216\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 4247.2393\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 4370.0723\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 4447.1143\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 4296.7920\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 4552.3511\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 4300.7915\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 4531.7598\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 4295.3423\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 4633.3955\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 4362.5317\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 4575.7656\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 4317.7354\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 4424.4595\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 4617.7471\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 4458.3848\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 4329.9800\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 4329.5967\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 4296.2168\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 4179.1245\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 4466.3223\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 4406.3730\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 4366.3311\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 4246.6685\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 4163.1631\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 4352.5791\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 4433.2559\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 4382.2207\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 4180.3350\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 4190.7466\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 4396.6636\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 4364.5615\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 4397.9424\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 4258.3511\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 4088.4524\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 4286.8179\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 4364.6431\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 4298.1597\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 4403.4858\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 4457.5996\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 4327.5479\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 4384.9619\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 4274.3311\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 4253.0713\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 4171.0283\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 4301.8081\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 4463.8242\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 4347.5107\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 4506.8828\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 4548.9019\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 4373.6460\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 4386.8901\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 4268.1504\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 4441.3047\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 4232.3447\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 4342.2778\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 4102.5508\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 4408.2637\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 4391.5361\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 4433.5308\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 4341.8545\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 4163.4102\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 4200.9277\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 4268.5938\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 4362.2617\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 4383.2500\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 4245.4590\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 4119.3008\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 4077.5837\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 4179.3677\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 4473.9775\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 4157.4150\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 4361.1211\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 4153.2285\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 4177.1865\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 4437.8271\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 4284.3506\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 4263.1299\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 4226.5557\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 4221.0767\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 4349.6885\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 4221.4790\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 4194.8564\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 4339.9756\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 4195.7344\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 4211.8721\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 4082.0967\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 4258.8042\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 4148.3462\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 4047.8843\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 4285.7710\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 4322.4116\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 4162.5620\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 4277.1748\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 4191.0303\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 4186.6953\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 4071.2234\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 4225.4941\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 4368.4253\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 4296.8765\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 4228.6621\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 4403.8096\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 4255.1909\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 4268.5308\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 4391.0405\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 4130.2900\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 4156.6055\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 4191.1807\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 4490.6084\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 4246.0264\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 4288.9907\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 4254.0269\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 4267.7368\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 4345.2881\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 4121.0361\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 4246.6133\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 4108.6743\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 4176.2998\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 4130.7881\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 4353.7319\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 4151.5479\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 4333.3574\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 4331.7393\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 4042.0518\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 4197.7100\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 4352.5127\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 4034.1675\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 4205.8286\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 4111.0337\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 4222.4326\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 4184.7095\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 4009.1724\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 4414.8545\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 4300.9092\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 4237.4414\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 4301.7808\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 4425.0156\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 4058.0471\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 4383.3169\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 4221.0928\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 4119.5488\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 4231.5645\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 4223.9717\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 4184.0513\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 4078.7314\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 4317.3711\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 4297.4761\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 4079.1445\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 4182.6519\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 4124.8159\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 4226.7856\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 4176.3379\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 4127.4219\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 4072.7046\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 4294.6982\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 3894.6599\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 4089.9443\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 4102.8135\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 4011.9849\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 4028.1440\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 4286.9888\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 4082.6152\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 4316.6191\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 4146.9580\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 3944.0710\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 4141.7896\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 4184.2207\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 4070.2517\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 4373.2002\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 4312.0869\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 4105.5508\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 4109.9829\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 4291.2617\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 4109.1167\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 4072.8586\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 4123.4971\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 4141.7529\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 4033.0576\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 4042.3149\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 4279.2656\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 4044.9656\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 4161.0298\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 4253.3076\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 4432.7559\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 4069.7341\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 3883.2393\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 3980.9849\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 4124.8730\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 4026.9163\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 4040.1250\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 4196.1582\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 4188.3818\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 4495.3936\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 4288.0059\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 4187.9102\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 4318.4492\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 4194.5000\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 4188.5146\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 4135.3862\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 4148.1865\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 4035.4863\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 4083.9927\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 4144.0078\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 4143.6182\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 4412.2373\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 4080.2402\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 4048.2090\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 3970.7988\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 4148.0127\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 4008.8423\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 4291.1758\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 4122.5830\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 4188.2612\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 4145.7451\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 4082.8813\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 3993.9485\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 4297.3037\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 4112.6504\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 4083.4221\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 4175.7920\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 4251.9609\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 4373.3154\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 4166.0127\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 4082.3677\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 4254.5059\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 4306.7021\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 4071.6797\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 4180.9956\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 3934.1533\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 4093.2480\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 4098.3438\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 4247.4395\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 3931.5596\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 4088.4956\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 4090.6682\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 4209.0835\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 4064.7349\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 3922.8271\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 3861.9326\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 3997.6423\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 3828.8479\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 4117.8486\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 3943.2412\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 4144.8154\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 4117.8892\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 4081.2993\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 4009.6680\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 4137.8076\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 3999.5249\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 4342.5947\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 3948.5750\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 3924.7085\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 3875.9536\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 3942.3774\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 4088.8191\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 3970.9116\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 4019.0835\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 4015.5630\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 3962.3701\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 4158.6699\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 3924.1514\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 3878.4194\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 3830.0200\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 4018.4753\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 4016.5552\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 4016.5210\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 3889.8840\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 4120.2412\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 3973.4380\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 4127.2090\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 4096.2588\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 3940.5508\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 3901.1824\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 4049.7021\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 4093.4292\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 3993.1836\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 3956.3135\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 4001.1116\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 3940.7727\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 3785.4058\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 4106.5112\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 4074.8721\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 4160.6006\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 4049.1709\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 3914.3281\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 4042.5969\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 3927.3306\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 3808.9409\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 4061.9854\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 3812.2881\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 4203.4619\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 3733.7859\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 4239.3892\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 3786.9961\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 4017.7432\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 4087.8887\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 4059.4321\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 4185.3965\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 4061.4106\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 3965.2461\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 3974.2466\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 4005.5571\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 3880.3774\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 4132.7959\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 3725.6326\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 4121.3682\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 4059.8662\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 3937.5464\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 3986.4854\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 4060.5200\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 4025.5908\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 3949.1401\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 3977.0957\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 3922.8110\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 3778.4968\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 4053.4253\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 4127.9600\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 3945.8728\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 4007.0784\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 3934.1353\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 4054.8958\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 3900.6365\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 4026.8313\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 4029.2454\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 3858.3765\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 3976.4839\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 3948.4260\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 3964.2061\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 4105.5879\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 3806.6147\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 3834.5874\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 3866.3801\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 3847.0186\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 3858.3289\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 3910.7876\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 3920.2056\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 3868.2649\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 3885.4929\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 3844.3677\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 4031.4263\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 3792.2051\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 3889.0188\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 3917.5071\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 4356.3169\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 3934.0322\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 3742.8594\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 3892.8369\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 3695.7104\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 3827.9243\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 4132.9399\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 3879.5659\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 3840.3704\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 3773.2773\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 3842.4551\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 3722.7446\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 4049.0234\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 3994.7886\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 3961.0405\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 3910.1775\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 3796.3306\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 4048.3289\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 3821.2480\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 3712.9663\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 3971.1382\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 3960.5754\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 3747.3916\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 3759.7966\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 3968.5322\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 4133.9834\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 4072.0532\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 3958.3748\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 3814.0520\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 3935.6611\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 4240.3271\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 3923.2588\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 3747.1328\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 3636.2771\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 3989.8931\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 3900.1377\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 3790.5903\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 3794.3208\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 4005.6921\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 3764.9729\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 3881.0945\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 3900.1531\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 3836.3513\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 3740.0112\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 4135.3643\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 3945.5330\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 3638.3057\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 3688.4380\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 4069.3267\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 3839.2390\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 3851.1685\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 3729.4226\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 3762.6414\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 3862.8008\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 3750.0742\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 3785.1301\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 3688.2297\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 3757.9185\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 3729.5796\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 3748.6582\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 3825.9612\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 4015.7842\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 3741.6982\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 3776.0986\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 3685.0698\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 3918.5356\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 3860.8484\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 3763.7026\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 3762.4971\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 3813.7507\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 3804.6318\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 3923.0698\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 3769.2192\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 3852.1909\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 3738.5442\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 4099.1421\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 3779.9001\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 4017.5811\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 3780.5996\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 3776.9734\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 3937.9409\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 3840.7944\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 3865.9624\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 3706.7712\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 3784.5876\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 3867.2192\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 3888.6230\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 3840.1584\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 3667.9116\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 3782.9771\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 3878.6096\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 3812.5073\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 3850.8564\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 3951.3765\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 3872.8662\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 3800.6475\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 3850.7354\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 3803.3203\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 3541.9287\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 3939.6431\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 3872.8867\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 3694.9771\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 3671.9502\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 3645.9414\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 3635.8691\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 3967.4666\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 3839.4688\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 3781.4463\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 3883.1130\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 3628.7788\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 3896.3489\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 3717.2917\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 3573.2717\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 3706.0166\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 3608.5076\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 3650.2520\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 3649.4795\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 3829.0283\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 3737.7161\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 3576.2930\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 3600.8975\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 3736.8359\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 3528.5771\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 3687.1379\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 3840.4880\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 3632.7036\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 3882.2598\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 3580.9102\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 3535.7734\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 3653.1509\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 3569.5640\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 3634.4565\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 3870.9155\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 3531.7639\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 3829.3127\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 3707.8633\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 3555.6724\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 3839.0127\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 3612.4268\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 3769.7129\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 3659.4541\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 3514.2095\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 3616.2197\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 3785.6746\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 3798.5203\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 3660.9910\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 3582.0894\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 3686.3413\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 3904.2791\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 3643.2722\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 3631.8926\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 3693.9670\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 3733.6985\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 3677.2827\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 4002.3301\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 3824.7546\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 3866.7656\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 3763.7954\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 3865.2920\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 3846.8896\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 3684.6331\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 3659.6890\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 3798.4331\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 3773.6689\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 3643.8572\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 3972.3577\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 3661.4971\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 3762.3955\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 3767.1785\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 3464.4619\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 3512.3569\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 3748.1216\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 3814.1733\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 3500.2358\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 3674.2515\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 3549.0469\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 3619.3936\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 3778.2515\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 3771.7637\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 3817.2654\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 3567.0439\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 4001.9888\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 3534.9348\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 3712.3306\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 3689.8406\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 3581.7534\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 3626.5447\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 3633.8760\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 3715.2871\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 3591.6797\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 3578.0637\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 3570.3457\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 3680.8652\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 3656.2170\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 3778.8572\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 3844.2798\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 3680.0391\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 3589.7380\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 3739.0066\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 3643.3369\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 3683.2568\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 3647.8328\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 3532.5156\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 3682.9126\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 3713.2488\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 3602.3535\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 3515.2588\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 3782.5117\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 3634.8774\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 3584.9194\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 3679.6970\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 3634.5469\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 3570.0789\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 3486.1841\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 3829.8459\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 3534.6484\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 3594.3418\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 3525.3237\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 3590.4541\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 3609.4116\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 3566.2856\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 3516.9475\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 3419.5649\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 3786.9480\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 3545.3809\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 3638.0994\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 3586.8252\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 3614.4517\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 3574.9229\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 3677.5315\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 3757.5850\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 3632.8892\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 3634.4453\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 3589.4004\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 3616.6802\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 3595.3662\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 3483.7998\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 3576.7446\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 3609.9873\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 3485.7354\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 3781.8369\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 3536.4487\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 3677.1523\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 3514.6831\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 3461.6636\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 3635.2397\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 3415.7427\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 3523.7310\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 3504.3379\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 3417.0334\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 3604.8633\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 3721.7773\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 3600.4775\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 3697.5396\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 3720.5886\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 3604.5332\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 3605.3521\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 3641.5122\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 3539.3579\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 3476.7537\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 3507.3472\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 3486.0552\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 3382.2065\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 3488.1592\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 3526.5071\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 3658.5486\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 3696.8301\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 3410.7478\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 3751.5789\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 3511.4163\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 3576.1433\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 3708.8157\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 3275.2673\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 3641.6924\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 3553.5669\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 3486.4587\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 3716.3298\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 3631.7937\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 3529.3650\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 3449.5229\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 3472.6001\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 3711.7312\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 3676.1914\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 3607.6670\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 3640.0166\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 3450.0586\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 3364.0308\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 3659.5957\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 3456.6047\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 3684.8188\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 3760.4897\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 3572.9023\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 3515.7205\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 3383.7334\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 3390.1592\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 3772.4338\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 3427.9961\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 3619.4644\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 3338.7617\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 3483.0623\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 3422.6245\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 3492.4795\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 3390.1128\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 3355.7349\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 3658.6602\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 3495.5854\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 3495.8318\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 3449.0474\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 3436.5681\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 3441.1816\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 3426.5645\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 3435.0767\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 3515.6963\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 3515.3606\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 3529.1880\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 3493.9075\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 3687.9951\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 3525.2747\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 3371.3936\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 3595.5654\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 3540.7122\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 3454.5918\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 3378.6284\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 3675.7847\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 3513.0632\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 3554.3286\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 3452.1636\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 3447.0229\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 3567.8137\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 3380.5383\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 3539.1470\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 3369.0928\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 3684.1555\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 3378.3911\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 3326.8384\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 3691.4854\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 3362.7642\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 3407.1101\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 3432.2231\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 3515.1169\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 3630.0337\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 3525.3359\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 3456.9075\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 3491.6523\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 3512.9050\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 3509.3691\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 3544.8806\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 3504.3567\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 3469.9751\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 3398.7615\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 3706.0964\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 3498.9307\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 3336.1956\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 3593.0562\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 3440.5911\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 3505.8357\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 3314.0317\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 3406.3970\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 3439.1880\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 3489.7166\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 3533.3669\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 3343.6079\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 3244.2817\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 3489.8579\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 3270.5962\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 3518.1714\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 3513.3921\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 3470.4141\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 3348.4224\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 3385.5220\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 3512.2913\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 3535.9497\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 3544.6599\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 3307.8450\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 3331.4353\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 3430.0752\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 3376.7422\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 3496.1758\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 3415.8696\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 3626.4482\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 3508.7466\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 3497.1062\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 3419.1816\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 3560.2920\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 3577.8828\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 3438.5083\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 3599.6509\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 3314.5122\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 3638.1538\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 3464.5586\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 3290.5110\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 3464.4678\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 3407.0471\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 3604.8398\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 3532.7146\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 3413.3455\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 3453.6289\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 3332.5571\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 3372.2041\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 3466.1541\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 3496.1489\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 3309.9248\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 3517.9316\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 3357.0095\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 3235.8516\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 3491.7898\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 3354.2959\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 3445.0557\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 3922.6389\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 3334.7720\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 3369.4495\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 3334.8389\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 3414.8298\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 3465.6040\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 3492.3235\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 3165.2241\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 3315.3257\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 3420.7886\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 3412.0200\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 3489.2212\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 3346.4128\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 3392.6123\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 3478.4121\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 3484.1807\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 3489.2898\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 3589.8188\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 3390.1519\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 3438.3518\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 3464.9038\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 3375.5510\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 3382.7891\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 3551.4331\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 3462.6050\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 3428.4082\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 3293.8516\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 3360.6821\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 3429.4592\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 3282.2959\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 3356.3389\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 3462.1079\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 3335.7295\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 3523.7920\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 3309.3865\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 3626.7129\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 3480.1931\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 3324.0107\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 3255.5383\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 3403.3794\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 3216.8604\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 3378.6038\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 3500.4324\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 3392.2336\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 3242.4019\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 3298.7939\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 3646.1506\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 3445.0437\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 3257.7124\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 3326.0088\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 3399.0181\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 3359.9497\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 3345.5498\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 3481.6179\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 3575.1670\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 3279.5872\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 3329.3096\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 3211.5684\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 3239.8062\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 3208.5659\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 3311.5151\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 3261.0237\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 3252.0781\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 3235.6587\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 3274.4727\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 3458.9385\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 3466.7683\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 3421.1802\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 3406.7671\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 3466.2988\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 3308.8772\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 3340.0469\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 3288.7817\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 3152.0923\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 3343.3909\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 3235.1135\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 3216.4688\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 3355.0278\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 3253.0925\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 3340.9136\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 3386.6616\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 3342.8467\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 3275.3557\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 3306.9502\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 3211.5454\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 3258.3298\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 3362.9890\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 3417.9277\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 3236.1089\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 3377.8521\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 3154.9976\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 3310.3247\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 3205.7205\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 3323.2666\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 3343.1018\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 3153.2339\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 3186.0081\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 3247.3333\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 3390.5146\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 3251.1265\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 3312.4192\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 3262.7688\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 3367.5186\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 3398.6816\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 3333.8311\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 3293.9553\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 3365.2385\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 3359.3857\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 3270.5916\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 3252.8276\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 3213.4067\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 3294.0581\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 3165.1770\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 3321.2607\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 3256.0249\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 3438.9487\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 3215.1699\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 3075.2839\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 3156.2974\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 3299.8608\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 3163.1055\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 3342.6270\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 3172.5884\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 3312.7292\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 3246.0410\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 3224.6650\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 3107.5747\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 3511.6553\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 3252.6387\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 3124.4131\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 3399.3628\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 3145.8220\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 3309.4202\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 3495.5474\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 3232.2583\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 3228.4661\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 3398.2917\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 3340.9077\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 3259.4775\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 3137.6147\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 3173.0747\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 3278.1265\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 3291.6519\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 3174.6548\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 3242.3367\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 3146.8003\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 3247.6260\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 3215.3560\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 3498.7026\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 3134.6558\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 3166.9658\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 3346.3818\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 3286.7991\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 3183.9219\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 3206.5171\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 3216.8794\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 3398.0742\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 3232.9600\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 3237.3213\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 3181.1577\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 3298.6919\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 3434.0649\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 3188.3379\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 3134.4333\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 3140.2444\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 3370.2898\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 3307.0156\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 3188.0439\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 3133.7695\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 3030.8381\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 3126.2964\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 3268.0454\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 3119.1108\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 3214.0073\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 3298.4951\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 3082.0762\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 3092.3816\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 3115.4858\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 3157.2288\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 3258.3335\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 3190.3484\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 3129.6250\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 3371.8452\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 3241.6357\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 3235.9023\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 3274.0256\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 3222.0156\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 3308.2778\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 3326.9526\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 3266.5566\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 3113.9275\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 3109.0171\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 3277.3115\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 3141.5369\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 3285.8784\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 3312.6768\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 3246.2905\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 3226.5439\n",
      "âœ… Epoch 9 Avg Loss: 3982.5554\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[529 591]\n",
      " [588 525]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4647\n",
      "\n",
      "ğŸ” Epoch 10/10\n",
      "  ğŸ“¦ Batch 1/1272 | Loss: 3334.1655\n",
      "  ğŸ“¦ Batch 2/1272 | Loss: 3219.1299\n",
      "  ğŸ“¦ Batch 3/1272 | Loss: 3209.9272\n",
      "  ğŸ“¦ Batch 4/1272 | Loss: 3297.1489\n",
      "  ğŸ“¦ Batch 5/1272 | Loss: 3250.5364\n",
      "  ğŸ“¦ Batch 6/1272 | Loss: 3041.3091\n",
      "  ğŸ“¦ Batch 7/1272 | Loss: 3189.8516\n",
      "  ğŸ“¦ Batch 8/1272 | Loss: 3108.2666\n",
      "  ğŸ“¦ Batch 9/1272 | Loss: 3307.8745\n",
      "  ğŸ“¦ Batch 10/1272 | Loss: 3182.4719\n",
      "  ğŸ“¦ Batch 11/1272 | Loss: 3215.0581\n",
      "  ğŸ“¦ Batch 12/1272 | Loss: 3000.5261\n",
      "  ğŸ“¦ Batch 13/1272 | Loss: 3315.2039\n",
      "  ğŸ“¦ Batch 14/1272 | Loss: 3174.9731\n",
      "  ğŸ“¦ Batch 15/1272 | Loss: 3093.2271\n",
      "  ğŸ“¦ Batch 16/1272 | Loss: 3130.2847\n",
      "  ğŸ“¦ Batch 17/1272 | Loss: 3276.3105\n",
      "  ğŸ“¦ Batch 18/1272 | Loss: 3128.0566\n",
      "  ğŸ“¦ Batch 19/1272 | Loss: 3115.8254\n",
      "  ğŸ“¦ Batch 20/1272 | Loss: 3096.3325\n",
      "  ğŸ“¦ Batch 21/1272 | Loss: 3193.3086\n",
      "  ğŸ“¦ Batch 22/1272 | Loss: 3094.3267\n",
      "  ğŸ“¦ Batch 23/1272 | Loss: 3068.4517\n",
      "  ğŸ“¦ Batch 24/1272 | Loss: 3056.2488\n",
      "  ğŸ“¦ Batch 25/1272 | Loss: 3284.6936\n",
      "  ğŸ“¦ Batch 26/1272 | Loss: 3091.4380\n",
      "  ğŸ“¦ Batch 27/1272 | Loss: 3209.0059\n",
      "  ğŸ“¦ Batch 28/1272 | Loss: 3071.8071\n",
      "  ğŸ“¦ Batch 29/1272 | Loss: 3182.0996\n",
      "  ğŸ“¦ Batch 30/1272 | Loss: 3231.2388\n",
      "  ğŸ“¦ Batch 31/1272 | Loss: 3228.7295\n",
      "  ğŸ“¦ Batch 32/1272 | Loss: 3023.7251\n",
      "  ğŸ“¦ Batch 33/1272 | Loss: 3187.1270\n",
      "  ğŸ“¦ Batch 34/1272 | Loss: 3006.4312\n",
      "  ğŸ“¦ Batch 35/1272 | Loss: 3231.9792\n",
      "  ğŸ“¦ Batch 36/1272 | Loss: 3022.3748\n",
      "  ğŸ“¦ Batch 37/1272 | Loss: 3349.6836\n",
      "  ğŸ“¦ Batch 38/1272 | Loss: 3208.5303\n",
      "  ğŸ“¦ Batch 39/1272 | Loss: 3371.7520\n",
      "  ğŸ“¦ Batch 40/1272 | Loss: 3174.5762\n",
      "  ğŸ“¦ Batch 41/1272 | Loss: 3144.6843\n",
      "  ğŸ“¦ Batch 42/1272 | Loss: 3178.9014\n",
      "  ğŸ“¦ Batch 43/1272 | Loss: 3155.7976\n",
      "  ğŸ“¦ Batch 44/1272 | Loss: 3089.2588\n",
      "  ğŸ“¦ Batch 45/1272 | Loss: 3250.1011\n",
      "  ğŸ“¦ Batch 46/1272 | Loss: 3045.8552\n",
      "  ğŸ“¦ Batch 47/1272 | Loss: 3122.6067\n",
      "  ğŸ“¦ Batch 48/1272 | Loss: 3199.3535\n",
      "  ğŸ“¦ Batch 49/1272 | Loss: 3228.1606\n",
      "  ğŸ“¦ Batch 50/1272 | Loss: 3185.6479\n",
      "  ğŸ“¦ Batch 51/1272 | Loss: 3049.7930\n",
      "  ğŸ“¦ Batch 52/1272 | Loss: 3182.7095\n",
      "  ğŸ“¦ Batch 53/1272 | Loss: 3240.2661\n",
      "  ğŸ“¦ Batch 54/1272 | Loss: 3123.3320\n",
      "  ğŸ“¦ Batch 55/1272 | Loss: 3211.8572\n",
      "  ğŸ“¦ Batch 56/1272 | Loss: 3174.7810\n",
      "  ğŸ“¦ Batch 57/1272 | Loss: 2953.8186\n",
      "  ğŸ“¦ Batch 58/1272 | Loss: 3208.4927\n",
      "  ğŸ“¦ Batch 59/1272 | Loss: 3011.0317\n",
      "  ğŸ“¦ Batch 60/1272 | Loss: 3234.7178\n",
      "  ğŸ“¦ Batch 61/1272 | Loss: 2988.3572\n",
      "  ğŸ“¦ Batch 62/1272 | Loss: 3027.0376\n",
      "  ğŸ“¦ Batch 63/1272 | Loss: 2991.1675\n",
      "  ğŸ“¦ Batch 64/1272 | Loss: 3090.5959\n",
      "  ğŸ“¦ Batch 65/1272 | Loss: 3190.7219\n",
      "  ğŸ“¦ Batch 66/1272 | Loss: 3193.6284\n",
      "  ğŸ“¦ Batch 67/1272 | Loss: 3072.3013\n",
      "  ğŸ“¦ Batch 68/1272 | Loss: 3021.7476\n",
      "  ğŸ“¦ Batch 69/1272 | Loss: 3036.3760\n",
      "  ğŸ“¦ Batch 70/1272 | Loss: 3061.4441\n",
      "  ğŸ“¦ Batch 71/1272 | Loss: 3214.1880\n",
      "  ğŸ“¦ Batch 72/1272 | Loss: 3037.5923\n",
      "  ğŸ“¦ Batch 73/1272 | Loss: 3225.3325\n",
      "  ğŸ“¦ Batch 74/1272 | Loss: 3054.6484\n",
      "  ğŸ“¦ Batch 75/1272 | Loss: 3091.4692\n",
      "  ğŸ“¦ Batch 76/1272 | Loss: 3377.8657\n",
      "  ğŸ“¦ Batch 77/1272 | Loss: 3184.2095\n",
      "  ğŸ“¦ Batch 78/1272 | Loss: 2975.2856\n",
      "  ğŸ“¦ Batch 79/1272 | Loss: 3213.1387\n",
      "  ğŸ“¦ Batch 80/1272 | Loss: 3119.4844\n",
      "  ğŸ“¦ Batch 81/1272 | Loss: 3179.0256\n",
      "  ğŸ“¦ Batch 82/1272 | Loss: 3100.4893\n",
      "  ğŸ“¦ Batch 83/1272 | Loss: 2934.0647\n",
      "  ğŸ“¦ Batch 84/1272 | Loss: 3167.5991\n",
      "  ğŸ“¦ Batch 85/1272 | Loss: 3126.5913\n",
      "  ğŸ“¦ Batch 86/1272 | Loss: 3019.9565\n",
      "  ğŸ“¦ Batch 87/1272 | Loss: 3111.2754\n",
      "  ğŸ“¦ Batch 88/1272 | Loss: 3122.2261\n",
      "  ğŸ“¦ Batch 89/1272 | Loss: 3022.7056\n",
      "  ğŸ“¦ Batch 90/1272 | Loss: 3070.5376\n",
      "  ğŸ“¦ Batch 91/1272 | Loss: 3077.5664\n",
      "  ğŸ“¦ Batch 92/1272 | Loss: 3168.3550\n",
      "  ğŸ“¦ Batch 93/1272 | Loss: 3095.2168\n",
      "  ğŸ“¦ Batch 94/1272 | Loss: 2988.8979\n",
      "  ğŸ“¦ Batch 95/1272 | Loss: 3080.6238\n",
      "  ğŸ“¦ Batch 96/1272 | Loss: 2951.6838\n",
      "  ğŸ“¦ Batch 97/1272 | Loss: 3136.5793\n",
      "  ğŸ“¦ Batch 98/1272 | Loss: 3208.8667\n",
      "  ğŸ“¦ Batch 99/1272 | Loss: 3042.3772\n",
      "  ğŸ“¦ Batch 100/1272 | Loss: 3118.1260\n",
      "  ğŸ“¦ Batch 101/1272 | Loss: 2985.0396\n",
      "  ğŸ“¦ Batch 102/1272 | Loss: 3178.7178\n",
      "  ğŸ“¦ Batch 103/1272 | Loss: 3037.0994\n",
      "  ğŸ“¦ Batch 104/1272 | Loss: 2959.1650\n",
      "  ğŸ“¦ Batch 105/1272 | Loss: 3016.9380\n",
      "  ğŸ“¦ Batch 106/1272 | Loss: 3028.4355\n",
      "  ğŸ“¦ Batch 107/1272 | Loss: 3177.7612\n",
      "  ğŸ“¦ Batch 108/1272 | Loss: 3072.8528\n",
      "  ğŸ“¦ Batch 109/1272 | Loss: 3206.0352\n",
      "  ğŸ“¦ Batch 110/1272 | Loss: 3062.9043\n",
      "  ğŸ“¦ Batch 111/1272 | Loss: 3049.9683\n",
      "  ğŸ“¦ Batch 112/1272 | Loss: 2934.3892\n",
      "  ğŸ“¦ Batch 113/1272 | Loss: 3153.8503\n",
      "  ğŸ“¦ Batch 114/1272 | Loss: 3196.5913\n",
      "  ğŸ“¦ Batch 115/1272 | Loss: 2949.5239\n",
      "  ğŸ“¦ Batch 116/1272 | Loss: 2990.0667\n",
      "  ğŸ“¦ Batch 117/1272 | Loss: 3075.0776\n",
      "  ğŸ“¦ Batch 118/1272 | Loss: 3084.9463\n",
      "  ğŸ“¦ Batch 119/1272 | Loss: 3047.1099\n",
      "  ğŸ“¦ Batch 120/1272 | Loss: 3198.3662\n",
      "  ğŸ“¦ Batch 121/1272 | Loss: 2950.0564\n",
      "  ğŸ“¦ Batch 122/1272 | Loss: 3166.2266\n",
      "  ğŸ“¦ Batch 123/1272 | Loss: 3088.2300\n",
      "  ğŸ“¦ Batch 124/1272 | Loss: 3110.0317\n",
      "  ğŸ“¦ Batch 125/1272 | Loss: 3091.0183\n",
      "  ğŸ“¦ Batch 126/1272 | Loss: 3070.0029\n",
      "  ğŸ“¦ Batch 127/1272 | Loss: 3121.0210\n",
      "  ğŸ“¦ Batch 128/1272 | Loss: 2998.6863\n",
      "  ğŸ“¦ Batch 129/1272 | Loss: 3126.7949\n",
      "  ğŸ“¦ Batch 130/1272 | Loss: 3028.8403\n",
      "  ğŸ“¦ Batch 131/1272 | Loss: 3132.3931\n",
      "  ğŸ“¦ Batch 132/1272 | Loss: 3167.4888\n",
      "  ğŸ“¦ Batch 133/1272 | Loss: 3039.3057\n",
      "  ğŸ“¦ Batch 134/1272 | Loss: 3063.2620\n",
      "  ğŸ“¦ Batch 135/1272 | Loss: 3157.2703\n",
      "  ğŸ“¦ Batch 136/1272 | Loss: 3111.8687\n",
      "  ğŸ“¦ Batch 137/1272 | Loss: 3010.1494\n",
      "  ğŸ“¦ Batch 138/1272 | Loss: 2903.8232\n",
      "  ğŸ“¦ Batch 139/1272 | Loss: 3050.4458\n",
      "  ğŸ“¦ Batch 140/1272 | Loss: 3056.1633\n",
      "  ğŸ“¦ Batch 141/1272 | Loss: 3023.6157\n",
      "  ğŸ“¦ Batch 142/1272 | Loss: 3061.8809\n",
      "  ğŸ“¦ Batch 143/1272 | Loss: 2998.2815\n",
      "  ğŸ“¦ Batch 144/1272 | Loss: 3160.7739\n",
      "  ğŸ“¦ Batch 145/1272 | Loss: 3101.2104\n",
      "  ğŸ“¦ Batch 146/1272 | Loss: 3156.0300\n",
      "  ğŸ“¦ Batch 147/1272 | Loss: 3054.3140\n",
      "  ğŸ“¦ Batch 148/1272 | Loss: 3128.3630\n",
      "  ğŸ“¦ Batch 149/1272 | Loss: 2976.2622\n",
      "  ğŸ“¦ Batch 150/1272 | Loss: 3099.0789\n",
      "  ğŸ“¦ Batch 151/1272 | Loss: 3162.8931\n",
      "  ğŸ“¦ Batch 152/1272 | Loss: 3000.9285\n",
      "  ğŸ“¦ Batch 153/1272 | Loss: 3136.6523\n",
      "  ğŸ“¦ Batch 154/1272 | Loss: 3006.3076\n",
      "  ğŸ“¦ Batch 155/1272 | Loss: 3070.3252\n",
      "  ğŸ“¦ Batch 156/1272 | Loss: 3030.9231\n",
      "  ğŸ“¦ Batch 157/1272 | Loss: 3109.3911\n",
      "  ğŸ“¦ Batch 158/1272 | Loss: 3165.7822\n",
      "  ğŸ“¦ Batch 159/1272 | Loss: 3026.4321\n",
      "  ğŸ“¦ Batch 160/1272 | Loss: 2974.5098\n",
      "  ğŸ“¦ Batch 161/1272 | Loss: 2903.7947\n",
      "  ğŸ“¦ Batch 162/1272 | Loss: 2992.0464\n",
      "  ğŸ“¦ Batch 163/1272 | Loss: 2923.9978\n",
      "  ğŸ“¦ Batch 164/1272 | Loss: 2887.8350\n",
      "  ğŸ“¦ Batch 165/1272 | Loss: 2890.4255\n",
      "  ğŸ“¦ Batch 166/1272 | Loss: 3074.4775\n",
      "  ğŸ“¦ Batch 167/1272 | Loss: 3182.6846\n",
      "  ğŸ“¦ Batch 168/1272 | Loss: 3034.3569\n",
      "  ğŸ“¦ Batch 169/1272 | Loss: 3090.1646\n",
      "  ğŸ“¦ Batch 170/1272 | Loss: 2917.1377\n",
      "  ğŸ“¦ Batch 171/1272 | Loss: 3009.3672\n",
      "  ğŸ“¦ Batch 172/1272 | Loss: 3091.2656\n",
      "  ğŸ“¦ Batch 173/1272 | Loss: 2982.0391\n",
      "  ğŸ“¦ Batch 174/1272 | Loss: 3031.2788\n",
      "  ğŸ“¦ Batch 175/1272 | Loss: 3127.5671\n",
      "  ğŸ“¦ Batch 176/1272 | Loss: 3038.6130\n",
      "  ğŸ“¦ Batch 177/1272 | Loss: 3169.9983\n",
      "  ğŸ“¦ Batch 178/1272 | Loss: 3051.1445\n",
      "  ğŸ“¦ Batch 179/1272 | Loss: 3033.1152\n",
      "  ğŸ“¦ Batch 180/1272 | Loss: 2993.2280\n",
      "  ğŸ“¦ Batch 181/1272 | Loss: 3203.5811\n",
      "  ğŸ“¦ Batch 182/1272 | Loss: 2900.3494\n",
      "  ğŸ“¦ Batch 183/1272 | Loss: 2996.3047\n",
      "  ğŸ“¦ Batch 184/1272 | Loss: 2939.2363\n",
      "  ğŸ“¦ Batch 185/1272 | Loss: 3170.7690\n",
      "  ğŸ“¦ Batch 186/1272 | Loss: 3048.1680\n",
      "  ğŸ“¦ Batch 187/1272 | Loss: 2981.9277\n",
      "  ğŸ“¦ Batch 188/1272 | Loss: 2923.2734\n",
      "  ğŸ“¦ Batch 189/1272 | Loss: 2965.4238\n",
      "  ğŸ“¦ Batch 190/1272 | Loss: 3003.5845\n",
      "  ğŸ“¦ Batch 191/1272 | Loss: 3115.8079\n",
      "  ğŸ“¦ Batch 192/1272 | Loss: 3009.5820\n",
      "  ğŸ“¦ Batch 193/1272 | Loss: 2853.8989\n",
      "  ğŸ“¦ Batch 194/1272 | Loss: 3024.7329\n",
      "  ğŸ“¦ Batch 195/1272 | Loss: 2954.1641\n",
      "  ğŸ“¦ Batch 196/1272 | Loss: 2887.0513\n",
      "  ğŸ“¦ Batch 197/1272 | Loss: 3025.3975\n",
      "  ğŸ“¦ Batch 198/1272 | Loss: 2977.7888\n",
      "  ğŸ“¦ Batch 199/1272 | Loss: 3032.5427\n",
      "  ğŸ“¦ Batch 200/1272 | Loss: 3062.6138\n",
      "  ğŸ“¦ Batch 201/1272 | Loss: 3034.1899\n",
      "  ğŸ“¦ Batch 202/1272 | Loss: 3054.3745\n",
      "  ğŸ“¦ Batch 203/1272 | Loss: 2903.3289\n",
      "  ğŸ“¦ Batch 204/1272 | Loss: 3023.3425\n",
      "  ğŸ“¦ Batch 205/1272 | Loss: 3071.8145\n",
      "  ğŸ“¦ Batch 206/1272 | Loss: 3043.3242\n",
      "  ğŸ“¦ Batch 207/1272 | Loss: 2955.5850\n",
      "  ğŸ“¦ Batch 208/1272 | Loss: 3218.8652\n",
      "  ğŸ“¦ Batch 209/1272 | Loss: 3026.8079\n",
      "  ğŸ“¦ Batch 210/1272 | Loss: 3111.7070\n",
      "  ğŸ“¦ Batch 211/1272 | Loss: 2928.8135\n",
      "  ğŸ“¦ Batch 212/1272 | Loss: 2964.8145\n",
      "  ğŸ“¦ Batch 213/1272 | Loss: 2782.6021\n",
      "  ğŸ“¦ Batch 214/1272 | Loss: 2995.5034\n",
      "  ğŸ“¦ Batch 215/1272 | Loss: 2976.3118\n",
      "  ğŸ“¦ Batch 216/1272 | Loss: 2902.5020\n",
      "  ğŸ“¦ Batch 217/1272 | Loss: 2970.2700\n",
      "  ğŸ“¦ Batch 218/1272 | Loss: 3122.5547\n",
      "  ğŸ“¦ Batch 219/1272 | Loss: 3057.6641\n",
      "  ğŸ“¦ Batch 220/1272 | Loss: 2896.3613\n",
      "  ğŸ“¦ Batch 221/1272 | Loss: 2965.5391\n",
      "  ğŸ“¦ Batch 222/1272 | Loss: 2905.9507\n",
      "  ğŸ“¦ Batch 223/1272 | Loss: 2956.5366\n",
      "  ğŸ“¦ Batch 224/1272 | Loss: 2951.5212\n",
      "  ğŸ“¦ Batch 225/1272 | Loss: 2802.2515\n",
      "  ğŸ“¦ Batch 226/1272 | Loss: 3029.4929\n",
      "  ğŸ“¦ Batch 227/1272 | Loss: 2939.5437\n",
      "  ğŸ“¦ Batch 228/1272 | Loss: 2776.0210\n",
      "  ğŸ“¦ Batch 229/1272 | Loss: 2914.0029\n",
      "  ğŸ“¦ Batch 230/1272 | Loss: 3035.8110\n",
      "  ğŸ“¦ Batch 231/1272 | Loss: 2898.6777\n",
      "  ğŸ“¦ Batch 232/1272 | Loss: 2906.0188\n",
      "  ğŸ“¦ Batch 233/1272 | Loss: 2954.5439\n",
      "  ğŸ“¦ Batch 234/1272 | Loss: 2961.4595\n",
      "  ğŸ“¦ Batch 235/1272 | Loss: 3017.0486\n",
      "  ğŸ“¦ Batch 236/1272 | Loss: 2899.1636\n",
      "  ğŸ“¦ Batch 237/1272 | Loss: 3061.5005\n",
      "  ğŸ“¦ Batch 238/1272 | Loss: 2870.0520\n",
      "  ğŸ“¦ Batch 239/1272 | Loss: 3012.7129\n",
      "  ğŸ“¦ Batch 240/1272 | Loss: 3077.9788\n",
      "  ğŸ“¦ Batch 241/1272 | Loss: 3039.9373\n",
      "  ğŸ“¦ Batch 242/1272 | Loss: 3116.4707\n",
      "  ğŸ“¦ Batch 243/1272 | Loss: 2998.1140\n",
      "  ğŸ“¦ Batch 244/1272 | Loss: 2792.9622\n",
      "  ğŸ“¦ Batch 245/1272 | Loss: 3005.6428\n",
      "  ğŸ“¦ Batch 246/1272 | Loss: 2793.8147\n",
      "  ğŸ“¦ Batch 247/1272 | Loss: 2895.8289\n",
      "  ğŸ“¦ Batch 248/1272 | Loss: 2923.3589\n",
      "  ğŸ“¦ Batch 249/1272 | Loss: 2962.4351\n",
      "  ğŸ“¦ Batch 250/1272 | Loss: 2829.5505\n",
      "  ğŸ“¦ Batch 251/1272 | Loss: 2910.6265\n",
      "  ğŸ“¦ Batch 252/1272 | Loss: 3013.1030\n",
      "  ğŸ“¦ Batch 253/1272 | Loss: 2882.0295\n",
      "  ğŸ“¦ Batch 254/1272 | Loss: 2816.3552\n",
      "  ğŸ“¦ Batch 255/1272 | Loss: 2902.5378\n",
      "  ğŸ“¦ Batch 256/1272 | Loss: 2827.5964\n",
      "  ğŸ“¦ Batch 257/1272 | Loss: 2786.8596\n",
      "  ğŸ“¦ Batch 258/1272 | Loss: 2947.9478\n",
      "  ğŸ“¦ Batch 259/1272 | Loss: 2914.2124\n",
      "  ğŸ“¦ Batch 260/1272 | Loss: 2957.6353\n",
      "  ğŸ“¦ Batch 261/1272 | Loss: 2893.3198\n",
      "  ğŸ“¦ Batch 262/1272 | Loss: 2890.5669\n",
      "  ğŸ“¦ Batch 263/1272 | Loss: 2957.2964\n",
      "  ğŸ“¦ Batch 264/1272 | Loss: 2971.5664\n",
      "  ğŸ“¦ Batch 265/1272 | Loss: 2855.3638\n",
      "  ğŸ“¦ Batch 266/1272 | Loss: 2861.2083\n",
      "  ğŸ“¦ Batch 267/1272 | Loss: 2892.8140\n",
      "  ğŸ“¦ Batch 268/1272 | Loss: 2934.2876\n",
      "  ğŸ“¦ Batch 269/1272 | Loss: 2776.5171\n",
      "  ğŸ“¦ Batch 270/1272 | Loss: 2896.1592\n",
      "  ğŸ“¦ Batch 271/1272 | Loss: 2967.3479\n",
      "  ğŸ“¦ Batch 272/1272 | Loss: 2976.7974\n",
      "  ğŸ“¦ Batch 273/1272 | Loss: 2947.1421\n",
      "  ğŸ“¦ Batch 274/1272 | Loss: 2900.9282\n",
      "  ğŸ“¦ Batch 275/1272 | Loss: 2751.0186\n",
      "  ğŸ“¦ Batch 276/1272 | Loss: 2784.2500\n",
      "  ğŸ“¦ Batch 277/1272 | Loss: 2883.9607\n",
      "  ğŸ“¦ Batch 278/1272 | Loss: 2826.9258\n",
      "  ğŸ“¦ Batch 279/1272 | Loss: 2828.9160\n",
      "  ğŸ“¦ Batch 280/1272 | Loss: 2892.3445\n",
      "  ğŸ“¦ Batch 281/1272 | Loss: 2878.1499\n",
      "  ğŸ“¦ Batch 282/1272 | Loss: 2934.7124\n",
      "  ğŸ“¦ Batch 283/1272 | Loss: 2916.4375\n",
      "  ğŸ“¦ Batch 284/1272 | Loss: 2927.7175\n",
      "  ğŸ“¦ Batch 285/1272 | Loss: 2963.5073\n",
      "  ğŸ“¦ Batch 286/1272 | Loss: 2889.5640\n",
      "  ğŸ“¦ Batch 287/1272 | Loss: 2817.7905\n",
      "  ğŸ“¦ Batch 288/1272 | Loss: 2878.3906\n",
      "  ğŸ“¦ Batch 289/1272 | Loss: 2870.9771\n",
      "  ğŸ“¦ Batch 290/1272 | Loss: 2946.9082\n",
      "  ğŸ“¦ Batch 291/1272 | Loss: 2785.3916\n",
      "  ğŸ“¦ Batch 292/1272 | Loss: 2723.1807\n",
      "  ğŸ“¦ Batch 293/1272 | Loss: 2911.5566\n",
      "  ğŸ“¦ Batch 294/1272 | Loss: 2878.6553\n",
      "  ğŸ“¦ Batch 295/1272 | Loss: 2957.3389\n",
      "  ğŸ“¦ Batch 296/1272 | Loss: 2915.8464\n",
      "  ğŸ“¦ Batch 297/1272 | Loss: 2789.8164\n",
      "  ğŸ“¦ Batch 298/1272 | Loss: 2906.7400\n",
      "  ğŸ“¦ Batch 299/1272 | Loss: 2829.6628\n",
      "  ğŸ“¦ Batch 300/1272 | Loss: 3083.3584\n",
      "  ğŸ“¦ Batch 301/1272 | Loss: 2878.5859\n",
      "  ğŸ“¦ Batch 302/1272 | Loss: 2877.9702\n",
      "  ğŸ“¦ Batch 303/1272 | Loss: 3029.7812\n",
      "  ğŸ“¦ Batch 304/1272 | Loss: 3190.9873\n",
      "  ğŸ“¦ Batch 305/1272 | Loss: 2886.2358\n",
      "  ğŸ“¦ Batch 306/1272 | Loss: 2940.8750\n",
      "  ğŸ“¦ Batch 307/1272 | Loss: 2785.3999\n",
      "  ğŸ“¦ Batch 308/1272 | Loss: 2839.6006\n",
      "  ğŸ“¦ Batch 309/1272 | Loss: 3003.0713\n",
      "  ğŸ“¦ Batch 310/1272 | Loss: 2967.3765\n",
      "  ğŸ“¦ Batch 311/1272 | Loss: 2820.3813\n",
      "  ğŸ“¦ Batch 312/1272 | Loss: 2893.0754\n",
      "  ğŸ“¦ Batch 313/1272 | Loss: 2933.3389\n",
      "  ğŸ“¦ Batch 314/1272 | Loss: 2875.8362\n",
      "  ğŸ“¦ Batch 315/1272 | Loss: 2773.3799\n",
      "  ğŸ“¦ Batch 316/1272 | Loss: 2944.6443\n",
      "  ğŸ“¦ Batch 317/1272 | Loss: 2997.5039\n",
      "  ğŸ“¦ Batch 318/1272 | Loss: 2888.8628\n",
      "  ğŸ“¦ Batch 319/1272 | Loss: 3040.6646\n",
      "  ğŸ“¦ Batch 320/1272 | Loss: 2827.2046\n",
      "  ğŸ“¦ Batch 321/1272 | Loss: 2947.5420\n",
      "  ğŸ“¦ Batch 322/1272 | Loss: 2898.5220\n",
      "  ğŸ“¦ Batch 323/1272 | Loss: 2843.1426\n",
      "  ğŸ“¦ Batch 324/1272 | Loss: 2863.3423\n",
      "  ğŸ“¦ Batch 325/1272 | Loss: 2854.6709\n",
      "  ğŸ“¦ Batch 326/1272 | Loss: 2877.3384\n",
      "  ğŸ“¦ Batch 327/1272 | Loss: 2887.1133\n",
      "  ğŸ“¦ Batch 328/1272 | Loss: 2839.3618\n",
      "  ğŸ“¦ Batch 329/1272 | Loss: 3002.6353\n",
      "  ğŸ“¦ Batch 330/1272 | Loss: 2976.4863\n",
      "  ğŸ“¦ Batch 331/1272 | Loss: 2872.9978\n",
      "  ğŸ“¦ Batch 332/1272 | Loss: 2734.6887\n",
      "  ğŸ“¦ Batch 333/1272 | Loss: 2775.5459\n",
      "  ğŸ“¦ Batch 334/1272 | Loss: 2921.9492\n",
      "  ğŸ“¦ Batch 335/1272 | Loss: 2689.2246\n",
      "  ğŸ“¦ Batch 336/1272 | Loss: 2889.0691\n",
      "  ğŸ“¦ Batch 337/1272 | Loss: 2834.4805\n",
      "  ğŸ“¦ Batch 338/1272 | Loss: 2976.6917\n",
      "  ğŸ“¦ Batch 339/1272 | Loss: 2846.8005\n",
      "  ğŸ“¦ Batch 340/1272 | Loss: 2844.4932\n",
      "  ğŸ“¦ Batch 341/1272 | Loss: 2792.1753\n",
      "  ğŸ“¦ Batch 342/1272 | Loss: 2894.8345\n",
      "  ğŸ“¦ Batch 343/1272 | Loss: 2989.2771\n",
      "  ğŸ“¦ Batch 344/1272 | Loss: 2806.6057\n",
      "  ğŸ“¦ Batch 345/1272 | Loss: 2840.9919\n",
      "  ğŸ“¦ Batch 346/1272 | Loss: 2800.4556\n",
      "  ğŸ“¦ Batch 347/1272 | Loss: 2722.6069\n",
      "  ğŸ“¦ Batch 348/1272 | Loss: 2785.3433\n",
      "  ğŸ“¦ Batch 349/1272 | Loss: 2921.8066\n",
      "  ğŸ“¦ Batch 350/1272 | Loss: 2872.2964\n",
      "  ğŸ“¦ Batch 351/1272 | Loss: 2834.5823\n",
      "  ğŸ“¦ Batch 352/1272 | Loss: 2942.8577\n",
      "  ğŸ“¦ Batch 353/1272 | Loss: 2845.5269\n",
      "  ğŸ“¦ Batch 354/1272 | Loss: 2799.3418\n",
      "  ğŸ“¦ Batch 355/1272 | Loss: 2925.8713\n",
      "  ğŸ“¦ Batch 356/1272 | Loss: 2862.2988\n",
      "  ğŸ“¦ Batch 357/1272 | Loss: 2783.4551\n",
      "  ğŸ“¦ Batch 358/1272 | Loss: 2752.3215\n",
      "  ğŸ“¦ Batch 359/1272 | Loss: 3000.4363\n",
      "  ğŸ“¦ Batch 360/1272 | Loss: 2925.7222\n",
      "  ğŸ“¦ Batch 361/1272 | Loss: 2954.8406\n",
      "  ğŸ“¦ Batch 362/1272 | Loss: 2728.5112\n",
      "  ğŸ“¦ Batch 363/1272 | Loss: 2705.8108\n",
      "  ğŸ“¦ Batch 364/1272 | Loss: 3016.5137\n",
      "  ğŸ“¦ Batch 365/1272 | Loss: 2670.4092\n",
      "  ğŸ“¦ Batch 366/1272 | Loss: 2902.2505\n",
      "  ğŸ“¦ Batch 367/1272 | Loss: 2708.5107\n",
      "  ğŸ“¦ Batch 368/1272 | Loss: 2825.1694\n",
      "  ğŸ“¦ Batch 369/1272 | Loss: 2818.0527\n",
      "  ğŸ“¦ Batch 370/1272 | Loss: 2932.7710\n",
      "  ğŸ“¦ Batch 371/1272 | Loss: 2869.6233\n",
      "  ğŸ“¦ Batch 372/1272 | Loss: 2865.1052\n",
      "  ğŸ“¦ Batch 373/1272 | Loss: 2806.5464\n",
      "  ğŸ“¦ Batch 374/1272 | Loss: 2901.3623\n",
      "  ğŸ“¦ Batch 375/1272 | Loss: 2891.3320\n",
      "  ğŸ“¦ Batch 376/1272 | Loss: 2755.0806\n",
      "  ğŸ“¦ Batch 377/1272 | Loss: 2836.2500\n",
      "  ğŸ“¦ Batch 378/1272 | Loss: 2850.2222\n",
      "  ğŸ“¦ Batch 379/1272 | Loss: 2812.2441\n",
      "  ğŸ“¦ Batch 380/1272 | Loss: 2656.3276\n",
      "  ğŸ“¦ Batch 381/1272 | Loss: 2728.9373\n",
      "  ğŸ“¦ Batch 382/1272 | Loss: 2774.0654\n",
      "  ğŸ“¦ Batch 383/1272 | Loss: 2776.6421\n",
      "  ğŸ“¦ Batch 384/1272 | Loss: 2904.2239\n",
      "  ğŸ“¦ Batch 385/1272 | Loss: 2835.9595\n",
      "  ğŸ“¦ Batch 386/1272 | Loss: 2783.3074\n",
      "  ğŸ“¦ Batch 387/1272 | Loss: 2765.7871\n",
      "  ğŸ“¦ Batch 388/1272 | Loss: 2747.1248\n",
      "  ğŸ“¦ Batch 389/1272 | Loss: 2837.7881\n",
      "  ğŸ“¦ Batch 390/1272 | Loss: 2880.5244\n",
      "  ğŸ“¦ Batch 391/1272 | Loss: 2905.1582\n",
      "  ğŸ“¦ Batch 392/1272 | Loss: 2803.8779\n",
      "  ğŸ“¦ Batch 393/1272 | Loss: 2708.3018\n",
      "  ğŸ“¦ Batch 394/1272 | Loss: 2758.9976\n",
      "  ğŸ“¦ Batch 395/1272 | Loss: 2820.3484\n",
      "  ğŸ“¦ Batch 396/1272 | Loss: 2626.5288\n",
      "  ğŸ“¦ Batch 397/1272 | Loss: 2840.9546\n",
      "  ğŸ“¦ Batch 398/1272 | Loss: 2709.1167\n",
      "  ğŸ“¦ Batch 399/1272 | Loss: 2810.2957\n",
      "  ğŸ“¦ Batch 400/1272 | Loss: 2810.7290\n",
      "  ğŸ“¦ Batch 401/1272 | Loss: 2769.1638\n",
      "  ğŸ“¦ Batch 402/1272 | Loss: 2737.6548\n",
      "  ğŸ“¦ Batch 403/1272 | Loss: 2865.5698\n",
      "  ğŸ“¦ Batch 404/1272 | Loss: 2734.0361\n",
      "  ğŸ“¦ Batch 405/1272 | Loss: 2863.6616\n",
      "  ğŸ“¦ Batch 406/1272 | Loss: 2731.0874\n",
      "  ğŸ“¦ Batch 407/1272 | Loss: 2767.1812\n",
      "  ğŸ“¦ Batch 408/1272 | Loss: 2710.4270\n",
      "  ğŸ“¦ Batch 409/1272 | Loss: 2792.4434\n",
      "  ğŸ“¦ Batch 410/1272 | Loss: 2819.5688\n",
      "  ğŸ“¦ Batch 411/1272 | Loss: 2656.4116\n",
      "  ğŸ“¦ Batch 412/1272 | Loss: 2846.5479\n",
      "  ğŸ“¦ Batch 413/1272 | Loss: 2760.3657\n",
      "  ğŸ“¦ Batch 414/1272 | Loss: 2783.1899\n",
      "  ğŸ“¦ Batch 415/1272 | Loss: 2713.4194\n",
      "  ğŸ“¦ Batch 416/1272 | Loss: 2763.7798\n",
      "  ğŸ“¦ Batch 417/1272 | Loss: 2679.3655\n",
      "  ğŸ“¦ Batch 418/1272 | Loss: 2691.3633\n",
      "  ğŸ“¦ Batch 419/1272 | Loss: 2839.8623\n",
      "  ğŸ“¦ Batch 420/1272 | Loss: 2706.9983\n",
      "  ğŸ“¦ Batch 421/1272 | Loss: 2760.0771\n",
      "  ğŸ“¦ Batch 422/1272 | Loss: 2777.3721\n",
      "  ğŸ“¦ Batch 423/1272 | Loss: 2708.8418\n",
      "  ğŸ“¦ Batch 424/1272 | Loss: 2686.1619\n",
      "  ğŸ“¦ Batch 425/1272 | Loss: 2722.9370\n",
      "  ğŸ“¦ Batch 426/1272 | Loss: 2851.1934\n",
      "  ğŸ“¦ Batch 427/1272 | Loss: 2823.6650\n",
      "  ğŸ“¦ Batch 428/1272 | Loss: 2717.9927\n",
      "  ğŸ“¦ Batch 429/1272 | Loss: 2805.2419\n",
      "  ğŸ“¦ Batch 430/1272 | Loss: 2758.0654\n",
      "  ğŸ“¦ Batch 431/1272 | Loss: 2772.4922\n",
      "  ğŸ“¦ Batch 432/1272 | Loss: 2749.7312\n",
      "  ğŸ“¦ Batch 433/1272 | Loss: 2760.9194\n",
      "  ğŸ“¦ Batch 434/1272 | Loss: 2904.7056\n",
      "  ğŸ“¦ Batch 435/1272 | Loss: 2708.4570\n",
      "  ğŸ“¦ Batch 436/1272 | Loss: 2786.6221\n",
      "  ğŸ“¦ Batch 437/1272 | Loss: 2829.8699\n",
      "  ğŸ“¦ Batch 438/1272 | Loss: 2766.8115\n",
      "  ğŸ“¦ Batch 439/1272 | Loss: 2907.3279\n",
      "  ğŸ“¦ Batch 440/1272 | Loss: 2721.6895\n",
      "  ğŸ“¦ Batch 441/1272 | Loss: 2654.8044\n",
      "  ğŸ“¦ Batch 442/1272 | Loss: 2742.4719\n",
      "  ğŸ“¦ Batch 443/1272 | Loss: 2681.3220\n",
      "  ğŸ“¦ Batch 444/1272 | Loss: 2731.7007\n",
      "  ğŸ“¦ Batch 445/1272 | Loss: 2693.0156\n",
      "  ğŸ“¦ Batch 446/1272 | Loss: 2687.5571\n",
      "  ğŸ“¦ Batch 447/1272 | Loss: 2744.1128\n",
      "  ğŸ“¦ Batch 448/1272 | Loss: 2631.5903\n",
      "  ğŸ“¦ Batch 449/1272 | Loss: 2833.8157\n",
      "  ğŸ“¦ Batch 450/1272 | Loss: 2745.3433\n",
      "  ğŸ“¦ Batch 451/1272 | Loss: 3012.3521\n",
      "  ğŸ“¦ Batch 452/1272 | Loss: 2935.7144\n",
      "  ğŸ“¦ Batch 453/1272 | Loss: 2696.6531\n",
      "  ğŸ“¦ Batch 454/1272 | Loss: 2787.9111\n",
      "  ğŸ“¦ Batch 455/1272 | Loss: 2844.0850\n",
      "  ğŸ“¦ Batch 456/1272 | Loss: 2954.6211\n",
      "  ğŸ“¦ Batch 457/1272 | Loss: 2894.8162\n",
      "  ğŸ“¦ Batch 458/1272 | Loss: 2863.8958\n",
      "  ğŸ“¦ Batch 459/1272 | Loss: 2682.2832\n",
      "  ğŸ“¦ Batch 460/1272 | Loss: 2846.1121\n",
      "  ğŸ“¦ Batch 461/1272 | Loss: 2809.5049\n",
      "  ğŸ“¦ Batch 462/1272 | Loss: 2784.1050\n",
      "  ğŸ“¦ Batch 463/1272 | Loss: 2929.1943\n",
      "  ğŸ“¦ Batch 464/1272 | Loss: 2757.3167\n",
      "  ğŸ“¦ Batch 465/1272 | Loss: 2791.4214\n",
      "  ğŸ“¦ Batch 466/1272 | Loss: 2617.5728\n",
      "  ğŸ“¦ Batch 467/1272 | Loss: 2691.6621\n",
      "  ğŸ“¦ Batch 468/1272 | Loss: 2738.5854\n",
      "  ğŸ“¦ Batch 469/1272 | Loss: 2778.6860\n",
      "  ğŸ“¦ Batch 470/1272 | Loss: 2877.4399\n",
      "  ğŸ“¦ Batch 471/1272 | Loss: 2750.1936\n",
      "  ğŸ“¦ Batch 472/1272 | Loss: 2711.9697\n",
      "  ğŸ“¦ Batch 473/1272 | Loss: 2838.4126\n",
      "  ğŸ“¦ Batch 474/1272 | Loss: 2724.9937\n",
      "  ğŸ“¦ Batch 475/1272 | Loss: 2574.5859\n",
      "  ğŸ“¦ Batch 476/1272 | Loss: 2740.4087\n",
      "  ğŸ“¦ Batch 477/1272 | Loss: 2719.7461\n",
      "  ğŸ“¦ Batch 478/1272 | Loss: 2606.2913\n",
      "  ğŸ“¦ Batch 479/1272 | Loss: 2678.9490\n",
      "  ğŸ“¦ Batch 480/1272 | Loss: 2697.9336\n",
      "  ğŸ“¦ Batch 481/1272 | Loss: 2781.1606\n",
      "  ğŸ“¦ Batch 482/1272 | Loss: 2603.2148\n",
      "  ğŸ“¦ Batch 483/1272 | Loss: 2832.6672\n",
      "  ğŸ“¦ Batch 484/1272 | Loss: 2631.5012\n",
      "  ğŸ“¦ Batch 485/1272 | Loss: 2644.2581\n",
      "  ğŸ“¦ Batch 486/1272 | Loss: 2629.2134\n",
      "  ğŸ“¦ Batch 487/1272 | Loss: 2797.3342\n",
      "  ğŸ“¦ Batch 488/1272 | Loss: 2633.0327\n",
      "  ğŸ“¦ Batch 489/1272 | Loss: 2786.0039\n",
      "  ğŸ“¦ Batch 490/1272 | Loss: 2689.9653\n",
      "  ğŸ“¦ Batch 491/1272 | Loss: 2635.5894\n",
      "  ğŸ“¦ Batch 492/1272 | Loss: 2781.5510\n",
      "  ğŸ“¦ Batch 493/1272 | Loss: 2657.9546\n",
      "  ğŸ“¦ Batch 494/1272 | Loss: 2815.7417\n",
      "  ğŸ“¦ Batch 495/1272 | Loss: 2583.8523\n",
      "  ğŸ“¦ Batch 496/1272 | Loss: 2753.1252\n",
      "  ğŸ“¦ Batch 497/1272 | Loss: 2680.9248\n",
      "  ğŸ“¦ Batch 498/1272 | Loss: 2677.7908\n",
      "  ğŸ“¦ Batch 499/1272 | Loss: 2649.5327\n",
      "  ğŸ“¦ Batch 500/1272 | Loss: 2624.7007\n",
      "  ğŸ“¦ Batch 501/1272 | Loss: 2617.3979\n",
      "  ğŸ“¦ Batch 502/1272 | Loss: 2595.5889\n",
      "  ğŸ“¦ Batch 503/1272 | Loss: 2733.4434\n",
      "  ğŸ“¦ Batch 504/1272 | Loss: 2671.5269\n",
      "  ğŸ“¦ Batch 505/1272 | Loss: 2856.0752\n",
      "  ğŸ“¦ Batch 506/1272 | Loss: 2754.4451\n",
      "  ğŸ“¦ Batch 507/1272 | Loss: 2608.5024\n",
      "  ğŸ“¦ Batch 508/1272 | Loss: 2700.8804\n",
      "  ğŸ“¦ Batch 509/1272 | Loss: 2722.2185\n",
      "  ğŸ“¦ Batch 510/1272 | Loss: 2753.2954\n",
      "  ğŸ“¦ Batch 511/1272 | Loss: 2748.7417\n",
      "  ğŸ“¦ Batch 512/1272 | Loss: 2582.5659\n",
      "  ğŸ“¦ Batch 513/1272 | Loss: 2673.9539\n",
      "  ğŸ“¦ Batch 514/1272 | Loss: 2748.8960\n",
      "  ğŸ“¦ Batch 515/1272 | Loss: 2724.3799\n",
      "  ğŸ“¦ Batch 516/1272 | Loss: 2530.5471\n",
      "  ğŸ“¦ Batch 517/1272 | Loss: 2768.3198\n",
      "  ğŸ“¦ Batch 518/1272 | Loss: 2612.9819\n",
      "  ğŸ“¦ Batch 519/1272 | Loss: 2630.1692\n",
      "  ğŸ“¦ Batch 520/1272 | Loss: 2694.9287\n",
      "  ğŸ“¦ Batch 521/1272 | Loss: 2731.4702\n",
      "  ğŸ“¦ Batch 522/1272 | Loss: 2477.3181\n",
      "  ğŸ“¦ Batch 523/1272 | Loss: 2699.2678\n",
      "  ğŸ“¦ Batch 524/1272 | Loss: 2621.6011\n",
      "  ğŸ“¦ Batch 525/1272 | Loss: 2766.8987\n",
      "  ğŸ“¦ Batch 526/1272 | Loss: 2673.6504\n",
      "  ğŸ“¦ Batch 527/1272 | Loss: 2551.0522\n",
      "  ğŸ“¦ Batch 528/1272 | Loss: 2699.6797\n",
      "  ğŸ“¦ Batch 529/1272 | Loss: 2630.0247\n",
      "  ğŸ“¦ Batch 530/1272 | Loss: 2589.8118\n",
      "  ğŸ“¦ Batch 531/1272 | Loss: 2581.5747\n",
      "  ğŸ“¦ Batch 532/1272 | Loss: 2700.4822\n",
      "  ğŸ“¦ Batch 533/1272 | Loss: 2568.1870\n",
      "  ğŸ“¦ Batch 534/1272 | Loss: 2561.9819\n",
      "  ğŸ“¦ Batch 535/1272 | Loss: 2702.2866\n",
      "  ğŸ“¦ Batch 536/1272 | Loss: 2656.1577\n",
      "  ğŸ“¦ Batch 537/1272 | Loss: 2684.5029\n",
      "  ğŸ“¦ Batch 538/1272 | Loss: 2644.7231\n",
      "  ğŸ“¦ Batch 539/1272 | Loss: 2779.7563\n",
      "  ğŸ“¦ Batch 540/1272 | Loss: 2644.0166\n",
      "  ğŸ“¦ Batch 541/1272 | Loss: 2670.7368\n",
      "  ğŸ“¦ Batch 542/1272 | Loss: 2600.5769\n",
      "  ğŸ“¦ Batch 543/1272 | Loss: 2660.8911\n",
      "  ğŸ“¦ Batch 544/1272 | Loss: 2650.5425\n",
      "  ğŸ“¦ Batch 545/1272 | Loss: 2555.1138\n",
      "  ğŸ“¦ Batch 546/1272 | Loss: 2688.5610\n",
      "  ğŸ“¦ Batch 547/1272 | Loss: 2671.6523\n",
      "  ğŸ“¦ Batch 548/1272 | Loss: 2557.7666\n",
      "  ğŸ“¦ Batch 549/1272 | Loss: 2725.3230\n",
      "  ğŸ“¦ Batch 550/1272 | Loss: 2691.8323\n",
      "  ğŸ“¦ Batch 551/1272 | Loss: 2665.4106\n",
      "  ğŸ“¦ Batch 552/1272 | Loss: 2693.1504\n",
      "  ğŸ“¦ Batch 553/1272 | Loss: 2828.5820\n",
      "  ğŸ“¦ Batch 554/1272 | Loss: 2595.6033\n",
      "  ğŸ“¦ Batch 555/1272 | Loss: 2729.6118\n",
      "  ğŸ“¦ Batch 556/1272 | Loss: 2772.4199\n",
      "  ğŸ“¦ Batch 557/1272 | Loss: 2722.3677\n",
      "  ğŸ“¦ Batch 558/1272 | Loss: 2643.6313\n",
      "  ğŸ“¦ Batch 559/1272 | Loss: 2730.9099\n",
      "  ğŸ“¦ Batch 560/1272 | Loss: 2801.0947\n",
      "  ğŸ“¦ Batch 561/1272 | Loss: 2548.8296\n",
      "  ğŸ“¦ Batch 562/1272 | Loss: 2575.4961\n",
      "  ğŸ“¦ Batch 563/1272 | Loss: 2505.4404\n",
      "  ğŸ“¦ Batch 564/1272 | Loss: 2771.9954\n",
      "  ğŸ“¦ Batch 565/1272 | Loss: 2768.4834\n",
      "  ğŸ“¦ Batch 566/1272 | Loss: 2538.4556\n",
      "  ğŸ“¦ Batch 567/1272 | Loss: 2706.7915\n",
      "  ğŸ“¦ Batch 568/1272 | Loss: 2510.4121\n",
      "  ğŸ“¦ Batch 569/1272 | Loss: 2657.9038\n",
      "  ğŸ“¦ Batch 570/1272 | Loss: 2584.2249\n",
      "  ğŸ“¦ Batch 571/1272 | Loss: 2619.3999\n",
      "  ğŸ“¦ Batch 572/1272 | Loss: 2549.4092\n",
      "  ğŸ“¦ Batch 573/1272 | Loss: 2650.9146\n",
      "  ğŸ“¦ Batch 574/1272 | Loss: 2690.0925\n",
      "  ğŸ“¦ Batch 575/1272 | Loss: 2593.9443\n",
      "  ğŸ“¦ Batch 576/1272 | Loss: 2865.2979\n",
      "  ğŸ“¦ Batch 577/1272 | Loss: 2696.3113\n",
      "  ğŸ“¦ Batch 578/1272 | Loss: 2486.0444\n",
      "  ğŸ“¦ Batch 579/1272 | Loss: 2600.8367\n",
      "  ğŸ“¦ Batch 580/1272 | Loss: 2518.3398\n",
      "  ğŸ“¦ Batch 581/1272 | Loss: 2642.7896\n",
      "  ğŸ“¦ Batch 582/1272 | Loss: 2530.7378\n",
      "  ğŸ“¦ Batch 583/1272 | Loss: 2572.2334\n",
      "  ğŸ“¦ Batch 584/1272 | Loss: 2676.9526\n",
      "  ğŸ“¦ Batch 585/1272 | Loss: 2658.7617\n",
      "  ğŸ“¦ Batch 586/1272 | Loss: 2580.5083\n",
      "  ğŸ“¦ Batch 587/1272 | Loss: 2656.4966\n",
      "  ğŸ“¦ Batch 588/1272 | Loss: 2651.5645\n",
      "  ğŸ“¦ Batch 589/1272 | Loss: 2550.6184\n",
      "  ğŸ“¦ Batch 590/1272 | Loss: 2739.4370\n",
      "  ğŸ“¦ Batch 591/1272 | Loss: 2680.5315\n",
      "  ğŸ“¦ Batch 592/1272 | Loss: 2608.0671\n",
      "  ğŸ“¦ Batch 593/1272 | Loss: 2508.8770\n",
      "  ğŸ“¦ Batch 594/1272 | Loss: 2468.4922\n",
      "  ğŸ“¦ Batch 595/1272 | Loss: 2844.7085\n",
      "  ğŸ“¦ Batch 596/1272 | Loss: 2716.9822\n",
      "  ğŸ“¦ Batch 597/1272 | Loss: 2596.2859\n",
      "  ğŸ“¦ Batch 598/1272 | Loss: 2594.7729\n",
      "  ğŸ“¦ Batch 599/1272 | Loss: 2563.8784\n",
      "  ğŸ“¦ Batch 600/1272 | Loss: 2559.8696\n",
      "  ğŸ“¦ Batch 601/1272 | Loss: 2487.7036\n",
      "  ğŸ“¦ Batch 602/1272 | Loss: 2480.9199\n",
      "  ğŸ“¦ Batch 603/1272 | Loss: 2566.0029\n",
      "  ğŸ“¦ Batch 604/1272 | Loss: 2610.6680\n",
      "  ğŸ“¦ Batch 605/1272 | Loss: 2725.2048\n",
      "  ğŸ“¦ Batch 606/1272 | Loss: 2529.5317\n",
      "  ğŸ“¦ Batch 607/1272 | Loss: 2619.1611\n",
      "  ğŸ“¦ Batch 608/1272 | Loss: 2507.9800\n",
      "  ğŸ“¦ Batch 609/1272 | Loss: 2479.8433\n",
      "  ğŸ“¦ Batch 610/1272 | Loss: 2768.6885\n",
      "  ğŸ“¦ Batch 611/1272 | Loss: 2696.0781\n",
      "  ğŸ“¦ Batch 612/1272 | Loss: 2448.3123\n",
      "  ğŸ“¦ Batch 613/1272 | Loss: 2641.9236\n",
      "  ğŸ“¦ Batch 614/1272 | Loss: 2566.6758\n",
      "  ğŸ“¦ Batch 615/1272 | Loss: 2602.7559\n",
      "  ğŸ“¦ Batch 616/1272 | Loss: 2474.4543\n",
      "  ğŸ“¦ Batch 617/1272 | Loss: 2638.0583\n",
      "  ğŸ“¦ Batch 618/1272 | Loss: 2457.5962\n",
      "  ğŸ“¦ Batch 619/1272 | Loss: 2657.7344\n",
      "  ğŸ“¦ Batch 620/1272 | Loss: 2687.6941\n",
      "  ğŸ“¦ Batch 621/1272 | Loss: 2691.1211\n",
      "  ğŸ“¦ Batch 622/1272 | Loss: 2442.7144\n",
      "  ğŸ“¦ Batch 623/1272 | Loss: 2455.6516\n",
      "  ğŸ“¦ Batch 624/1272 | Loss: 2543.3462\n",
      "  ğŸ“¦ Batch 625/1272 | Loss: 2605.2632\n",
      "  ğŸ“¦ Batch 626/1272 | Loss: 2641.7251\n",
      "  ğŸ“¦ Batch 627/1272 | Loss: 2524.1787\n",
      "  ğŸ“¦ Batch 628/1272 | Loss: 2767.4673\n",
      "  ğŸ“¦ Batch 629/1272 | Loss: 2588.2393\n",
      "  ğŸ“¦ Batch 630/1272 | Loss: 2478.2349\n",
      "  ğŸ“¦ Batch 631/1272 | Loss: 2625.8843\n",
      "  ğŸ“¦ Batch 632/1272 | Loss: 2554.5962\n",
      "  ğŸ“¦ Batch 633/1272 | Loss: 2512.6038\n",
      "  ğŸ“¦ Batch 634/1272 | Loss: 2617.9600\n",
      "  ğŸ“¦ Batch 635/1272 | Loss: 2644.8398\n",
      "  ğŸ“¦ Batch 636/1272 | Loss: 2527.7710\n",
      "  ğŸ“¦ Batch 637/1272 | Loss: 2610.3628\n",
      "  ğŸ“¦ Batch 638/1272 | Loss: 2530.4961\n",
      "  ğŸ“¦ Batch 639/1272 | Loss: 2513.8735\n",
      "  ğŸ“¦ Batch 640/1272 | Loss: 2520.6445\n",
      "  ğŸ“¦ Batch 641/1272 | Loss: 2666.6899\n",
      "  ğŸ“¦ Batch 642/1272 | Loss: 2558.1038\n",
      "  ğŸ“¦ Batch 643/1272 | Loss: 2419.7368\n",
      "  ğŸ“¦ Batch 644/1272 | Loss: 2629.5137\n",
      "  ğŸ“¦ Batch 645/1272 | Loss: 2623.8987\n",
      "  ğŸ“¦ Batch 646/1272 | Loss: 2550.0142\n",
      "  ğŸ“¦ Batch 647/1272 | Loss: 2535.8948\n",
      "  ğŸ“¦ Batch 648/1272 | Loss: 2525.5256\n",
      "  ğŸ“¦ Batch 649/1272 | Loss: 2635.4092\n",
      "  ğŸ“¦ Batch 650/1272 | Loss: 2593.9460\n",
      "  ğŸ“¦ Batch 651/1272 | Loss: 2537.0393\n",
      "  ğŸ“¦ Batch 652/1272 | Loss: 2429.6694\n",
      "  ğŸ“¦ Batch 653/1272 | Loss: 2657.2139\n",
      "  ğŸ“¦ Batch 654/1272 | Loss: 2560.7756\n",
      "  ğŸ“¦ Batch 655/1272 | Loss: 2495.9431\n",
      "  ğŸ“¦ Batch 656/1272 | Loss: 2555.9067\n",
      "  ğŸ“¦ Batch 657/1272 | Loss: 2536.6279\n",
      "  ğŸ“¦ Batch 658/1272 | Loss: 2648.5847\n",
      "  ğŸ“¦ Batch 659/1272 | Loss: 2594.6130\n",
      "  ğŸ“¦ Batch 660/1272 | Loss: 2526.6211\n",
      "  ğŸ“¦ Batch 661/1272 | Loss: 2563.4893\n",
      "  ğŸ“¦ Batch 662/1272 | Loss: 2561.6426\n",
      "  ğŸ“¦ Batch 663/1272 | Loss: 2620.4700\n",
      "  ğŸ“¦ Batch 664/1272 | Loss: 2537.3413\n",
      "  ğŸ“¦ Batch 665/1272 | Loss: 2501.0381\n",
      "  ğŸ“¦ Batch 666/1272 | Loss: 2574.1719\n",
      "  ğŸ“¦ Batch 667/1272 | Loss: 2592.0811\n",
      "  ğŸ“¦ Batch 668/1272 | Loss: 2628.9128\n",
      "  ğŸ“¦ Batch 669/1272 | Loss: 2461.0854\n",
      "  ğŸ“¦ Batch 670/1272 | Loss: 2586.1196\n",
      "  ğŸ“¦ Batch 671/1272 | Loss: 2459.9907\n",
      "  ğŸ“¦ Batch 672/1272 | Loss: 2487.7092\n",
      "  ğŸ“¦ Batch 673/1272 | Loss: 2531.7473\n",
      "  ğŸ“¦ Batch 674/1272 | Loss: 2545.0239\n",
      "  ğŸ“¦ Batch 675/1272 | Loss: 2692.7896\n",
      "  ğŸ“¦ Batch 676/1272 | Loss: 2563.2383\n",
      "  ğŸ“¦ Batch 677/1272 | Loss: 2579.1616\n",
      "  ğŸ“¦ Batch 678/1272 | Loss: 2517.8088\n",
      "  ğŸ“¦ Batch 679/1272 | Loss: 2602.1423\n",
      "  ğŸ“¦ Batch 680/1272 | Loss: 2394.1343\n",
      "  ğŸ“¦ Batch 681/1272 | Loss: 2586.9902\n",
      "  ğŸ“¦ Batch 682/1272 | Loss: 2548.9434\n",
      "  ğŸ“¦ Batch 683/1272 | Loss: 2465.9648\n",
      "  ğŸ“¦ Batch 684/1272 | Loss: 2470.2537\n",
      "  ğŸ“¦ Batch 685/1272 | Loss: 2507.4636\n",
      "  ğŸ“¦ Batch 686/1272 | Loss: 2497.3352\n",
      "  ğŸ“¦ Batch 687/1272 | Loss: 2528.8735\n",
      "  ğŸ“¦ Batch 688/1272 | Loss: 2395.5869\n",
      "  ğŸ“¦ Batch 689/1272 | Loss: 2659.3057\n",
      "  ğŸ“¦ Batch 690/1272 | Loss: 2581.4395\n",
      "  ğŸ“¦ Batch 691/1272 | Loss: 2507.0720\n",
      "  ğŸ“¦ Batch 692/1272 | Loss: 2567.3381\n",
      "  ğŸ“¦ Batch 693/1272 | Loss: 2454.9570\n",
      "  ğŸ“¦ Batch 694/1272 | Loss: 2405.8547\n",
      "  ğŸ“¦ Batch 695/1272 | Loss: 2449.2234\n",
      "  ğŸ“¦ Batch 696/1272 | Loss: 2765.4504\n",
      "  ğŸ“¦ Batch 697/1272 | Loss: 2551.8203\n",
      "  ğŸ“¦ Batch 698/1272 | Loss: 2552.2620\n",
      "  ğŸ“¦ Batch 699/1272 | Loss: 2594.5295\n",
      "  ğŸ“¦ Batch 700/1272 | Loss: 2461.7661\n",
      "  ğŸ“¦ Batch 701/1272 | Loss: 2402.6968\n",
      "  ğŸ“¦ Batch 702/1272 | Loss: 2423.1646\n",
      "  ğŸ“¦ Batch 703/1272 | Loss: 2433.5869\n",
      "  ğŸ“¦ Batch 704/1272 | Loss: 2518.7195\n",
      "  ğŸ“¦ Batch 705/1272 | Loss: 2661.7744\n",
      "  ğŸ“¦ Batch 706/1272 | Loss: 2523.3950\n",
      "  ğŸ“¦ Batch 707/1272 | Loss: 2471.8760\n",
      "  ğŸ“¦ Batch 708/1272 | Loss: 2513.8203\n",
      "  ğŸ“¦ Batch 709/1272 | Loss: 2510.3120\n",
      "  ğŸ“¦ Batch 710/1272 | Loss: 2510.8950\n",
      "  ğŸ“¦ Batch 711/1272 | Loss: 2519.7881\n",
      "  ğŸ“¦ Batch 712/1272 | Loss: 2538.9771\n",
      "  ğŸ“¦ Batch 713/1272 | Loss: 2632.7285\n",
      "  ğŸ“¦ Batch 714/1272 | Loss: 2434.5486\n",
      "  ğŸ“¦ Batch 715/1272 | Loss: 2583.0903\n",
      "  ğŸ“¦ Batch 716/1272 | Loss: 2573.5576\n",
      "  ğŸ“¦ Batch 717/1272 | Loss: 2514.5012\n",
      "  ğŸ“¦ Batch 718/1272 | Loss: 2465.5605\n",
      "  ğŸ“¦ Batch 719/1272 | Loss: 2447.2554\n",
      "  ğŸ“¦ Batch 720/1272 | Loss: 2490.7939\n",
      "  ğŸ“¦ Batch 721/1272 | Loss: 2588.9241\n",
      "  ğŸ“¦ Batch 722/1272 | Loss: 2501.3772\n",
      "  ğŸ“¦ Batch 723/1272 | Loss: 2590.9985\n",
      "  ğŸ“¦ Batch 724/1272 | Loss: 2529.3423\n",
      "  ğŸ“¦ Batch 725/1272 | Loss: 2470.7817\n",
      "  ğŸ“¦ Batch 726/1272 | Loss: 2517.2637\n",
      "  ğŸ“¦ Batch 727/1272 | Loss: 2413.9771\n",
      "  ğŸ“¦ Batch 728/1272 | Loss: 2462.8169\n",
      "  ğŸ“¦ Batch 729/1272 | Loss: 2499.1443\n",
      "  ğŸ“¦ Batch 730/1272 | Loss: 2551.8848\n",
      "  ğŸ“¦ Batch 731/1272 | Loss: 2600.6882\n",
      "  ğŸ“¦ Batch 732/1272 | Loss: 2412.8245\n",
      "  ğŸ“¦ Batch 733/1272 | Loss: 2463.8350\n",
      "  ğŸ“¦ Batch 734/1272 | Loss: 2710.2441\n",
      "  ğŸ“¦ Batch 735/1272 | Loss: 2570.3022\n",
      "  ğŸ“¦ Batch 736/1272 | Loss: 2460.6011\n",
      "  ğŸ“¦ Batch 737/1272 | Loss: 2512.7441\n",
      "  ğŸ“¦ Batch 738/1272 | Loss: 2414.5732\n",
      "  ğŸ“¦ Batch 739/1272 | Loss: 2478.5085\n",
      "  ğŸ“¦ Batch 740/1272 | Loss: 2510.0061\n",
      "  ğŸ“¦ Batch 741/1272 | Loss: 2561.9788\n",
      "  ğŸ“¦ Batch 742/1272 | Loss: 2571.4614\n",
      "  ğŸ“¦ Batch 743/1272 | Loss: 2440.3762\n",
      "  ğŸ“¦ Batch 744/1272 | Loss: 2489.4248\n",
      "  ğŸ“¦ Batch 745/1272 | Loss: 2451.9866\n",
      "  ğŸ“¦ Batch 746/1272 | Loss: 2506.7114\n",
      "  ğŸ“¦ Batch 747/1272 | Loss: 2506.9497\n",
      "  ğŸ“¦ Batch 748/1272 | Loss: 2389.2625\n",
      "  ğŸ“¦ Batch 749/1272 | Loss: 2644.8716\n",
      "  ğŸ“¦ Batch 750/1272 | Loss: 2472.9556\n",
      "  ğŸ“¦ Batch 751/1272 | Loss: 2514.9746\n",
      "  ğŸ“¦ Batch 752/1272 | Loss: 2544.2903\n",
      "  ğŸ“¦ Batch 753/1272 | Loss: 2580.5869\n",
      "  ğŸ“¦ Batch 754/1272 | Loss: 2356.9333\n",
      "  ğŸ“¦ Batch 755/1272 | Loss: 2364.5728\n",
      "  ğŸ“¦ Batch 756/1272 | Loss: 2480.0923\n",
      "  ğŸ“¦ Batch 757/1272 | Loss: 2437.3125\n",
      "  ğŸ“¦ Batch 758/1272 | Loss: 2421.2578\n",
      "  ğŸ“¦ Batch 759/1272 | Loss: 2463.3843\n",
      "  ğŸ“¦ Batch 760/1272 | Loss: 2423.7163\n",
      "  ğŸ“¦ Batch 761/1272 | Loss: 2427.6558\n",
      "  ğŸ“¦ Batch 762/1272 | Loss: 2594.4377\n",
      "  ğŸ“¦ Batch 763/1272 | Loss: 2572.9180\n",
      "  ğŸ“¦ Batch 764/1272 | Loss: 2445.1941\n",
      "  ğŸ“¦ Batch 765/1272 | Loss: 2452.7844\n",
      "  ğŸ“¦ Batch 766/1272 | Loss: 2577.3201\n",
      "  ğŸ“¦ Batch 767/1272 | Loss: 2487.5347\n",
      "  ğŸ“¦ Batch 768/1272 | Loss: 2434.0713\n",
      "  ğŸ“¦ Batch 769/1272 | Loss: 2483.6157\n",
      "  ğŸ“¦ Batch 770/1272 | Loss: 2470.7908\n",
      "  ğŸ“¦ Batch 771/1272 | Loss: 2482.4375\n",
      "  ğŸ“¦ Batch 772/1272 | Loss: 2477.9246\n",
      "  ğŸ“¦ Batch 773/1272 | Loss: 2466.1682\n",
      "  ğŸ“¦ Batch 774/1272 | Loss: 2819.9341\n",
      "  ğŸ“¦ Batch 775/1272 | Loss: 2550.2080\n",
      "  ğŸ“¦ Batch 776/1272 | Loss: 2441.5410\n",
      "  ğŸ“¦ Batch 777/1272 | Loss: 2324.6245\n",
      "  ğŸ“¦ Batch 778/1272 | Loss: 2508.6062\n",
      "  ğŸ“¦ Batch 779/1272 | Loss: 2562.9150\n",
      "  ğŸ“¦ Batch 780/1272 | Loss: 2467.6841\n",
      "  ğŸ“¦ Batch 781/1272 | Loss: 2448.5591\n",
      "  ğŸ“¦ Batch 782/1272 | Loss: 2478.5129\n",
      "  ğŸ“¦ Batch 783/1272 | Loss: 2518.4771\n",
      "  ğŸ“¦ Batch 784/1272 | Loss: 2511.0874\n",
      "  ğŸ“¦ Batch 785/1272 | Loss: 2498.0449\n",
      "  ğŸ“¦ Batch 786/1272 | Loss: 2436.2271\n",
      "  ğŸ“¦ Batch 787/1272 | Loss: 2377.1089\n",
      "  ğŸ“¦ Batch 788/1272 | Loss: 2490.8535\n",
      "  ğŸ“¦ Batch 789/1272 | Loss: 2458.6333\n",
      "  ğŸ“¦ Batch 790/1272 | Loss: 2394.7432\n",
      "  ğŸ“¦ Batch 791/1272 | Loss: 2393.8833\n",
      "  ğŸ“¦ Batch 792/1272 | Loss: 2508.2844\n",
      "  ğŸ“¦ Batch 793/1272 | Loss: 2535.2700\n",
      "  ğŸ“¦ Batch 794/1272 | Loss: 2356.1084\n",
      "  ğŸ“¦ Batch 795/1272 | Loss: 2557.2075\n",
      "  ğŸ“¦ Batch 796/1272 | Loss: 2408.3970\n",
      "  ğŸ“¦ Batch 797/1272 | Loss: 2491.1975\n",
      "  ğŸ“¦ Batch 798/1272 | Loss: 2337.2656\n",
      "  ğŸ“¦ Batch 799/1272 | Loss: 2364.0264\n",
      "  ğŸ“¦ Batch 800/1272 | Loss: 2428.6035\n",
      "  ğŸ“¦ Batch 801/1272 | Loss: 2449.9600\n",
      "  ğŸ“¦ Batch 802/1272 | Loss: 2414.6016\n",
      "  ğŸ“¦ Batch 803/1272 | Loss: 2300.3699\n",
      "  ğŸ“¦ Batch 804/1272 | Loss: 2303.1782\n",
      "  ğŸ“¦ Batch 805/1272 | Loss: 2426.6941\n",
      "  ğŸ“¦ Batch 806/1272 | Loss: 2348.0227\n",
      "  ğŸ“¦ Batch 807/1272 | Loss: 2490.8936\n",
      "  ğŸ“¦ Batch 808/1272 | Loss: 2380.5845\n",
      "  ğŸ“¦ Batch 809/1272 | Loss: 2491.5017\n",
      "  ğŸ“¦ Batch 810/1272 | Loss: 2367.8281\n",
      "  ğŸ“¦ Batch 811/1272 | Loss: 2469.5537\n",
      "  ğŸ“¦ Batch 812/1272 | Loss: 2462.4099\n",
      "  ğŸ“¦ Batch 813/1272 | Loss: 2505.9583\n",
      "  ğŸ“¦ Batch 814/1272 | Loss: 2420.7212\n",
      "  ğŸ“¦ Batch 815/1272 | Loss: 2374.1768\n",
      "  ğŸ“¦ Batch 816/1272 | Loss: 2579.8284\n",
      "  ğŸ“¦ Batch 817/1272 | Loss: 2663.1558\n",
      "  ğŸ“¦ Batch 818/1272 | Loss: 2535.9094\n",
      "  ğŸ“¦ Batch 819/1272 | Loss: 2586.3782\n",
      "  ğŸ“¦ Batch 820/1272 | Loss: 2422.2498\n",
      "  ğŸ“¦ Batch 821/1272 | Loss: 2401.3828\n",
      "  ğŸ“¦ Batch 822/1272 | Loss: 2459.6721\n",
      "  ğŸ“¦ Batch 823/1272 | Loss: 2485.4509\n",
      "  ğŸ“¦ Batch 824/1272 | Loss: 2372.2419\n",
      "  ğŸ“¦ Batch 825/1272 | Loss: 2405.6108\n",
      "  ğŸ“¦ Batch 826/1272 | Loss: 2518.9517\n",
      "  ğŸ“¦ Batch 827/1272 | Loss: 2439.0034\n",
      "  ğŸ“¦ Batch 828/1272 | Loss: 2429.6704\n",
      "  ğŸ“¦ Batch 829/1272 | Loss: 2487.9932\n",
      "  ğŸ“¦ Batch 830/1272 | Loss: 2387.9385\n",
      "  ğŸ“¦ Batch 831/1272 | Loss: 2403.4978\n",
      "  ğŸ“¦ Batch 832/1272 | Loss: 2372.2056\n",
      "  ğŸ“¦ Batch 833/1272 | Loss: 2337.3926\n",
      "  ğŸ“¦ Batch 834/1272 | Loss: 2399.5391\n",
      "  ğŸ“¦ Batch 835/1272 | Loss: 2528.3481\n",
      "  ğŸ“¦ Batch 836/1272 | Loss: 2517.3977\n",
      "  ğŸ“¦ Batch 837/1272 | Loss: 2396.8503\n",
      "  ğŸ“¦ Batch 838/1272 | Loss: 2451.7217\n",
      "  ğŸ“¦ Batch 839/1272 | Loss: 2526.2615\n",
      "  ğŸ“¦ Batch 840/1272 | Loss: 2333.3022\n",
      "  ğŸ“¦ Batch 841/1272 | Loss: 2473.7839\n",
      "  ğŸ“¦ Batch 842/1272 | Loss: 2429.8162\n",
      "  ğŸ“¦ Batch 843/1272 | Loss: 2296.9004\n",
      "  ğŸ“¦ Batch 844/1272 | Loss: 2465.7227\n",
      "  ğŸ“¦ Batch 845/1272 | Loss: 2439.0962\n",
      "  ğŸ“¦ Batch 846/1272 | Loss: 2364.1572\n",
      "  ğŸ“¦ Batch 847/1272 | Loss: 2349.2266\n",
      "  ğŸ“¦ Batch 848/1272 | Loss: 2463.2397\n",
      "  ğŸ“¦ Batch 849/1272 | Loss: 2490.5093\n",
      "  ğŸ“¦ Batch 850/1272 | Loss: 2229.5195\n",
      "  ğŸ“¦ Batch 851/1272 | Loss: 2348.1836\n",
      "  ğŸ“¦ Batch 852/1272 | Loss: 2381.6523\n",
      "  ğŸ“¦ Batch 853/1272 | Loss: 2346.8887\n",
      "  ğŸ“¦ Batch 854/1272 | Loss: 2531.5271\n",
      "  ğŸ“¦ Batch 855/1272 | Loss: 2542.2000\n",
      "  ğŸ“¦ Batch 856/1272 | Loss: 2282.1167\n",
      "  ğŸ“¦ Batch 857/1272 | Loss: 2351.5762\n",
      "  ğŸ“¦ Batch 858/1272 | Loss: 2348.3784\n",
      "  ğŸ“¦ Batch 859/1272 | Loss: 2364.3665\n",
      "  ğŸ“¦ Batch 860/1272 | Loss: 2689.3879\n",
      "  ğŸ“¦ Batch 861/1272 | Loss: 2394.3928\n",
      "  ğŸ“¦ Batch 862/1272 | Loss: 2417.5286\n",
      "  ğŸ“¦ Batch 863/1272 | Loss: 2451.0024\n",
      "  ğŸ“¦ Batch 864/1272 | Loss: 2315.8208\n",
      "  ğŸ“¦ Batch 865/1272 | Loss: 2443.2153\n",
      "  ğŸ“¦ Batch 866/1272 | Loss: 2408.3555\n",
      "  ğŸ“¦ Batch 867/1272 | Loss: 2362.2451\n",
      "  ğŸ“¦ Batch 868/1272 | Loss: 2297.9575\n",
      "  ğŸ“¦ Batch 869/1272 | Loss: 2362.9961\n",
      "  ğŸ“¦ Batch 870/1272 | Loss: 2395.9443\n",
      "  ğŸ“¦ Batch 871/1272 | Loss: 2220.3606\n",
      "  ğŸ“¦ Batch 872/1272 | Loss: 2458.7700\n",
      "  ğŸ“¦ Batch 873/1272 | Loss: 2443.1663\n",
      "  ğŸ“¦ Batch 874/1272 | Loss: 2483.0117\n",
      "  ğŸ“¦ Batch 875/1272 | Loss: 2318.4575\n",
      "  ğŸ“¦ Batch 876/1272 | Loss: 2390.9495\n",
      "  ğŸ“¦ Batch 877/1272 | Loss: 2385.5068\n",
      "  ğŸ“¦ Batch 878/1272 | Loss: 2392.4399\n",
      "  ğŸ“¦ Batch 879/1272 | Loss: 2412.8750\n",
      "  ğŸ“¦ Batch 880/1272 | Loss: 2271.7183\n",
      "  ğŸ“¦ Batch 881/1272 | Loss: 2377.8550\n",
      "  ğŸ“¦ Batch 882/1272 | Loss: 2238.7788\n",
      "  ğŸ“¦ Batch 883/1272 | Loss: 2332.1519\n",
      "  ğŸ“¦ Batch 884/1272 | Loss: 2543.6321\n",
      "  ğŸ“¦ Batch 885/1272 | Loss: 2489.6592\n",
      "  ğŸ“¦ Batch 886/1272 | Loss: 2331.9346\n",
      "  ğŸ“¦ Batch 887/1272 | Loss: 2465.9871\n",
      "  ğŸ“¦ Batch 888/1272 | Loss: 2263.5718\n",
      "  ğŸ“¦ Batch 889/1272 | Loss: 2504.4673\n",
      "  ğŸ“¦ Batch 890/1272 | Loss: 2431.6260\n",
      "  ğŸ“¦ Batch 891/1272 | Loss: 2321.5940\n",
      "  ğŸ“¦ Batch 892/1272 | Loss: 2449.9490\n",
      "  ğŸ“¦ Batch 893/1272 | Loss: 2448.2339\n",
      "  ğŸ“¦ Batch 894/1272 | Loss: 2284.5703\n",
      "  ğŸ“¦ Batch 895/1272 | Loss: 2436.1089\n",
      "  ğŸ“¦ Batch 896/1272 | Loss: 2474.1392\n",
      "  ğŸ“¦ Batch 897/1272 | Loss: 2460.7290\n",
      "  ğŸ“¦ Batch 898/1272 | Loss: 2657.1436\n",
      "  ğŸ“¦ Batch 899/1272 | Loss: 2267.2935\n",
      "  ğŸ“¦ Batch 900/1272 | Loss: 2399.3730\n",
      "  ğŸ“¦ Batch 901/1272 | Loss: 2364.3252\n",
      "  ğŸ“¦ Batch 902/1272 | Loss: 2368.4819\n",
      "  ğŸ“¦ Batch 903/1272 | Loss: 2303.8047\n",
      "  ğŸ“¦ Batch 904/1272 | Loss: 2373.4751\n",
      "  ğŸ“¦ Batch 905/1272 | Loss: 2285.7349\n",
      "  ğŸ“¦ Batch 906/1272 | Loss: 2334.4944\n",
      "  ğŸ“¦ Batch 907/1272 | Loss: 2352.5952\n",
      "  ğŸ“¦ Batch 908/1272 | Loss: 2435.5703\n",
      "  ğŸ“¦ Batch 909/1272 | Loss: 2369.8403\n",
      "  ğŸ“¦ Batch 910/1272 | Loss: 2265.5803\n",
      "  ğŸ“¦ Batch 911/1272 | Loss: 2260.2788\n",
      "  ğŸ“¦ Batch 912/1272 | Loss: 2326.2610\n",
      "  ğŸ“¦ Batch 913/1272 | Loss: 2283.9868\n",
      "  ğŸ“¦ Batch 914/1272 | Loss: 2513.4487\n",
      "  ğŸ“¦ Batch 915/1272 | Loss: 2420.0964\n",
      "  ğŸ“¦ Batch 916/1272 | Loss: 2472.2461\n",
      "  ğŸ“¦ Batch 917/1272 | Loss: 2414.3066\n",
      "  ğŸ“¦ Batch 918/1272 | Loss: 2405.8220\n",
      "  ğŸ“¦ Batch 919/1272 | Loss: 2528.6357\n",
      "  ğŸ“¦ Batch 920/1272 | Loss: 2484.5337\n",
      "  ğŸ“¦ Batch 921/1272 | Loss: 2231.6482\n",
      "  ğŸ“¦ Batch 922/1272 | Loss: 2351.2649\n",
      "  ğŸ“¦ Batch 923/1272 | Loss: 2441.3403\n",
      "  ğŸ“¦ Batch 924/1272 | Loss: 2343.2810\n",
      "  ğŸ“¦ Batch 925/1272 | Loss: 2470.7407\n",
      "  ğŸ“¦ Batch 926/1272 | Loss: 2459.7849\n",
      "  ğŸ“¦ Batch 927/1272 | Loss: 2269.9507\n",
      "  ğŸ“¦ Batch 928/1272 | Loss: 2290.5791\n",
      "  ğŸ“¦ Batch 929/1272 | Loss: 2361.5359\n",
      "  ğŸ“¦ Batch 930/1272 | Loss: 2328.5234\n",
      "  ğŸ“¦ Batch 931/1272 | Loss: 2439.6260\n",
      "  ğŸ“¦ Batch 932/1272 | Loss: 2275.6497\n",
      "  ğŸ“¦ Batch 933/1272 | Loss: 2348.4268\n",
      "  ğŸ“¦ Batch 934/1272 | Loss: 2266.8496\n",
      "  ğŸ“¦ Batch 935/1272 | Loss: 2244.7158\n",
      "  ğŸ“¦ Batch 936/1272 | Loss: 2267.7964\n",
      "  ğŸ“¦ Batch 937/1272 | Loss: 2342.5042\n",
      "  ğŸ“¦ Batch 938/1272 | Loss: 2408.7856\n",
      "  ğŸ“¦ Batch 939/1272 | Loss: 2381.1318\n",
      "  ğŸ“¦ Batch 940/1272 | Loss: 2199.7754\n",
      "  ğŸ“¦ Batch 941/1272 | Loss: 2302.8640\n",
      "  ğŸ“¦ Batch 942/1272 | Loss: 2336.0967\n",
      "  ğŸ“¦ Batch 943/1272 | Loss: 2467.4731\n",
      "  ğŸ“¦ Batch 944/1272 | Loss: 2352.6255\n",
      "  ğŸ“¦ Batch 945/1272 | Loss: 2421.1116\n",
      "  ğŸ“¦ Batch 946/1272 | Loss: 2345.2954\n",
      "  ğŸ“¦ Batch 947/1272 | Loss: 2306.0132\n",
      "  ğŸ“¦ Batch 948/1272 | Loss: 2340.9419\n",
      "  ğŸ“¦ Batch 949/1272 | Loss: 2214.7568\n",
      "  ğŸ“¦ Batch 950/1272 | Loss: 2447.5977\n",
      "  ğŸ“¦ Batch 951/1272 | Loss: 2251.0137\n",
      "  ğŸ“¦ Batch 952/1272 | Loss: 2365.3696\n",
      "  ğŸ“¦ Batch 953/1272 | Loss: 2360.1545\n",
      "  ğŸ“¦ Batch 954/1272 | Loss: 2262.0952\n",
      "  ğŸ“¦ Batch 955/1272 | Loss: 2302.8413\n",
      "  ğŸ“¦ Batch 956/1272 | Loss: 2402.9331\n",
      "  ğŸ“¦ Batch 957/1272 | Loss: 2433.3940\n",
      "  ğŸ“¦ Batch 958/1272 | Loss: 2275.4868\n",
      "  ğŸ“¦ Batch 959/1272 | Loss: 2359.9192\n",
      "  ğŸ“¦ Batch 960/1272 | Loss: 2297.6953\n",
      "  ğŸ“¦ Batch 961/1272 | Loss: 2230.4316\n",
      "  ğŸ“¦ Batch 962/1272 | Loss: 2440.6741\n",
      "  ğŸ“¦ Batch 963/1272 | Loss: 2204.9805\n",
      "  ğŸ“¦ Batch 964/1272 | Loss: 2444.5750\n",
      "  ğŸ“¦ Batch 965/1272 | Loss: 2416.7124\n",
      "  ğŸ“¦ Batch 966/1272 | Loss: 2312.9907\n",
      "  ğŸ“¦ Batch 967/1272 | Loss: 2329.2483\n",
      "  ğŸ“¦ Batch 968/1272 | Loss: 2357.3091\n",
      "  ğŸ“¦ Batch 969/1272 | Loss: 2343.9492\n",
      "  ğŸ“¦ Batch 970/1272 | Loss: 2264.6460\n",
      "  ğŸ“¦ Batch 971/1272 | Loss: 2293.7759\n",
      "  ğŸ“¦ Batch 972/1272 | Loss: 2330.6528\n",
      "  ğŸ“¦ Batch 973/1272 | Loss: 2305.4536\n",
      "  ğŸ“¦ Batch 974/1272 | Loss: 2178.7671\n",
      "  ğŸ“¦ Batch 975/1272 | Loss: 2262.4136\n",
      "  ğŸ“¦ Batch 976/1272 | Loss: 2232.6016\n",
      "  ğŸ“¦ Batch 977/1272 | Loss: 2304.6252\n",
      "  ğŸ“¦ Batch 978/1272 | Loss: 2214.6282\n",
      "  ğŸ“¦ Batch 979/1272 | Loss: 2252.2126\n",
      "  ğŸ“¦ Batch 980/1272 | Loss: 2344.8892\n",
      "  ğŸ“¦ Batch 981/1272 | Loss: 2335.2073\n",
      "  ğŸ“¦ Batch 982/1272 | Loss: 2288.2231\n",
      "  ğŸ“¦ Batch 983/1272 | Loss: 2307.4473\n",
      "  ğŸ“¦ Batch 984/1272 | Loss: 2313.8599\n",
      "  ğŸ“¦ Batch 985/1272 | Loss: 2297.4512\n",
      "  ğŸ“¦ Batch 986/1272 | Loss: 2278.5291\n",
      "  ğŸ“¦ Batch 987/1272 | Loss: 2366.9683\n",
      "  ğŸ“¦ Batch 988/1272 | Loss: 2291.8210\n",
      "  ğŸ“¦ Batch 989/1272 | Loss: 2346.9307\n",
      "  ğŸ“¦ Batch 990/1272 | Loss: 2231.7075\n",
      "  ğŸ“¦ Batch 991/1272 | Loss: 2197.8826\n",
      "  ğŸ“¦ Batch 992/1272 | Loss: 2267.2039\n",
      "  ğŸ“¦ Batch 993/1272 | Loss: 2405.5005\n",
      "  ğŸ“¦ Batch 994/1272 | Loss: 2268.7495\n",
      "  ğŸ“¦ Batch 995/1272 | Loss: 2373.4409\n",
      "  ğŸ“¦ Batch 996/1272 | Loss: 2284.1921\n",
      "  ğŸ“¦ Batch 997/1272 | Loss: 2319.4944\n",
      "  ğŸ“¦ Batch 998/1272 | Loss: 2210.0764\n",
      "  ğŸ“¦ Batch 999/1272 | Loss: 2237.8286\n",
      "  ğŸ“¦ Batch 1000/1272 | Loss: 2189.1670\n",
      "  ğŸ“¦ Batch 1001/1272 | Loss: 2276.5503\n",
      "  ğŸ“¦ Batch 1002/1272 | Loss: 2219.2551\n",
      "  ğŸ“¦ Batch 1003/1272 | Loss: 2348.3735\n",
      "  ğŸ“¦ Batch 1004/1272 | Loss: 2268.3130\n",
      "  ğŸ“¦ Batch 1005/1272 | Loss: 2306.5576\n",
      "  ğŸ“¦ Batch 1006/1272 | Loss: 2216.5181\n",
      "  ğŸ“¦ Batch 1007/1272 | Loss: 2346.3613\n",
      "  ğŸ“¦ Batch 1008/1272 | Loss: 2404.5596\n",
      "  ğŸ“¦ Batch 1009/1272 | Loss: 2161.5454\n",
      "  ğŸ“¦ Batch 1010/1272 | Loss: 2258.4858\n",
      "  ğŸ“¦ Batch 1011/1272 | Loss: 2219.6526\n",
      "  ğŸ“¦ Batch 1012/1272 | Loss: 2505.4971\n",
      "  ğŸ“¦ Batch 1013/1272 | Loss: 2232.2708\n",
      "  ğŸ“¦ Batch 1014/1272 | Loss: 2351.9897\n",
      "  ğŸ“¦ Batch 1015/1272 | Loss: 2241.0081\n",
      "  ğŸ“¦ Batch 1016/1272 | Loss: 2338.9312\n",
      "  ğŸ“¦ Batch 1017/1272 | Loss: 2379.9194\n",
      "  ğŸ“¦ Batch 1018/1272 | Loss: 2356.2078\n",
      "  ğŸ“¦ Batch 1019/1272 | Loss: 2280.6016\n",
      "  ğŸ“¦ Batch 1020/1272 | Loss: 2393.6013\n",
      "  ğŸ“¦ Batch 1021/1272 | Loss: 2165.1284\n",
      "  ğŸ“¦ Batch 1022/1272 | Loss: 2203.2756\n",
      "  ğŸ“¦ Batch 1023/1272 | Loss: 2177.1755\n",
      "  ğŸ“¦ Batch 1024/1272 | Loss: 2134.4851\n",
      "  ğŸ“¦ Batch 1025/1272 | Loss: 2276.5205\n",
      "  ğŸ“¦ Batch 1026/1272 | Loss: 2281.3298\n",
      "  ğŸ“¦ Batch 1027/1272 | Loss: 2147.0474\n",
      "  ğŸ“¦ Batch 1028/1272 | Loss: 2213.6990\n",
      "  ğŸ“¦ Batch 1029/1272 | Loss: 2282.9116\n",
      "  ğŸ“¦ Batch 1030/1272 | Loss: 2331.6978\n",
      "  ğŸ“¦ Batch 1031/1272 | Loss: 2353.2402\n",
      "  ğŸ“¦ Batch 1032/1272 | Loss: 2230.4304\n",
      "  ğŸ“¦ Batch 1033/1272 | Loss: 2292.5815\n",
      "  ğŸ“¦ Batch 1034/1272 | Loss: 2324.5420\n",
      "  ğŸ“¦ Batch 1035/1272 | Loss: 2302.6284\n",
      "  ğŸ“¦ Batch 1036/1272 | Loss: 2240.7288\n",
      "  ğŸ“¦ Batch 1037/1272 | Loss: 2359.7417\n",
      "  ğŸ“¦ Batch 1038/1272 | Loss: 2366.9080\n",
      "  ğŸ“¦ Batch 1039/1272 | Loss: 2261.0720\n",
      "  ğŸ“¦ Batch 1040/1272 | Loss: 2258.9746\n",
      "  ğŸ“¦ Batch 1041/1272 | Loss: 2254.6289\n",
      "  ğŸ“¦ Batch 1042/1272 | Loss: 2361.9478\n",
      "  ğŸ“¦ Batch 1043/1272 | Loss: 2306.6521\n",
      "  ğŸ“¦ Batch 1044/1272 | Loss: 2272.8491\n",
      "  ğŸ“¦ Batch 1045/1272 | Loss: 2330.0708\n",
      "  ğŸ“¦ Batch 1046/1272 | Loss: 2292.1240\n",
      "  ğŸ“¦ Batch 1047/1272 | Loss: 2289.8013\n",
      "  ğŸ“¦ Batch 1048/1272 | Loss: 2255.4104\n",
      "  ğŸ“¦ Batch 1049/1272 | Loss: 2164.3525\n",
      "  ğŸ“¦ Batch 1050/1272 | Loss: 2360.7537\n",
      "  ğŸ“¦ Batch 1051/1272 | Loss: 2104.8848\n",
      "  ğŸ“¦ Batch 1052/1272 | Loss: 2411.0371\n",
      "  ğŸ“¦ Batch 1053/1272 | Loss: 2361.2498\n",
      "  ğŸ“¦ Batch 1054/1272 | Loss: 2315.9172\n",
      "  ğŸ“¦ Batch 1055/1272 | Loss: 2306.3120\n",
      "  ğŸ“¦ Batch 1056/1272 | Loss: 2207.8550\n",
      "  ğŸ“¦ Batch 1057/1272 | Loss: 2314.1130\n",
      "  ğŸ“¦ Batch 1058/1272 | Loss: 2298.7666\n",
      "  ğŸ“¦ Batch 1059/1272 | Loss: 2312.2090\n",
      "  ğŸ“¦ Batch 1060/1272 | Loss: 2129.9736\n",
      "  ğŸ“¦ Batch 1061/1272 | Loss: 2269.7920\n",
      "  ğŸ“¦ Batch 1062/1272 | Loss: 2106.7646\n",
      "  ğŸ“¦ Batch 1063/1272 | Loss: 2242.4634\n",
      "  ğŸ“¦ Batch 1064/1272 | Loss: 2257.8794\n",
      "  ğŸ“¦ Batch 1065/1272 | Loss: 2187.8447\n",
      "  ğŸ“¦ Batch 1066/1272 | Loss: 2333.5127\n",
      "  ğŸ“¦ Batch 1067/1272 | Loss: 2228.9919\n",
      "  ğŸ“¦ Batch 1068/1272 | Loss: 2264.6199\n",
      "  ğŸ“¦ Batch 1069/1272 | Loss: 2275.7336\n",
      "  ğŸ“¦ Batch 1070/1272 | Loss: 2283.6897\n",
      "  ğŸ“¦ Batch 1071/1272 | Loss: 2375.6221\n",
      "  ğŸ“¦ Batch 1072/1272 | Loss: 2104.9580\n",
      "  ğŸ“¦ Batch 1073/1272 | Loss: 2110.3887\n",
      "  ğŸ“¦ Batch 1074/1272 | Loss: 2193.1216\n",
      "  ğŸ“¦ Batch 1075/1272 | Loss: 2148.6450\n",
      "  ğŸ“¦ Batch 1076/1272 | Loss: 2228.6997\n",
      "  ğŸ“¦ Batch 1077/1272 | Loss: 2233.0256\n",
      "  ğŸ“¦ Batch 1078/1272 | Loss: 2318.8237\n",
      "  ğŸ“¦ Batch 1079/1272 | Loss: 2228.3359\n",
      "  ğŸ“¦ Batch 1080/1272 | Loss: 2158.1541\n",
      "  ğŸ“¦ Batch 1081/1272 | Loss: 2243.1836\n",
      "  ğŸ“¦ Batch 1082/1272 | Loss: 2173.4678\n",
      "  ğŸ“¦ Batch 1083/1272 | Loss: 2172.2407\n",
      "  ğŸ“¦ Batch 1084/1272 | Loss: 2208.4097\n",
      "  ğŸ“¦ Batch 1085/1272 | Loss: 2164.8379\n",
      "  ğŸ“¦ Batch 1086/1272 | Loss: 2134.4619\n",
      "  ğŸ“¦ Batch 1087/1272 | Loss: 2260.4304\n",
      "  ğŸ“¦ Batch 1088/1272 | Loss: 2181.3379\n",
      "  ğŸ“¦ Batch 1089/1272 | Loss: 2175.1194\n",
      "  ğŸ“¦ Batch 1090/1272 | Loss: 2306.7422\n",
      "  ğŸ“¦ Batch 1091/1272 | Loss: 2083.2351\n",
      "  ğŸ“¦ Batch 1092/1272 | Loss: 2338.9980\n",
      "  ğŸ“¦ Batch 1093/1272 | Loss: 2186.3799\n",
      "  ğŸ“¦ Batch 1094/1272 | Loss: 2118.7212\n",
      "  ğŸ“¦ Batch 1095/1272 | Loss: 2106.7288\n",
      "  ğŸ“¦ Batch 1096/1272 | Loss: 2174.2056\n",
      "  ğŸ“¦ Batch 1097/1272 | Loss: 2199.0891\n",
      "  ğŸ“¦ Batch 1098/1272 | Loss: 2282.5781\n",
      "  ğŸ“¦ Batch 1099/1272 | Loss: 2285.4500\n",
      "  ğŸ“¦ Batch 1100/1272 | Loss: 2284.2456\n",
      "  ğŸ“¦ Batch 1101/1272 | Loss: 2121.6162\n",
      "  ğŸ“¦ Batch 1102/1272 | Loss: 2220.7937\n",
      "  ğŸ“¦ Batch 1103/1272 | Loss: 2197.8254\n",
      "  ğŸ“¦ Batch 1104/1272 | Loss: 2061.8157\n",
      "  ğŸ“¦ Batch 1105/1272 | Loss: 2104.1567\n",
      "  ğŸ“¦ Batch 1106/1272 | Loss: 2120.2266\n",
      "  ğŸ“¦ Batch 1107/1272 | Loss: 2149.3044\n",
      "  ğŸ“¦ Batch 1108/1272 | Loss: 2244.9565\n",
      "  ğŸ“¦ Batch 1109/1272 | Loss: 2360.9702\n",
      "  ğŸ“¦ Batch 1110/1272 | Loss: 2111.3091\n",
      "  ğŸ“¦ Batch 1111/1272 | Loss: 2107.4668\n",
      "  ğŸ“¦ Batch 1112/1272 | Loss: 2132.7881\n",
      "  ğŸ“¦ Batch 1113/1272 | Loss: 2197.3057\n",
      "  ğŸ“¦ Batch 1114/1272 | Loss: 2150.3037\n",
      "  ğŸ“¦ Batch 1115/1272 | Loss: 2086.8452\n",
      "  ğŸ“¦ Batch 1116/1272 | Loss: 2363.9731\n",
      "  ğŸ“¦ Batch 1117/1272 | Loss: 2064.7529\n",
      "  ğŸ“¦ Batch 1118/1272 | Loss: 2249.4312\n",
      "  ğŸ“¦ Batch 1119/1272 | Loss: 2136.0469\n",
      "  ğŸ“¦ Batch 1120/1272 | Loss: 2259.8081\n",
      "  ğŸ“¦ Batch 1121/1272 | Loss: 2185.1326\n",
      "  ğŸ“¦ Batch 1122/1272 | Loss: 2169.2224\n",
      "  ğŸ“¦ Batch 1123/1272 | Loss: 2156.5188\n",
      "  ğŸ“¦ Batch 1124/1272 | Loss: 2258.8013\n",
      "  ğŸ“¦ Batch 1125/1272 | Loss: 2153.9365\n",
      "  ğŸ“¦ Batch 1126/1272 | Loss: 2266.9004\n",
      "  ğŸ“¦ Batch 1127/1272 | Loss: 2262.0454\n",
      "  ğŸ“¦ Batch 1128/1272 | Loss: 2156.1943\n",
      "  ğŸ“¦ Batch 1129/1272 | Loss: 2188.6211\n",
      "  ğŸ“¦ Batch 1130/1272 | Loss: 2197.7129\n",
      "  ğŸ“¦ Batch 1131/1272 | Loss: 2182.5222\n",
      "  ğŸ“¦ Batch 1132/1272 | Loss: 2295.7817\n",
      "  ğŸ“¦ Batch 1133/1272 | Loss: 2215.2156\n",
      "  ğŸ“¦ Batch 1134/1272 | Loss: 2259.7969\n",
      "  ğŸ“¦ Batch 1135/1272 | Loss: 2207.5452\n",
      "  ğŸ“¦ Batch 1136/1272 | Loss: 2268.6843\n",
      "  ğŸ“¦ Batch 1137/1272 | Loss: 2103.1694\n",
      "  ğŸ“¦ Batch 1138/1272 | Loss: 2103.9209\n",
      "  ğŸ“¦ Batch 1139/1272 | Loss: 2293.1826\n",
      "  ğŸ“¦ Batch 1140/1272 | Loss: 2264.2200\n",
      "  ğŸ“¦ Batch 1141/1272 | Loss: 2172.4778\n",
      "  ğŸ“¦ Batch 1142/1272 | Loss: 2249.4082\n",
      "  ğŸ“¦ Batch 1143/1272 | Loss: 2157.3457\n",
      "  ğŸ“¦ Batch 1144/1272 | Loss: 2243.6628\n",
      "  ğŸ“¦ Batch 1145/1272 | Loss: 2020.7207\n",
      "  ğŸ“¦ Batch 1146/1272 | Loss: 2133.5840\n",
      "  ğŸ“¦ Batch 1147/1272 | Loss: 2150.6899\n",
      "  ğŸ“¦ Batch 1148/1272 | Loss: 2188.9109\n",
      "  ğŸ“¦ Batch 1149/1272 | Loss: 2285.7241\n",
      "  ğŸ“¦ Batch 1150/1272 | Loss: 2165.6514\n",
      "  ğŸ“¦ Batch 1151/1272 | Loss: 2208.6404\n",
      "  ğŸ“¦ Batch 1152/1272 | Loss: 2048.4028\n",
      "  ğŸ“¦ Batch 1153/1272 | Loss: 2186.5518\n",
      "  ğŸ“¦ Batch 1154/1272 | Loss: 2108.6194\n",
      "  ğŸ“¦ Batch 1155/1272 | Loss: 2160.6758\n",
      "  ğŸ“¦ Batch 1156/1272 | Loss: 2173.2869\n",
      "  ğŸ“¦ Batch 1157/1272 | Loss: 2242.9314\n",
      "  ğŸ“¦ Batch 1158/1272 | Loss: 2237.2646\n",
      "  ğŸ“¦ Batch 1159/1272 | Loss: 2167.9783\n",
      "  ğŸ“¦ Batch 1160/1272 | Loss: 2087.5356\n",
      "  ğŸ“¦ Batch 1161/1272 | Loss: 2276.9077\n",
      "  ğŸ“¦ Batch 1162/1272 | Loss: 2120.1724\n",
      "  ğŸ“¦ Batch 1163/1272 | Loss: 2077.8005\n",
      "  ğŸ“¦ Batch 1164/1272 | Loss: 2130.0703\n",
      "  ğŸ“¦ Batch 1165/1272 | Loss: 2039.6655\n",
      "  ğŸ“¦ Batch 1166/1272 | Loss: 2176.1689\n",
      "  ğŸ“¦ Batch 1167/1272 | Loss: 2207.1353\n",
      "  ğŸ“¦ Batch 1168/1272 | Loss: 2254.2798\n",
      "  ğŸ“¦ Batch 1169/1272 | Loss: 2268.3325\n",
      "  ğŸ“¦ Batch 1170/1272 | Loss: 2286.1626\n",
      "  ğŸ“¦ Batch 1171/1272 | Loss: 2084.9221\n",
      "  ğŸ“¦ Batch 1172/1272 | Loss: 2132.9932\n",
      "  ğŸ“¦ Batch 1173/1272 | Loss: 2140.2202\n",
      "  ğŸ“¦ Batch 1174/1272 | Loss: 2236.2651\n",
      "  ğŸ“¦ Batch 1175/1272 | Loss: 2168.7429\n",
      "  ğŸ“¦ Batch 1176/1272 | Loss: 2110.7710\n",
      "  ğŸ“¦ Batch 1177/1272 | Loss: 2263.8652\n",
      "  ğŸ“¦ Batch 1178/1272 | Loss: 2200.7834\n",
      "  ğŸ“¦ Batch 1179/1272 | Loss: 2108.9573\n",
      "  ğŸ“¦ Batch 1180/1272 | Loss: 2159.8918\n",
      "  ğŸ“¦ Batch 1181/1272 | Loss: 2130.6526\n",
      "  ğŸ“¦ Batch 1182/1272 | Loss: 2094.1355\n",
      "  ğŸ“¦ Batch 1183/1272 | Loss: 2107.6055\n",
      "  ğŸ“¦ Batch 1184/1272 | Loss: 2262.9783\n",
      "  ğŸ“¦ Batch 1185/1272 | Loss: 2120.8374\n",
      "  ğŸ“¦ Batch 1186/1272 | Loss: 2129.8804\n",
      "  ğŸ“¦ Batch 1187/1272 | Loss: 2055.5574\n",
      "  ğŸ“¦ Batch 1188/1272 | Loss: 2223.7920\n",
      "  ğŸ“¦ Batch 1189/1272 | Loss: 2222.6870\n",
      "  ğŸ“¦ Batch 1190/1272 | Loss: 2161.3818\n",
      "  ğŸ“¦ Batch 1191/1272 | Loss: 2126.5747\n",
      "  ğŸ“¦ Batch 1192/1272 | Loss: 2010.7433\n",
      "  ğŸ“¦ Batch 1193/1272 | Loss: 2152.7432\n",
      "  ğŸ“¦ Batch 1194/1272 | Loss: 2193.5044\n",
      "  ğŸ“¦ Batch 1195/1272 | Loss: 2025.6644\n",
      "  ğŸ“¦ Batch 1196/1272 | Loss: 2212.8413\n",
      "  ğŸ“¦ Batch 1197/1272 | Loss: 2056.4006\n",
      "  ğŸ“¦ Batch 1198/1272 | Loss: 2035.2905\n",
      "  ğŸ“¦ Batch 1199/1272 | Loss: 2119.4653\n",
      "  ğŸ“¦ Batch 1200/1272 | Loss: 2053.5283\n",
      "  ğŸ“¦ Batch 1201/1272 | Loss: 2139.2041\n",
      "  ğŸ“¦ Batch 1202/1272 | Loss: 2047.5419\n",
      "  ğŸ“¦ Batch 1203/1272 | Loss: 2192.9487\n",
      "  ğŸ“¦ Batch 1204/1272 | Loss: 2177.7795\n",
      "  ğŸ“¦ Batch 1205/1272 | Loss: 2215.8062\n",
      "  ğŸ“¦ Batch 1206/1272 | Loss: 2070.3442\n",
      "  ğŸ“¦ Batch 1207/1272 | Loss: 2100.3110\n",
      "  ğŸ“¦ Batch 1208/1272 | Loss: 2182.7900\n",
      "  ğŸ“¦ Batch 1209/1272 | Loss: 2157.8948\n",
      "  ğŸ“¦ Batch 1210/1272 | Loss: 2234.8125\n",
      "  ğŸ“¦ Batch 1211/1272 | Loss: 2092.1724\n",
      "  ğŸ“¦ Batch 1212/1272 | Loss: 2116.5518\n",
      "  ğŸ“¦ Batch 1213/1272 | Loss: 2218.7529\n",
      "  ğŸ“¦ Batch 1214/1272 | Loss: 2094.1511\n",
      "  ğŸ“¦ Batch 1215/1272 | Loss: 2242.0171\n",
      "  ğŸ“¦ Batch 1216/1272 | Loss: 2060.7617\n",
      "  ğŸ“¦ Batch 1217/1272 | Loss: 2155.4778\n",
      "  ğŸ“¦ Batch 1218/1272 | Loss: 2059.5322\n",
      "  ğŸ“¦ Batch 1219/1272 | Loss: 2154.4407\n",
      "  ğŸ“¦ Batch 1220/1272 | Loss: 2005.3311\n",
      "  ğŸ“¦ Batch 1221/1272 | Loss: 2010.2937\n",
      "  ğŸ“¦ Batch 1222/1272 | Loss: 2234.0840\n",
      "  ğŸ“¦ Batch 1223/1272 | Loss: 2156.7786\n",
      "  ğŸ“¦ Batch 1224/1272 | Loss: 2158.2085\n",
      "  ğŸ“¦ Batch 1225/1272 | Loss: 2255.0659\n",
      "  ğŸ“¦ Batch 1226/1272 | Loss: 2187.9077\n",
      "  ğŸ“¦ Batch 1227/1272 | Loss: 2155.6221\n",
      "  ğŸ“¦ Batch 1228/1272 | Loss: 2122.7515\n",
      "  ğŸ“¦ Batch 1229/1272 | Loss: 2132.7212\n",
      "  ğŸ“¦ Batch 1230/1272 | Loss: 2039.7892\n",
      "  ğŸ“¦ Batch 1231/1272 | Loss: 2099.5503\n",
      "  ğŸ“¦ Batch 1232/1272 | Loss: 2159.3281\n",
      "  ğŸ“¦ Batch 1233/1272 | Loss: 2133.5962\n",
      "  ğŸ“¦ Batch 1234/1272 | Loss: 2086.8464\n",
      "  ğŸ“¦ Batch 1235/1272 | Loss: 2187.5454\n",
      "  ğŸ“¦ Batch 1236/1272 | Loss: 2152.8921\n",
      "  ğŸ“¦ Batch 1237/1272 | Loss: 2046.5852\n",
      "  ğŸ“¦ Batch 1238/1272 | Loss: 2116.0071\n",
      "  ğŸ“¦ Batch 1239/1272 | Loss: 2145.4165\n",
      "  ğŸ“¦ Batch 1240/1272 | Loss: 2221.4612\n",
      "  ğŸ“¦ Batch 1241/1272 | Loss: 2094.0779\n",
      "  ğŸ“¦ Batch 1242/1272 | Loss: 2042.3036\n",
      "  ğŸ“¦ Batch 1243/1272 | Loss: 2054.0913\n",
      "  ğŸ“¦ Batch 1244/1272 | Loss: 2091.7324\n",
      "  ğŸ“¦ Batch 1245/1272 | Loss: 2149.0669\n",
      "  ğŸ“¦ Batch 1246/1272 | Loss: 2054.5769\n",
      "  ğŸ“¦ Batch 1247/1272 | Loss: 2163.6011\n",
      "  ğŸ“¦ Batch 1248/1272 | Loss: 2098.9780\n",
      "  ğŸ“¦ Batch 1249/1272 | Loss: 2029.4351\n",
      "  ğŸ“¦ Batch 1250/1272 | Loss: 2202.7148\n",
      "  ğŸ“¦ Batch 1251/1272 | Loss: 2134.5410\n",
      "  ğŸ“¦ Batch 1252/1272 | Loss: 2255.4434\n",
      "  ğŸ“¦ Batch 1253/1272 | Loss: 2060.2109\n",
      "  ğŸ“¦ Batch 1254/1272 | Loss: 2148.2061\n",
      "  ğŸ“¦ Batch 1255/1272 | Loss: 2141.7273\n",
      "  ğŸ“¦ Batch 1256/1272 | Loss: 1971.8374\n",
      "  ğŸ“¦ Batch 1257/1272 | Loss: 2117.1919\n",
      "  ğŸ“¦ Batch 1258/1272 | Loss: 2034.7969\n",
      "  ğŸ“¦ Batch 1259/1272 | Loss: 2080.0120\n",
      "  ğŸ“¦ Batch 1260/1272 | Loss: 1966.2275\n",
      "  ğŸ“¦ Batch 1261/1272 | Loss: 2095.3135\n",
      "  ğŸ“¦ Batch 1262/1272 | Loss: 2043.4054\n",
      "  ğŸ“¦ Batch 1263/1272 | Loss: 2020.6552\n",
      "  ğŸ“¦ Batch 1264/1272 | Loss: 2122.9224\n",
      "  ğŸ“¦ Batch 1265/1272 | Loss: 2096.3396\n",
      "  ğŸ“¦ Batch 1266/1272 | Loss: 2172.0386\n",
      "  ğŸ“¦ Batch 1267/1272 | Loss: 2041.7201\n",
      "  ğŸ“¦ Batch 1268/1272 | Loss: 2130.4221\n",
      "  ğŸ“¦ Batch 1269/1272 | Loss: 2171.1016\n",
      "  ğŸ“¦ Batch 1270/1272 | Loss: 2059.3501\n",
      "  ğŸ“¦ Batch 1271/1272 | Loss: 2132.6909\n",
      "  ğŸ“¦ Batch 1272/1272 | Loss: 2037.8180\n",
      "âœ… Epoch 10 Avg Loss: 2603.5148\n",
      "ğŸ“Š Confusion Matrix:\n",
      "[[537 583]\n",
      " [580 533]]\n",
      "ğŸ“ˆ Image-level ROC AUC: 0.4718\n"
     ]
    }
   ],
   "source": [
    "train_msflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf906ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f298f08-5166-43bc-8710-c4e7a11d3d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-gpu)",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
